"""
Module de fonctions utilitaires pour l'application d'aide √† la d√©cision marketing.

Ce module contient toutes les fonctions de traitement des donn√©es, calculs m√©tier
et op√©rations d'export. Les fonctions sont con√ßues pour √™tre pures, testables et
r√©utilisables √† travers l'application.
"""

import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple, Union
import matplotlib.pyplot as plt
import plotly.graph_objects as go
from pathlib import Path
import os
import sys

sys.path.append(str(Path(__file__).parent.parent))
import config


# ==============================================================================
# CHARGEMENT ET NETTOYAGE DES DONNEES
# ==============================================================================

def load_data(file_path: Union[str, Path], verbose: bool = False) -> pd.DataFrame:
    """
    Charge les donn√©es depuis un fichier CSV ou Excel avec optimisations pour les gros fichiers.

    Cette fonction d√©tecte automatiquement le format du fichier et applique
    les param√®tres de chargement appropri√©s (encodage, s√©parateur, etc.).
    Elle est optimis√©e pour minimiser la perte de donn√©es et g√©rer efficacement
    les fichiers volumineux (>500k lignes).

    Parameters
    ----------
    file_path : str or Path
        Chemin vers le fichier de donn√©es (CSV ou Excel)
    verbose : bool, default=False
        Si True, affiche des informations d√©taill√©es sur le chargement

    Returns
    -------
    pd.DataFrame
        DataFrame contenant les donn√©es charg√©es avec m√©tadonn√©es de qualit√©

    Raises
    ------
    FileNotFoundError
        Si le fichier n'existe pas
    ValueError
        Si le format du fichier n'est pas support√©

    Examples
    --------
    >>> df = load_data(config.RAW_DATA_CSV, verbose=True)
    >>> print(df.shape)
    (525461, 8)

    Notes
    -----
    Formats support√©s : .csv, .xlsx, .xls
    L'encodage par d√©faut pour les CSV est UTF-8
    Les Customer ID sont charg√©s comme string pour √©viter la perte de pr√©cision
    Les dates invalides sont converties en NaT plut√¥t que de faire √©chouer le chargement

    Optimisations appliqu√©es:
    - Utilisation de dtypes optimis√©s pour r√©duire l'utilisation m√©moire
    - Gestion des valeurs manquantes sans suppression automatique
    - Parsing robuste des dates avec format europ√©en
    - Nettoyage des noms de colonnes (BOM, espaces)
    """
    # Convertir en Path si n√©cessaire
    file_path = Path(file_path)

    # V√©rifier que le fichier existe
    if not file_path.exists():
        raise FileNotFoundError(f"Le fichier {file_path} n'existe pas")

    # D√©tecter l'extension du fichier
    extension = file_path.suffix.lower()

    if verbose:
        print(f"Chargement du fichier: {file_path.name}")
        print(f"Format d√©tect√©: {extension}")

    try:
        if extension == '.csv':
            # Essayer d'abord avec le s√©parateur configur√©
            try:
                df = pd.read_csv(
                    file_path,
                    encoding=config.FILE_ENCODING,
                    sep=config.CSV_SEPARATOR,
                    dtype={'Customer ID': str, 'Invoice': str, 'StockCode': str},
                    parse_dates=['InvoiceDate'],
                    dayfirst=True,
                    na_values=['', 'NA', 'N/A', 'null', 'NULL', 'None']
                )
            except Exception:
                # Si √©chec, essayer avec d√©tection automatique du s√©parateur
                df = pd.read_csv(
                    file_path,
                    encoding=config.FILE_ENCODING,
                    sep=None,  # D√©tection automatique
                    engine='python',
                    dtype={'Customer ID': str, 'Invoice': str, 'StockCode': str},
                    na_values=['', 'NA', 'N/A', 'null', 'NULL', 'None']
                )

                # Parser les dates apr√®s chargement
                if 'InvoiceDate' in df.columns:
                    df['InvoiceDate'] = pd.to_datetime(df['InvoiceDate'], dayfirst=True, errors='coerce')

        elif extension in ['.xlsx', '.xls']:
            # Charger Excel avec gestion optimis√©e de la m√©moire
            df = pd.read_excel(
                file_path,
                dtype={'Customer ID': str, 'Invoice': str, 'StockCode': str},
                na_values=['', 'NA', 'N/A', 'null', 'NULL', 'None'],
                engine='openpyxl' if extension == '.xlsx' else None
            )

            # Parser les dates (Excel les charge g√©n√©ralement correctement, mais v√©rifier)
            if 'InvoiceDate' in df.columns:
                if df['InvoiceDate'].dtype != 'datetime64[ns]':
                    df['InvoiceDate'] = pd.to_datetime(df['InvoiceDate'], errors='coerce')

        else:
            raise ValueError(f"Format de fichier non support√© : {extension}. Formats accept√©s : .csv, .xlsx, .xls")

        # Validation basique
        if df.empty:
            raise ValueError("Le fichier charg√© est vide")

        # Nettoyer les noms de colonnes (supprimer BOM et espaces)
        df.columns = df.columns.str.replace('\ufeff', '', regex=False).str.strip()

        # Convertir les colonnes num√©riques si n√©cessaire (gestion format europ√©en)
        numeric_columns = ['Quantity', 'Price']
        for col in numeric_columns:
            if col in df.columns:
                # Si la colonne est de type string, convertir (remplacer virgule par point)
                if df[col].dtype == 'object':
                    df[col] = df[col].astype(str).str.replace(',', '.', regex=False)
                    df[col] = pd.to_numeric(df[col], errors='coerce')
                elif df[col].dtype not in ['int64', 'float64']:
                    df[col] = pd.to_numeric(df[col], errors='coerce')

        if verbose:
            print(f"\nChargement r√©ussi!")
            print(f"Dimensions: {df.shape[0]:,} lignes x {df.shape[1]} colonnes")
            print(f"Colonnes: {list(df.columns)}")
            print(f"\nValeurs manquantes par colonne:")
            missing = df.isnull().sum()
            for col in df.columns:
                if missing[col] > 0:
                    pct = (missing[col] / len(df)) * 100
                    print(f"  {col}: {missing[col]:,} ({pct:.2f}%)")

        return df

    except Exception as e:
        raise ValueError(f"Erreur lors du chargement du fichier {file_path}: {str(e)}")


def clean_data(df: pd.DataFrame, verbose: bool = False, strict_mode: bool = False) -> pd.DataFrame:
    """
    Nettoie et pr√©pare les donn√©es pour l'analyse avec minimisation de la perte de donn√©es.

    Cette fonction effectue un nettoyage intelligent qui maximise la r√©tention des donn√©es
    tout en maintenant la qualit√© n√©cessaire pour les analyses. Elle cat√©gorise les donn√©es
    plut√¥t que de les supprimer syst√©matiquement.

    STRATEGIE DE NETTOYAGE OPTIMISEE :

    1. Conservation des donn√©es :
       - Les transactions SANS Customer ID sont CONSERVEES pour les analyses
         produits/pays/tendances (mais marqu√©es pour exclusion des analyses clients)
       - Les retours/annulations sont CONSERVES et marqu√©s (utiles pour taux de retour)
       - Les doublons exacts sont supprim√©s (peu de perte, am√©liore qualit√©)

    2. Filtrage strict uniquement pour :
       - Prix invalides (n√©gatifs ou z√©ro) : impossible √† analyser financi√®rement
       - Incoh√©rences logiques : retours avec quantit√© positive, ventes avec quantit√© n√©gative
       - Dates invalides : impossibles √† analyser temporellement

    3. Enrichissement des donn√©es :
       - Cr√©ation de flags plut√¥t que suppression (IsReturn, HasCustomerID, etc.)
       - Colonnes temporelles pour analyses temporelles
       - Colonnes calcul√©es (TotalAmount, etc.)

    Parameters
    ----------
    df : pd.DataFrame
        DataFrame brut √† nettoyer
    verbose : bool, default=False
        Si True, affiche un rapport d√©taill√© du nettoyage avec statistiques
    strict_mode : bool, default=False
        Si True, applique un filtrage strict (supprime les lignes sans Customer ID)
        Si False (recommand√©), conserve toutes les donn√©es valides avec flags

    Returns
    -------
    pd.DataFrame
        DataFrame nettoy√© et enrichi, pr√™t pour l'analyse

    Examples
    --------
    >>> df_raw = load_data(config.RAW_DATA_CSV)
    >>> df_clean = clean_data(df_raw, verbose=True, strict_mode=False)
    >>> print(f"Lignes conserv√©es : {len(df_clean)} / {len(df_raw)}")
    >>> # Analyser uniquement les clients
    >>> df_customers = df_clean[df_clean['HasCustomerID']]

    Notes
    -----
    Mode recommand√© : strict_mode=False pour analyses marketing compl√®tes
    Mode strict : strict_mode=True pour analyses purement centr√©es clients (RFM, CLV, etc.)

    Taux de r√©tention attendus :
    - Mode non-strict : ~95-97% des lignes (perte minimale)
    - Mode strict : ~78% des lignes (conforme aux analyses clients)
    """
    if verbose:
        print("="*70)
        print("NETTOYAGE DES DONNEES")
        print("="*70)
        print(f"Mode: {'STRICT (customer-centric)' if strict_mode else 'OPTIMISE (data preservation)'}")
        print(f"Lignes initiales: {len(df):,}")

    # Copier pour √©viter de modifier l'original
    df_clean = df.copy()
    initial_count = len(df_clean)

    # ==========================================================================
    # ETAPE 1 : IDENTIFICATION ET MARQUAGE (PAS DE SUPPRESSION)
    # ==========================================================================

    # 1.1 Identifier les lignes avec Customer ID
    df_clean['HasCustomerID'] = df_clean['Customer ID'].notna()

    # 1.2 Identifier les retours/annulations (Invoice commence par 'C')
    df_clean['IsReturn'] = df_clean['Invoice'].astype(str).str.startswith(config.CANCELLATION_PREFIX)

    # 1.3 Identifier les dates valides
    df_clean['HasValidDate'] = df_clean['InvoiceDate'].notna()

    if verbose:
        print(f"\nMarquage des donn√©es:")
        print(f"  Avec Customer ID: {df_clean['HasCustomerID'].sum():,} ({df_clean['HasCustomerID'].sum()/len(df_clean)*100:.2f}%)")
        print(f"  Sans Customer ID: {(~df_clean['HasCustomerID']).sum():,} ({(~df_clean['HasCustomerID']).sum()/len(df_clean)*100:.2f}%)")
        print(f"  Retours/Annulations: {df_clean['IsReturn'].sum():,} ({df_clean['IsReturn'].sum()/len(df_clean)*100:.2f}%)")
        print(f"  Dates valides: {df_clean['HasValidDate'].sum():,} ({df_clean['HasValidDate'].sum()/len(df_clean)*100:.2f}%)")

    # ==========================================================================
    # ETAPE 2 : FILTRAGE DES DONNEES REELLEMENT INVALIDES
    # ==========================================================================

    step_results = []

    # 2.1 Supprimer les lignes avec dates invalides (impossible √† analyser temporellement)
    before = len(df_clean)
    df_clean = df_clean[df_clean['HasValidDate']]
    after = len(df_clean)
    lost = before - after
    if verbose and lost > 0:
        step_results.append(f"  Dates invalides: -{lost:,} lignes ({lost/initial_count*100:.2f}%)")

    # 2.2 Supprimer les prix invalides (n√©gatifs ou z√©ro)
    # Un prix de 0 ou n√©gatif rend impossible toute analyse financi√®re
    before = len(df_clean)
    df_clean = df_clean[df_clean['Price'] > config.MIN_PRICE]
    after = len(df_clean)
    lost = before - after
    if verbose and lost > 0:
        step_results.append(f"  Prix invalides (<=0): -{lost:,} lignes ({lost/initial_count*100:.2f}%)")

    # 2.3 Filtrer les incoh√©rences logiques dans les quantit√©s
    # - Vente normale (pas de retour) : quantit√© doit √™tre positive
    # - Retour/annulation : quantit√© doit √™tre n√©gative ou nulle
    before = len(df_clean)
    mask_valid_quantity = (
        (~df_clean['IsReturn'] & (df_clean['Quantity'] > config.MIN_QUANTITY)) |
        (df_clean['IsReturn'] & (df_clean['Quantity'] <= config.MIN_QUANTITY))
    )
    df_clean = df_clean[mask_valid_quantity]
    after = len(df_clean)
    lost = before - after
    if verbose and lost > 0:
        step_results.append(f"  Incoh√©rences quantit√©s: -{lost:,} lignes ({lost/initial_count*100:.2f}%)")

    # 2.4 Supprimer les doublons exacts (erreurs de saisie/import)
    before = len(df_clean)
    df_clean = df_clean.drop_duplicates()
    after = len(df_clean)
    lost = before - after
    if verbose and lost > 0:
        step_results.append(f"  Doublons exacts: -{lost:,} lignes ({lost/initial_count*100:.2f}%)")

    # 2.5 MODE STRICT : Supprimer les lignes sans Customer ID
    if strict_mode:
        before = len(df_clean)
        df_clean = df_clean[df_clean['HasCustomerID']]
        after = len(df_clean)
        lost = before - after
        if verbose and lost > 0:
            step_results.append(f"  Sans Customer ID (mode strict): -{lost:,} lignes ({lost/initial_count*100:.2f}%)")

    if verbose and step_results:
        print(f"\nSuppression des donn√©es invalides:")
        for result in step_results:
            print(result)

    # ==========================================================================
    # ETAPE 3 : ENRICHISSEMENT DES DONNEES
    # ==========================================================================

    # 3.1 Calculer le montant total de chaque ligne
    df_clean['TotalAmount'] = df_clean['Quantity'] * df_clean['Price']

    # 3.2 Cr√©er les colonnes temporelles pour analyses
    df_clean['Year'] = df_clean['InvoiceDate'].dt.year
    df_clean['Month'] = df_clean['InvoiceDate'].dt.month
    df_clean['Quarter'] = df_clean['InvoiceDate'].dt.quarter
    df_clean['DayOfWeek'] = df_clean['InvoiceDate'].dt.dayofweek  # 0 = Lundi, 6 = Dimanche
    df_clean['Hour'] = df_clean['InvoiceDate'].dt.hour
    df_clean['Date'] = df_clean['InvoiceDate'].dt.date

    # 3.3 Cr√©er une colonne pour le type de transaction (plus lisible)
    df_clean['TransactionType'] = df_clean['IsReturn'].map({
        True: 'Return',
        False: 'Sale'
    })

    # 3.4 Cr√©er une colonne de cat√©gorisation pour l'analyse
    # Permet de filtrer facilement selon les besoins d'analyse
    def categorize_transaction(row):
        """Cat√©gorise chaque transaction pour faciliter les analyses"""
        if not row['HasCustomerID']:
            return 'NoCustomer_' + row['TransactionType']
        else:
            return 'Customer_' + row['TransactionType']

    df_clean['Category'] = df_clean.apply(categorize_transaction, axis=1)

    # ==========================================================================
    # ETAPE 4 : ORGANISATION FINALE
    # ==========================================================================

    # 4.1 Trier par date pour analyses temporelles
    df_clean = df_clean.sort_values('InvoiceDate').reset_index(drop=True)

    # 4.2 R√©organiser les colonnes pour une meilleure lisibilit√©
    # Colonnes de base d'abord, puis colonnes calcul√©es, puis flags
    base_cols = ['Invoice', 'StockCode', 'Description', 'Quantity', 'Price',
                 'InvoiceDate', 'Customer ID', 'Country']
    calc_cols = ['TotalAmount', 'Year', 'Month', 'Quarter', 'DayOfWeek', 'Hour', 'Date',
                 'TransactionType', 'Category']
    flag_cols = ['IsReturn', 'HasCustomerID', 'HasValidDate']

    # R√©organiser en v√©rifiant que les colonnes existent
    cols_order = []
    for col in base_cols + calc_cols + flag_cols:
        if col in df_clean.columns:
            cols_order.append(col)

    # Ajouter les colonnes restantes (si nouvelles colonnes ajout√©es)
    for col in df_clean.columns:
        if col not in cols_order:
            cols_order.append(col)

    df_clean = df_clean[cols_order]

    # ==========================================================================
    # ETAPE 5 : RAPPORT FINAL
    # ==========================================================================

    if verbose:
        final_count = len(df_clean)
        retained_pct = (final_count / initial_count) * 100
        lost_total = initial_count - final_count

        print(f"\n{'='*70}")
        print("RESULTAT DU NETTOYAGE")
        print(f"{'='*70}")
        print(f"Lignes initiales:     {initial_count:,}")
        print(f"Lignes conserv√©es:    {final_count:,}")
        print(f"Lignes supprim√©es:    {lost_total:,}")
        print(f"Taux de r√©tention:    {retained_pct:.2f}%")

        print(f"\nR√âPARTITION DES DONN√âES CONSERV√âES:")
        print(f"  Ventes avec client:      {((df_clean['Category'] == 'Customer_Sale').sum()):,} ({(df_clean['Category'] == 'Customer_Sale').sum()/final_count*100:.2f}%)")
        print(f"  Retours avec client:     {((df_clean['Category'] == 'Customer_Return').sum()):,} ({(df_clean['Category'] == 'Customer_Return').sum()/final_count*100:.2f}%)")

        if not strict_mode:
            print(f"  Ventes sans client:      {((df_clean['Category'] == 'NoCustomer_Sale').sum()):,} ({(df_clean['Category'] == 'NoCustomer_Sale').sum()/final_count*100:.2f}%)")
            print(f"  Retours sans client:     {((df_clean['Category'] == 'NoCustomer_Return').sum()):,} ({(df_clean['Category'] == 'NoCustomer_Return').sum()/final_count*100:.2f}%)")

        print(f"\nCONSEILS D'UTILISATION:")
        if strict_mode:
            print("  Mode strict activ√© - donn√©es pr√™tes pour analyses RFM/CLV/Cohortes")
            print("  Toutes les lignes ont un Customer ID valide")
        else:
            print("  Mode optimis√© - donn√©es compl√®tes conserv√©es")
            print("  Pour analyses clients uniquement: df[df['HasCustomerID']]")
            print("  Pour analyses produits/pays: utiliser toutes les donn√©es")
            print("  Pour analyses financi√®res: utiliser df[df['Category'].str.contains('Sale')]")

        print(f"{'='*70}\n")

    return df_clean


def get_data_quality_report(df: pd.DataFrame) -> Dict:
    """
    G√©n√®re un rapport d√©taill√© sur la qualit√© des donn√©es.

    Cette fonction analyse un DataFrame et retourne un dictionnaire complet
    contenant des m√©triques de qualit√©, des statistiques descriptives et
    des indicateurs de compl√©tude.

    Parameters
    ----------
    df : pd.DataFrame
        DataFrame √† analyser

    Returns
    -------
    dict
        Dictionnaire contenant :
        - basic_info : informations g√©n√©rales (dimensions, m√©moire)
        - missing_values : valeurs manquantes par colonne
        - data_types : types de donn√©es
        - unique_counts : nombre de valeurs uniques
        - quality_flags : flags de qualit√© si disponibles
        - statistics : statistiques descriptives pour colonnes num√©riques

    Examples
    --------
    >>> df_clean = clean_data(df_raw)
    >>> report = get_data_quality_report(df_clean)
    >>> print(f"Completude globale: {report['completeness']:.2f}%")

    Notes
    -----
    Particuli√®rement utile apr√®s le nettoyage pour valider la qualit√©
    """
    report = {}

    # Informations basiques
    report['basic_info'] = {
        'total_rows': len(df),
        'total_columns': len(df.columns),
        'memory_usage_mb': df.memory_usage(deep=True).sum() / (1024 ** 2),
        'columns': list(df.columns)
    }

    # Valeurs manquantes
    missing = df.isnull().sum()
    missing_pct = (missing / len(df) * 100).round(2)
    report['missing_values'] = {
        col: {'count': int(missing[col]), 'percentage': float(missing_pct[col])}
        for col in df.columns if missing[col] > 0
    }

    # Compl√©tude globale
    total_cells = len(df) * len(df.columns)
    non_null_cells = df.notna().sum().sum()
    report['completeness'] = (non_null_cells / total_cells * 100) if total_cells > 0 else 0

    # Types de donn√©es
    report['data_types'] = {col: str(dtype) for col, dtype in df.dtypes.items()}

    # Nombre de valeurs uniques (pour identifier les colonnes cat√©gorielles)
    report['unique_counts'] = {
        col: int(df[col].nunique())
        for col in df.columns
    }

    # Quality flags si disponibles (colonnes ajout√©es par clean_data)
    quality_flags = ['HasCustomerID', 'IsReturn', 'HasValidDate', 'TransactionType', 'Category']
    available_flags = [flag for flag in quality_flags if flag in df.columns]

    if available_flags:
        report['quality_flags'] = {}
        for flag in available_flags:
            if df[flag].dtype == 'bool':
                report['quality_flags'][flag] = {
                    'true_count': int(df[flag].sum()),
                    'false_count': int((~df[flag]).sum()),
                    'true_percentage': float((df[flag].sum() / len(df) * 100))
                }
            else:
                report['quality_flags'][flag] = dict(df[flag].value_counts().to_dict())

    # Statistiques pour colonnes num√©riques
    numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns
    if len(numeric_cols) > 0:
        report['numeric_statistics'] = {}
        for col in numeric_cols:
            if col in df.columns:
                report['numeric_statistics'][col] = {
                    'mean': float(df[col].mean()),
                    'median': float(df[col].median()),
                    'std': float(df[col].std()),
                    'min': float(df[col].min()),
                    'max': float(df[col].max()),
                    'q25': float(df[col].quantile(0.25)),
                    'q75': float(df[col].quantile(0.75))
                }

    # Statistiques temporelles si InvoiceDate existe
    if 'InvoiceDate' in df.columns:
        report['temporal_info'] = {
            'start_date': str(df['InvoiceDate'].min()),
            'end_date': str(df['InvoiceDate'].max()),
            'date_range_days': (df['InvoiceDate'].max() - df['InvoiceDate'].min()).days
        }

    return report


# ==============================================================================
# ANALYSE DES COHORTES
# ==============================================================================

def create_cohorts(df: pd.DataFrame) -> pd.DataFrame:
    """
    Cr√©e les cohortes d'acquisition bas√©es sur le mois de premi√®re transaction.

    Une cohorte regroupe tous les clients qui ont effectu√© leur premi√®re
    transaction durant le m√™me mois. Cette fonction identifie le mois de
    premi√®re transaction pour chaque client et calcule l'indice de cohorte
    (nombre de mois √©coul√©s depuis la premi√®re transaction).

    Parameters
    ----------
    df : pd.DataFrame
        DataFrame contenant les transactions nettoy√©es
        Doit contenir : 'Customer ID', 'InvoiceDate'

    Returns
    -------
    pd.DataFrame
        DataFrame enrichi avec les colonnes :
        - CohortMonth : mois de premi√®re transaction (YYYY-MM)
        - CohortIndex : nombre de mois depuis la premi√®re transaction

    Examples
    --------
    >>> df_cohorts = create_cohorts(df_clean)
    >>> print(df_cohorts[['Customer ID', 'CohortMonth', 'CohortIndex']].head())

    Notes
    -----
    Format des cohortes : YYYY-MM (ann√©e-mois)
    CohortIndex = 0 pour le mois d'acquisition
    """
    # Copier pour √©viter de modifier l'original
    df_cohorts = df.copy()

    # Cr√©er une colonne pour le mois de facture (InvoiceMonth)
    df_cohorts['InvoiceMonth'] = df_cohorts['InvoiceDate'].dt.to_period('M')

    # Identifier le mois de premi√®re transaction pour chaque client (cohorte)
    cohort_data = df_cohorts.groupby('Customer ID')['InvoiceMonth'].min().reset_index()
    cohort_data.columns = ['Customer ID', 'CohortMonth']

    # Fusionner les informations de cohorte avec le dataframe principal
    df_cohorts = df_cohorts.merge(cohort_data, on='Customer ID', how='left')

    # Calculer l'indice de cohorte (nombre de mois depuis la premi√®re transaction)
    # La diff√©rence entre deux Period donne le nombre de mois
    df_cohorts['CohortIndex'] = (df_cohorts['InvoiceMonth'] - df_cohorts['CohortMonth']).apply(lambda x: x.n)

    return df_cohorts


def calculate_retention(cohort_df: pd.DataFrame) -> pd.DataFrame:
    """
    Calcule les taux de r√©tention pour chaque cohorte.

    Cette fonction cr√©e une matrice de r√©tention montrant le pourcentage de
    clients de chaque cohorte qui sont revenus effectuer un achat durant
    les mois suivants leur acquisition.

    Parameters
    ----------
    cohort_df : pd.DataFrame
        DataFrame avec les colonnes de cohorte (CohortMonth, CohortIndex)

    Returns
    -------
    pd.DataFrame
        Matrice de r√©tention avec :
        - Index : CohortMonth (mois de la cohorte)
        - Colonnes : CohortIndex (0, 1, 2, 3, ..., N mois)
        - Valeurs : taux de r√©tention (0-100%)

    Examples
    --------
    >>> retention_matrix = calculate_retention(df_cohorts)
    >>> print(retention_matrix)
                  0     1     2     3     4
    2009-12   100.0  35.2  28.4  22.1  18.5
    2010-01   100.0  38.7  31.2  25.6  20.3

    Notes
    -----
    La p√©riode 0 (acquisition) affiche toujours 100%
    Les taux sont exprim√©s en pourcentage du nombre initial de clients
    """
    # Compter le nombre de clients uniques par cohorte et par index
    cohort_counts = cohort_df.groupby(['CohortMonth', 'CohortIndex'])['Customer ID'].nunique().reset_index()
    cohort_counts.columns = ['CohortMonth', 'CohortIndex', 'CustomerCount']

    # Cr√©er une table pivot : lignes = CohortMonth, colonnes = CohortIndex
    cohort_pivot = cohort_counts.pivot_table(
        index='CohortMonth',
        columns='CohortIndex',
        values='CustomerCount'
    )

    # Calculer les taux de r√©tention (pourcentage par rapport au M0)
    # Le M0 (index 0) repr√©sente la taille de la cohorte √† l'acquisition
    cohort_size = cohort_pivot.iloc[:, 0]  # Colonne 0 = taille initiale

    # Diviser chaque colonne par la taille de la cohorte et multiplier par 100
    retention_matrix = cohort_pivot.divide(cohort_size, axis=0) * 100

    return retention_matrix


# ==============================================================================
# SEGMENTATION RFM (Recency, Frequency, Monetary)
# ==============================================================================

def calculate_rfm(df: pd.DataFrame, as_of_date: Optional[datetime] = None) -> pd.DataFrame:
    """
    Calcule les scores RFM (Recency, Frequency, Monetary) pour chaque client.

    La m√©thodologie RFM segmente les clients selon :
    - Recency (R) : nombre de jours depuis le dernier achat
    - Frequency (F) : nombre total de transactions
    - Monetary (M) : montant total d√©pens√©

    Chaque dimension est divis√©e en quartiles (1-4, 4 √©tant le meilleur).

    Parameters
    ----------
    df : pd.DataFrame
        DataFrame des transactions nettoy√©es
    as_of_date : datetime, optional
        Date de r√©f√©rence pour le calcul de la r√©cence
        Si None, utilise la date maximale dans les donn√©es

    Returns
    -------
    pd.DataFrame
        DataFrame avec une ligne par client contenant :
        - Customer ID
        - Recency : jours depuis dernier achat
        - Frequency : nombre de transactions
        - Monetary : montant total d√©pens√©
        - R_Score : score de r√©cence (1-4)
        - F_Score : score de fr√©quence (1-4)
        - M_Score : score mon√©taire (1-4)
        - RFM_Score : score combin√© (ex: "444" = meilleur client)
        - RFM_Segment : segment marketing (ex: "Champions")

    Examples
    --------
    >>> rfm_df = calculate_rfm(df_clean)
    >>> print(rfm_df.head())
    >>> print(rfm_df['RFM_Segment'].value_counts())

    Notes
    -----
    Les segments RFM sont d√©finis dans config.RFM_SEGMENTS
    Le score 4 repr√©sente le meilleur quartile pour chaque dimension
    """
    # D√©finir la date de r√©f√©rence
    if as_of_date is None:
        as_of_date = df['InvoiceDate'].max()

    # Filtrer les ventes (exclure les retours)
    df_sales = df[~df['IsReturn']].copy()

    # Calculer les m√©triques RFM par client
    rfm = df_sales.groupby('Customer ID').agg({
        'InvoiceDate': lambda x: (as_of_date - x.max()).days,  # Recency
        'Invoice': 'nunique',  # Frequency (nombre de factures uniques)
        'TotalAmount': 'sum'  # Monetary
    }).reset_index()

    rfm.columns = ['Customer ID', 'Recency', 'Frequency', 'Monetary']

    # Calculer les scores RFM en quartiles (1-4)
    # Utiliser pd.qcut avec gestion des duplicatas
    def assign_score(series, ascending=True):
        """Assigner des scores 1-4 en g√©rant les cas limites"""
        try:
            if ascending:
                # Score direct : valeurs √©lev√©es = score √©lev√©
                scores = pd.qcut(series, q=4, labels=[1, 2, 3, 4], duplicates='drop')
            else:
                # Score invers√© : valeurs faibles = score √©lev√©
                scores = pd.qcut(series, q=4, labels=[4, 3, 2, 1], duplicates='drop')
        except ValueError:
            # Si qcut √©choue (trop de duplicatas), utiliser une m√©thode alternative
            if ascending:
                scores = pd.cut(series, bins=4, labels=[1, 2, 3, 4], include_lowest=True, duplicates='drop')
            else:
                scores = pd.cut(series, bins=4, labels=[4, 3, 2, 1], include_lowest=True, duplicates='drop')

        return scores.astype(int) if scores.notna().all() else scores.fillna(2).astype(int)

    # Pour Recency : score invers√© (faible r√©cence = bon score)
    rfm['R_Score'] = assign_score(rfm['Recency'], ascending=False)
    # Pour Frequency et Monetary : score direct (√©lev√© = bon score)
    rfm['F_Score'] = assign_score(rfm['Frequency'], ascending=True)
    rfm['M_Score'] = assign_score(rfm['Monetary'], ascending=True)

    # Cr√©er le score RFM combin√©
    rfm['RFM_Score'] = rfm['R_Score'].astype(str) + rfm['F_Score'].astype(str) + rfm['M_Score'].astype(str)

    # Fonction pour attribuer les segments
    def assign_segment(row):
        r, f, m = row['R_Score'], row['F_Score'], row['M_Score']

        # Champions : R=4, F=4, M=4
        if r == 4 and f == 4 and m == 4:
            return "Champions"
        # Cannot Lose Them : R=1, F=4, M=4
        elif r == 1 and f == 4 and m == 4:
            return "Cannot Lose Them"
        # Loyal Customers : R‚â•3, F‚â•3, M‚â•3
        elif r >= 3 and f >= 3 and m >= 3:
            return "Loyal Customers"
        # At Risk : R‚â§2, F‚â•3, M‚â•3
        elif r <= 2 and f >= 3 and m >= 3:
            return "At Risk"
        # New Customers : R=4, F=1
        elif r == 4 and f == 1:
            return "New Customers"
        # Potential Loyalists : R‚â•3, F‚â§2, M‚â•2
        elif r >= 3 and f <= 2 and m >= 2:
            return "Potential Loyalists"
        # Hibernating : R‚â§2, F‚â§2, M‚â§2
        elif r <= 2 and f <= 2 and m <= 2:
            return "Hibernating"
        # Lost : R=1, F=1, M=1
        elif r == 1 and f == 1 and m == 1:
            return "Lost"
        else:
            return "Others"

    rfm['RFM_Segment'] = rfm.apply(assign_segment, axis=1)
    # Ajouter aussi 'Segment' pour compatibilit√©
    rfm['Segment'] = rfm['RFM_Segment']

    return rfm


# ==============================================================================
# CALCUL DE LA CUSTOMER LIFETIME VALUE (CLV)
# ==============================================================================

def calculate_clv_empirical(df: pd.DataFrame, period_months: int = 12) -> pd.DataFrame:
    """
    Calcule la Customer Lifetime Value empirique bas√©e sur les donn√©es historiques.

    Cette m√©thode calcule la CLV en se basant sur l'analyse des cohortes :
    - Valeur moyenne par transaction
    - Nombre moyen de transactions par p√©riode
    - Taux de r√©tention observ√© par cohorte

    Parameters
    ----------
    df : pd.DataFrame
        DataFrame avec les donn√©es de cohortes
    period_months : int, default=12
        P√©riode d'observation en mois

    Returns
    -------
    pd.DataFrame
        DataFrame avec la CLV par cohorte/segment contenant :
        - CohortMonth ou Segment
        - Average Order Value (AOV)
        - Purchase Frequency
        - Customer Value
        - Retention Rate
        - CLV

    Examples
    --------
    >>> clv_empirical = calculate_clv_empirical(df_cohorts, period_months=12)
    >>> print(f"CLV moyenne : {clv_empirical['CLV'].mean():.2f}")

    Notes
    -----
    Formule simplifi√©e : CLV = AOV * Frequency * Retention Rate
    Cette approche utilise les donn√©es historiques r√©elles
    """
    # Filtrer les ventes (hors retours)
    df_sales = df[~df['IsReturn']].copy()

    # Calculer les m√©triques par client sur la p√©riode
    clv_data = df_sales.groupby('Customer ID').agg({
        'TotalAmount': 'sum',  # CA total
        'Invoice': 'nunique',  # Nombre de transactions
        'InvoiceDate': ['min', 'max']  # Dates premi√®re et derni√®re transaction
    }).reset_index()

    # Aplatir les colonnes multi-index
    clv_data.columns = ['Customer ID', 'Total_Revenue', 'Num_Transactions', 'First_Purchase', 'Last_Purchase']

    # Calculer le nombre de jours depuis la derni√®re transaction
    max_date = df['InvoiceDate'].max()
    clv_data['Last_Purchase_Days'] = (max_date - clv_data['Last_Purchase']).dt.days

    # Calculer l'AOV (Average Order Value)
    clv_data['AOV'] = clv_data['Total_Revenue'] / clv_data['Num_Transactions']

    # Calculer la dur√©e de vie en mois (du premier au dernier achat)
    clv_data['Lifespan_Days'] = (clv_data['Last_Purchase'] - clv_data['First_Purchase']).dt.days
    clv_data['Lifespan_Months'] = clv_data['Lifespan_Days'] / 30.44  # Moyenne jours/mois

    # √âviter division par z√©ro : si lifespan = 0, utiliser 1 mois
    clv_data['Lifespan_Months'] = clv_data['Lifespan_Months'].replace(0, 1)

    # Calculer la fr√©quence mensuelle (transactions par mois)
    clv_data['Monthly_Frequency'] = clv_data['Num_Transactions'] / clv_data['Lifespan_Months']

    # CLV empirique = CA observ√© sur la p√©riode
    clv_data['CLV_Empirical'] = clv_data['Total_Revenue']

    # S√©lectionner les colonnes pertinentes
    result = clv_data[[
        'Customer ID',
        'CLV_Empirical',
        'Num_Transactions',
        'AOV',
        'Last_Purchase_Days'
    ]].rename(columns={
        'Num_Transactions': 'nb_transactions',
        'AOV': 'avg_basket',
        'Last_Purchase_Days': 'last_purchase_days'
    })

    return result


def calculate_clv_formula(
    df: pd.DataFrame,
    retention_rate: Optional[float] = None,
    discount_rate: Optional[float] = None,
    forecast_periods: int = 36
) -> pd.DataFrame:
    """
    Calcule la Customer Lifetime Value pr√©dictive avec la formule classique.

    Cette m√©thode utilise la formule th√©orique de CLV :
    CLV = (AOV * Purchase Frequency * Gross Margin) / (1 + Discount Rate - Retention Rate)

    Parameters
    ----------
    df : pd.DataFrame
        DataFrame contenant les donn√©es clients
    retention_rate : float, optional
        Taux de r√©tention (0-1). Si None, utilise config.DEFAULT_RETENTION_RATE
    discount_rate : float, optional
        Taux d'actualisation (0-1). Si None, utilise config.DEFAULT_DISCOUNT_RATE
    forecast_periods : int, default=36
        Nombre de p√©riodes (mois) pour la pr√©vision

    Returns
    -------
    pd.DataFrame
        DataFrame avec la CLV calcul√©e par client

    Examples
    --------
    >>> clv_formula = calculate_clv_formula(
    ...     df_clean,
    ...     retention_rate=0.35,
    ...     discount_rate=0.10
    ... )
    >>> print(f"CLV totale pr√©vue : {clv_formula['CLV'].sum():,.2f}")

    Notes
    -----
    Cette approche est pr√©dictive et bas√©e sur des hypoth√®ses
    Les taux peuvent √™tre ajust√©s selon le contexte business
    """
    # Utiliser les valeurs par d√©faut si non sp√©cifi√©es
    if retention_rate is None:
        retention_rate = config.DEFAULT_RETENTION_RATE
    if discount_rate is None:
        discount_rate = config.DEFAULT_DISCOUNT_RATE

    # Filtrer les ventes (hors retours)
    df_sales = df[~df['IsReturn']].copy()

    # Calculer les m√©triques par client
    clv_data = df_sales.groupby('Customer ID').agg({
        'TotalAmount': 'sum',
        'Invoice': 'nunique',
        'InvoiceDate': ['min', 'max']
    }).reset_index()

    clv_data.columns = ['Customer ID', 'Total_Revenue', 'Num_Transactions', 'First_Purchase', 'Last_Purchase']

    # Calculer la dur√©e de vie en mois
    clv_data['Lifespan_Days'] = (clv_data['Last_Purchase'] - clv_data['First_Purchase']).dt.days
    clv_data['Lifespan_Months'] = clv_data['Lifespan_Days'] / 30.44
    clv_data['Lifespan_Months'] = clv_data['Lifespan_Months'].replace(0, 1)

    # Calculer l'AOV et la fr√©quence mensuelle
    clv_data['AOV'] = clv_data['Total_Revenue'] / clv_data['Num_Transactions']
    clv_data['Monthly_Frequency'] = clv_data['Num_Transactions'] / clv_data['Lifespan_Months']
    clv_data['Monthly_Revenue'] = clv_data['AOV'] * clv_data['Monthly_Frequency']

    # Formule CLV : Marge √ó (r / (1 + d - r))
    # O√π Marge = revenu mensuel moyen, r = taux de r√©tention, d = taux d'actualisation
    clv_data['CLV_Formula'] = clv_data['Monthly_Revenue'] * (
        retention_rate / (1 + discount_rate - retention_rate)
    )

    # S√©lectionner les colonnes pertinentes
    result = clv_data[[
        'Customer ID',
        'CLV_Formula',
        'Monthly_Revenue'
    ]].rename(columns={
        'Monthly_Revenue': 'monthly_avg_revenue'
    })

    return result


# ==============================================================================
# FILTRAGE ET TRANSFORMATION
# ==============================================================================

def apply_filters(df: pd.DataFrame, filters_dict: Dict) -> pd.DataFrame:
    """
    Applique un ensemble de filtres au DataFrame.

    Les filtres possibles incluent :
    - Plages de dates
    - Pays
    - Unit√© temporelle (mois / trimestre)
    - Type de client
    - Valeur mini de commande
    - Mode retours : Inclure / Exclure / Neutraliser
    """

    df_filtered = df.copy()

    # -------------------------
    # 1) FILTRE : DATES
    # -------------------------
    if 'start_date' in filters_dict and filters_dict['start_date'] is not None:
        start_date = pd.to_datetime(filters_dict['start_date'])
        df_filtered = df_filtered[df_filtered['InvoiceDate'] >= start_date]

    if 'end_date' in filters_dict and filters_dict['end_date'] is not None:
        end_date = pd.to_datetime(filters_dict['end_date'])
        df_filtered = df_filtered[df_filtered['InvoiceDate'] <= end_date]

    # Compatibilit√© ancien format
    if 'date_range' in filters_dict and filters_dict['date_range'] is not None:
        start_date, end_date = filters_dict['date_range']
        if start_date:
            df_filtered = df_filtered[df_filtered['InvoiceDate'] >= pd.to_datetime(start_date)]
        if end_date:
            df_filtered = df_filtered[df_filtered['InvoiceDate'] <= pd.to_datetime(end_date)]

    # -------------------------
    # 2) FILTRE : PAYS
    # -------------------------
    if 'countries' in filters_dict and filters_dict['countries']:
        df_filtered = df_filtered[df_filtered['Country'].isin(filters_dict['countries'])]

    # -------------------------
    # 3) FILTRE : UNITE TEMPORELLE (mois / trimestre)
    # -------------------------
    if filters_dict.get("time_unit") == "Mois":
        df_filtered["TimeUnit"] = df_filtered["InvoiceDate"].dt.to_period("M")
    elif filters_dict.get("time_unit") == "Trimestre":
        df_filtered["TimeUnit"] = df_filtered["InvoiceDate"].dt.to_period("Q")

    # -------------------------
    # 4) FILTRE : TYPE CLIENT (B2B / B2C)
    # -------------------------
    customer_type = filters_dict.get("customer_type")
    if customer_type and customer_type != "Tous":
        if "CustomerType" in df_filtered.columns:
            df_filtered = df_filtered[df_filtered["CustomerType"] == customer_type]

    # -------------------------
    # 5) FILTRE : MONTANT MIN DE COMMANDE
    # -------------------------
    if 'min_order_value' in filters_dict and filters_dict['min_order_value'] is not None:
        df_filtered = df_filtered[df_filtered['TotalAmount'] >= filters_dict['min_order_value']]

    # Compatibilit√© ancien ‚Äúmin_amount‚Äù
    if 'min_amount' in filters_dict and filters_dict['min_amount'] is not None:
        df_filtered = df_filtered[df_filtered['TotalAmount'] >= filters_dict['min_amount']]

    # -------------------------
    # 6) FILTRE : LISTE DE CLIENTS
    # -------------------------
    if 'customer_ids' in filters_dict and filters_dict['customer_ids']:
        df_filtered = df_filtered[df_filtered['Customer ID'].isin(filters_dict['customer_ids'])]

    # -------------------------
    # 7) FILTRE : SEGMENTS RFM
    # -------------------------
    if 'segments' in filters_dict and filters_dict['segments']:
        if 'RFM_Segment' in df_filtered.columns:
            df_filtered = df_filtered[df_filtered['RFM_Segment'].isin(filters_dict['segments'])]

    # ============================================================
    # 8) üî• MODE RETOURS (NOUVEAU)
    # ============================================================
    mode = filters_dict.get("returns_mode", "Inclure")

    if mode == "Exclure":
        # On enl√®ve toutes les lignes retours
        df_filtered = df_filtered[~df_filtered["IsReturn"]]

    elif mode == "Neutraliser":
        # Les retours n'annulent pas les ventes, mais leur montant devient 0
        df_filtered["TotalAmount"] = df_filtered["TotalAmount"].where(
            ~df_filtered["IsReturn"],
            -df_filtered["TotalAmount"]  # neutralisation
        )

    # ============================================================
    # 9) üî• TYPE CLIENT (VERSION PRO)
    # ============================================================
    customer_type = filters_dict.get("customer_type")
    if customer_type and customer_type != "Tous":
        if "CustomerType" in df_filtered.columns:
            df_filtered = df_filtered[df_filtered["CustomerType"] == customer_type]

    return df_filtered

# ==============================================================================
# SIMULATION DE SCENARIOS
# ==============================================================================

def simulate_scenario(df: pd.DataFrame, params: Dict) -> Dict:
    """
    Simule l'impact de diff√©rents sc√©narios marketing sur les KPIs.

    Cette fonction permet de mod√©liser l'effet de changements dans :
    - Taux de r√©tention
    - Valeur moyenne des commandes (AOV)
    - Fr√©quence d'achat
    - Taille de la base client

    Parameters
    ----------
    df : pd.DataFrame
        DataFrame de base pour la simulation
    params : dict
        Param√®tres du sc√©nario :
        {
            'retention_increase': 0.10,  # +10%
            'aov_increase': 0.15,        # +15%
            'frequency_increase': 0.20,  # +20%
            'customer_growth': 0.05      # +5%
        }

    Returns
    -------
    dict
        R√©sultats de la simulation :
        {
            'current': {
                'revenue': float,
                'customers': int,
                'clv': float,
                ...
            },
            'projected': {
                'revenue': float,
                'customers': int,
                'clv': float,
                ...
            },
            'delta': {
                'revenue': float,
                'customers': int,
                'clv': float,
                ...
            }
        }

    Examples
    --------
    >>> scenario = {
    ...     'retention_increase': 0.10,
    ...     'aov_increase': 0.05
    ... }
    >>> results = simulate_scenario(df_clean, scenario)
    >>> print(f"Augmentation revenue : {results['delta']['revenue']:,.2f}")

    Notes
    -----
    Les sc√©narios pr√©d√©finis sont dans config.SIMULATION_SCENARIOS
    Les r√©sultats sont des projections bas√©es sur des hypoth√®ses
    """
    # Extraire les param√®tres avec valeurs par d√©faut
    margin_pct = params.get('margin_pct', 0.4)
    retention_delta = params.get('retention_delta', params.get('retention_increase', 0.0))
    discount_pct = params.get('discount_pct', 0.0)
    target_segment = params.get('target_segment', None)
    aov_increase = params.get('aov_increase', 0.0)
    frequency_increase = params.get('frequency_increase', 0.0)
    customer_growth = params.get('customer_growth', 0.0)

    # Filtrer les ventes (hors retours)
    df_sales = df[~df['IsReturn']].copy()

    # Si un segment est cibl√©, filtrer
    if target_segment and 'RFM_Segment' in df_sales.columns:
        df_target = df_sales[df_sales['RFM_Segment'] == target_segment]
    else:
        df_target = df_sales

    # Calculer les KPIs actuels
    current_revenue = df_target['TotalAmount'].sum()
    current_customers = df_target['Customer ID'].nunique()
    current_transactions = df_target['Invoice'].nunique()
    current_aov = current_revenue / current_transactions if current_transactions > 0 else 0
    current_frequency = current_transactions / current_customers if current_customers > 0 else 0

    # Calculer la CLV actuelle
    current_retention_rate = config.DEFAULT_RETENTION_RATE
    current_discount_rate = config.DEFAULT_DISCOUNT_RATE
    current_monthly_revenue = current_revenue / 12  # Approximation
    current_clv = current_monthly_revenue * (current_retention_rate / (1 + current_discount_rate - current_retention_rate))

    # Projections avec les changements
    projected_customers = current_customers * (1 + customer_growth)
    projected_aov = current_aov * (1 + aov_increase) * (1 - discount_pct)
    projected_frequency = current_frequency * (1 + frequency_increase)
    projected_transactions = projected_customers * projected_frequency
    projected_revenue = projected_aov * projected_transactions

    # CLV projet√©e
    projected_retention_rate = min(current_retention_rate + retention_delta, 0.95)  # Cap √† 95%
    projected_monthly_revenue = projected_revenue / 12
    projected_clv = projected_monthly_revenue * (projected_retention_rate / (1 + current_discount_rate - projected_retention_rate))

    # Calculer les deltas
    delta_revenue = projected_revenue - current_revenue
    delta_customers = projected_customers - current_customers
    delta_clv = projected_clv - current_clv
    delta_revenue_pct = (delta_revenue / current_revenue * 100) if current_revenue > 0 else 0

    # Construire le r√©sultat
    results = {
        'current': {
            'revenue': current_revenue,
            'customers': int(current_customers),
            'transactions': int(current_transactions),
            'aov': current_aov,
            'frequency': current_frequency,
            'retention_rate': current_retention_rate,
            'clv': current_clv
        },
        'projected': {
            'revenue': projected_revenue,
            'customers': int(projected_customers),
            'transactions': int(projected_transactions),
            'aov': projected_aov,
            'frequency': projected_frequency,
            'retention_rate': projected_retention_rate,
            'clv': projected_clv
        },
        'delta': {
            'revenue': delta_revenue,
            'revenue_pct': delta_revenue_pct,
            'customers': int(delta_customers),
            'clv': delta_clv
        },
        'params': params
    }

    return results


# ==============================================================================
# EXPORT DE DONNEES
# ==============================================================================

def export_to_csv(df: pd.DataFrame, filename: str, directory: Optional[Path] = None) -> str:
    """
    Exporte un DataFrame au format CSV.

    Cette fonction g√®re l'export de donn√©es avec les bonnes configurations :
    - Encodage UTF-8
    - S√©parateur selon config
    - Formats de dates standardis√©s
    - Cr√©ation automatique des r√©pertoires

    Parameters
    ----------
    df : pd.DataFrame
        DataFrame √† exporter
    filename : str
        Nom du fichier (avec ou sans extension .csv)
    directory : Path, optional
        R√©pertoire de destination. Si None, utilise config.EXPORT_DATA_DIR

    Returns
    -------
    str
        Chemin complet du fichier cr√©√©

    Examples
    --------
    >>> file_path = export_to_csv(rfm_df, 'rfm_analysis_2024.csv')
    >>> print(f"Fichier export√© : {file_path}")

    Notes
    -----
    Le r√©pertoire de destination est cr√©√© s'il n'existe pas
    Les fichiers existants sont √©cras√©s sans avertissement
    """
    # Utiliser le r√©pertoire par d√©faut si non sp√©cifi√©
    if directory is None:
        directory = config.PROCESSED_DATA_DIR

    # Convertir en Path si n√©cessaire
    directory = Path(directory)

    # Cr√©er le r√©pertoire s'il n'existe pas
    directory.mkdir(parents=True, exist_ok=True)

    # Ajouter l'extension .csv si absente
    if not filename.endswith('.csv'):
        filename += '.csv'

    # Construire le chemin complet
    file_path = directory / filename

    try:
        # Exporter le DataFrame
        df.to_csv(
            file_path,
            index=False,
            encoding=config.FILE_ENCODING,
            sep=config.CSV_SEPARATOR,
            date_format=config.EXPORT_DATETIME_FORMAT
        )

        return str(file_path.absolute())

    except Exception as e:
        raise IOError(f"Erreur lors de l'export vers {file_path}: {str(e)}")


def export_chart_to_png(
    fig: Union[plt.Figure, go.Figure],
    filename: str,
    directory: Optional[Path] = None,
    dpi: int = 300
) -> str:
    """
    Exporte un graphique (Matplotlib ou Plotly) au format PNG.

    Cette fonction unifie l'export de visualisations, qu'elles soient cr√©√©es
    avec Matplotlib ou Plotly, en g√©rant automatiquement le format appropri√©.

    Parameters
    ----------
    fig : matplotlib.figure.Figure or plotly.graph_objects.Figure
        Figure √† exporter
    filename : str
        Nom du fichier (avec ou sans extension .png)
    directory : Path, optional
        R√©pertoire de destination. Si None, utilise config.EXPORT_CHARTS_DIR
    dpi : int, default=300
        R√©solution pour les exports Matplotlib (DPI)

    Returns
    -------
    str
        Chemin complet du fichier cr√©√©

    Examples
    --------
    >>> import matplotlib.pyplot as plt
    >>> fig, ax = plt.subplots()
    >>> ax.plot([1, 2, 3], [1, 4, 9])
    >>> file_path = export_chart_to_png(fig, 'my_chart.png')

    Notes
    -----
    Format Plotly : utilise write_image()
    Format Matplotlib : utilise savefig() avec bbox_inches='tight'
    """
    # Utiliser le r√©pertoire par d√©faut si non sp√©cifi√©
    if directory is None:
        directory = config.EXPORT_CHARTS_DIR

    # Convertir en Path si n√©cessaire
    directory = Path(directory)

    # Cr√©er le r√©pertoire s'il n'existe pas
    directory.mkdir(parents=True, exist_ok=True)

    # Ajouter l'extension .png si absente
    if not filename.endswith('.png'):
        filename += '.png'

    # Construire le chemin complet
    file_path = directory / filename

    try:
        # D√©tecter le type de figure et exporter
        if isinstance(fig, plt.Figure):
            # Export Matplotlib
            fig.savefig(
                file_path,
                dpi=dpi,
                bbox_inches='tight',
                facecolor='white',
                edgecolor='none'
            )
        elif isinstance(fig, go.Figure):
            # Export Plotly
            fig.write_image(
                file_path,
                width=1200,
                height=600,
                scale=2  # Haute r√©solution
            )
        else:
            raise TypeError(f"Type de figure non support√© : {type(fig)}. Attendu : matplotlib.figure.Figure ou plotly.graph_objects.Figure")

        return str(file_path.absolute())

    except Exception as e:
        raise IOError(f"Erreur lors de l'export du graphique vers {file_path}: {str(e)}")


# ==============================================================================
# CALCULS DE METRIQUES
# ==============================================================================

def calculate_kpis(df: pd.DataFrame) -> Dict[str, Union[float, int]]:
    """
    Calcule les principaux KPIs m√©tier.

    Cette fonction centralise le calcul de tous les indicateurs cl√©s :
    - Nombre total de clients
    - Revenu total
    - Panier moyen (Average Order Value)
    - Fr√©quence d'achat moyenne
    - Taux de r√©tention global
    - CLV moyenne
    - Nombre de transactions

    Parameters
    ----------
    df : pd.DataFrame
        DataFrame des transactions

    Returns
    -------
    dict
        Dictionnaire des KPIs :
        {
            'total_customers': int,
            'total_revenue': float,
            'avg_order_value': float,
            'purchase_frequency': float,
            'retention_rate': float,
            'avg_clv': float,
            'total_transactions': int
        }

    Examples
    --------
    >>> kpis = calculate_kpis(df_clean)
    >>> for key, value in kpis.items():
    ...     print(f"{key}: {value:,.2f}")
    """
    # Filtrer les ventes (hors retours)
    df_sales = df[~df['IsReturn']].copy()

    # KPIs de base
    total_customers = df_sales['Customer ID'].nunique()
    total_revenue = df_sales['TotalAmount'].sum()
    total_transactions = df_sales['Invoice'].nunique()

    # Panier moyen
    avg_order_value = total_revenue / total_transactions if total_transactions > 0 else 0

    # Fr√©quence d'achat moyenne (transactions par client)
    purchase_frequency = total_transactions / total_customers if total_customers > 0 else 0

    # Calculer le taux de r√©tention (clients qui reviennent au moins une fois)
    customer_transaction_counts = df_sales.groupby('Customer ID')['Invoice'].nunique()
    repeat_customers = (customer_transaction_counts > 1).sum()
    retention_rate = repeat_customers / total_customers if total_customers > 0 else 0

    # Taux de r√©tention √† M+1 et M+3
    if 'CohortIndex' in df_sales.columns:
        # Pour M+1 : clients qui ont fait une transaction au moins 1 mois apr√®s leur premi√®re
        customers_m1 = df_sales[df_sales['CohortIndex'] >= 1]['Customer ID'].nunique()
        retention_rate_m1 = customers_m1 / total_customers if total_customers > 0 else 0

        # Pour M+3
        customers_m3 = df_sales[df_sales['CohortIndex'] >= 3]['Customer ID'].nunique()
        retention_rate_m3 = customers_m3 / total_customers if total_customers > 0 else 0
    else:
        retention_rate_m1 = retention_rate
        retention_rate_m3 = retention_rate * 0.7  # Approximation

    # Taux de retour
    total_returns = df[df['IsReturn']]['Invoice'].nunique()
    return_rate = total_returns / (total_transactions + total_returns) if (total_transactions + total_returns) > 0 else 0

    # Clients actifs (derniers 90 jours)
    max_date = df['InvoiceDate'].max()
    cutoff_date = max_date - timedelta(days=90)
    active_customers = df_sales[df_sales['InvoiceDate'] >= cutoff_date]['Customer ID'].nunique()

    # CLV moyenne (bas√©e sur la formule simple)
    avg_clv = (avg_order_value * purchase_frequency) / (1 - retention_rate + 0.1) if retention_rate < 0.9 else avg_order_value * purchase_frequency * 10

    # Construire le dictionnaire de KPIs
    kpis = {
        'total_customers': int(total_customers),
        'total_revenue': float(total_revenue),
        'total_transactions': int(total_transactions),
        'avg_order_value': float(avg_order_value),
        'purchase_frequency': float(purchase_frequency),
        'retention_rate': float(retention_rate),
        'retention_rate_m1': float(retention_rate_m1),
        'retention_rate_m3': float(retention_rate_m3),
        'return_rate': float(return_rate),
        'active_customers': int(active_customers),
        'avg_clv': float(avg_clv)
    }

    return kpis


def calculate_churn_rate(df: pd.DataFrame, inactive_months: int = 6) -> float:
    """
    Calcule le taux de churn (attrition) des clients.

    Un client est consid√©r√© comme churn√© s'il n'a pas effectu√© de transaction
    depuis un nombre de mois d√©fini.

    Parameters
    ----------
    df : pd.DataFrame
        DataFrame des transactions
    inactive_months : int, default=6
        Nombre de mois d'inactivit√© pour consid√©rer un client churn√©

    Returns
    -------
    float
        Taux de churn (0-1)

    Examples
    --------
    >>> churn = calculate_churn_rate(df_clean, inactive_months=6)
    >>> print(f"Taux de churn : {churn:.1%}")
    """
    # Filtrer les ventes (hors retours)
    df_sales = df[~df['IsReturn']].copy()

    # Date de r√©f√©rence (date maximale dans les donn√©es)
    max_date = df_sales['InvoiceDate'].max()

    # Date de cutoff pour le churn
    cutoff_date = max_date - timedelta(days=inactive_months * 30)

    # Identifier la derni√®re transaction de chaque client
    last_purchase = df_sales.groupby('Customer ID')['InvoiceDate'].max()

    # Clients churn√©s : derni√®re transaction avant la date de cutoff
    churned_customers = (last_purchase < cutoff_date).sum()

    # Total de clients
    total_customers = df_sales['Customer ID'].nunique()

    # Taux de churn
    churn_rate = churned_customers / total_customers if total_customers > 0 else 0

    return float(churn_rate)


def get_churn_predictions(rfm_df: pd.DataFrame) -> pd.DataFrame:
    """
    Identifie les clients √† risque de churn bas√© sur leur profil RFM.

    Cette fonction analyse les scores RFM pour identifier les clients
    susceptibles de churner et leur attribue une probabilit√© et un niveau de risque.

    Parameters
    ----------
    rfm_df : pd.DataFrame
        DataFrame RFM avec scores et segments

    Returns
    -------
    pd.DataFrame
        DataFrame avec colonnes :
        - Customer ID
        - churn_probability : probabilit√© de churn (0-1)
        - risk_level : niveau de risque ('Low', 'Medium', 'High', 'Critical')

    Examples
    --------
    >>> rfm = calculate_rfm(df_clean)
    >>> churn_pred = get_churn_predictions(rfm)
    >>> high_risk = churn_pred[churn_pred['risk_level'] == 'High']
    >>> print(f"Clients √† haut risque : {len(high_risk)}")

    Notes
    -----
    Les r√®gles sont bas√©es sur les scores RFM :
    - R_score faible = risque √©lev√©
    - Segments "At Risk", "Lost", "Cannot Lose Them" = priorit√© haute
    """
    # Copier pour √©viter de modifier l'original
    churn_df = rfm_df.copy()

    # Calculer la probabilit√© de churn bas√©e sur les scores RFM
    # Formule : plus R est faible, plus la probabilit√© est √©lev√©e
    # Ajuster avec F et M pour affiner
    churn_df['churn_probability'] = (
        (5 - churn_df['R_Score']) * 0.6 +  # Recency p√®se 60%
        (5 - churn_df['F_Score']) * 0.2 +  # Frequency p√®se 20%
        (5 - churn_df['M_Score']) * 0.2    # Monetary p√®se 20%
    ) / 4  # Normaliser entre 0 et 1

    # D√©finir le niveau de risque
    def assign_risk_level(row):
        prob = row['churn_probability']
        segment = row['RFM_Segment']

        # Segments critiques
        if segment in ['Lost', 'Cannot Lose Them']:
            return 'Critical'
        # Segments √† haut risque
        elif segment in ['At Risk', 'Hibernating'] or prob >= 0.7:
            return 'High'
        # Risque moyen
        elif prob >= 0.4:
            return 'Medium'
        # Faible risque
        else:
            return 'Low'

    churn_df['risk_level'] = churn_df.apply(assign_risk_level, axis=1)

    # S√©lectionner les colonnes pertinentes
    result = churn_df[['Customer ID', 'churn_probability', 'risk_level']]

    # Trier par probabilit√© d√©croissante
    result = result.sort_values('churn_probability', ascending=False).reset_index(drop=True)

    return result


# ==============================================================================
# UTILITAIRES DE VALIDATION
# ==============================================================================

def validate_dataframe(df: pd.DataFrame, required_columns: List[str]) -> Tuple[bool, List[str]]:
    """
    Valide qu'un DataFrame contient toutes les colonnes requises.

    Parameters
    ----------
    df : pd.DataFrame
        DataFrame √† valider
    required_columns : list of str
        Liste des colonnes requises

    Returns
    -------
    tuple of (bool, list)
        (validation_ok, missing_columns)

    Examples
    --------
    >>> valid, missing = validate_dataframe(df, config.REQUIRED_COLUMNS)
    >>> if not valid:
    ...     print(f"Colonnes manquantes : {missing}")
    """
    # Identifier les colonnes manquantes
    missing_columns = [col for col in required_columns if col not in df.columns]

    # Validation OK si aucune colonne manquante
    is_valid = len(missing_columns) == 0

    return is_valid, missing_columns


# ==============================================================================
# UTILITAIRES DE FORMATAGE
# ==============================================================================

def format_currency(amount: float, currency: str = "GBP") -> str:
    """
    Formate un montant en devise.

    Parameters
    ----------
    amount : float
        Montant √† formater
    currency : str, default="GBP"
        Code de la devise (GBP, EUR, USD, etc.)

    Returns
    -------
    str
        Montant format√© (ex: "¬£1,234.56")

    Examples
    --------
    >>> print(format_currency(1234.56))
    ¬£1,234.56
    """
    # Dictionnaire des symboles de devises
    currency_symbols = {
        'GBP': '¬£',
        'EUR': '‚Ç¨',
        'USD': '$',
        'JPY': '¬•',
        'CHF': 'CHF '
    }

    # Obtenir le symbole (par d√©faut, utiliser le code de devise)
    symbol = currency_symbols.get(currency.upper(), currency + ' ')

    # Formater avec s√©parateurs de milliers
    formatted = f"{symbol}{amount:,.2f}"

    return formatted


def format_percentage(value: float, decimals: int = 1) -> str:
    """
    Formate un nombre en pourcentage.

    Parameters
    ----------
    value : float
        Valeur √† formater (0-1)
    decimals : int, default=1
        Nombre de d√©cimales

    Returns
    -------
    str
        Pourcentage format√© (ex: "25.5%")

    Examples
    --------
    >>> print(format_percentage(0.255))
    25.5%
    """
    # Convertir en pourcentage et formater
    percentage = value * 100
    formatted = f"{percentage:.{decimals}f}%"

    return formatted


# ==============================================================================
# MODULE INFO
# ==============================================================================

if __name__ == "__main__":
    print("Module utils.py - Fonctions utilitaires")
    print("=" * 70)
    print("Ce module contient toutes les fonctions m√©tier de l'application")
    print("Pour utiliser ces fonctions, importez-les dans vos scripts ou pages")
    print("=" * 70)
