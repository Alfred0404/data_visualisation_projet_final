{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Exploration des Donn√©es - Online Retail II\n",
    "\n",
    "**Auteur:** Projet Marketing Analytics  \n",
    "**Date:** 2024  \n",
    "**Objectif:** Analyse exploratoire du dataset Online Retail II pour comprendre les patterns de vente et le comportement client\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "context",
   "metadata": {},
   "source": [
    "## 1. Introduction et Contexte du Projet\n",
    "\n",
    "### 1.1 Contexte Business\n",
    "\n",
    "Ce projet vise √† cr√©er une application d'aide √† la d√©cision marketing bas√©e sur l'analyse des donn√©es transactionnelles d'un retailer en ligne. L'objectif est de fournir des insights actionnables pour optimiser les strat√©gies d'acquisition, de fid√©lisation et de valorisation client.\n",
    "\n",
    "### 1.2 Objectifs de l'Analyse\n",
    "\n",
    "- **Comprendre la structure des donn√©es** : identifier les variables cl√©s et leur qualit√©\n",
    "- **Analyser les patterns de vente** : tendances temporelles, saisonnalit√©, volumes\n",
    "- **Profiler les clients** : comportements d'achat, segmentation naturelle\n",
    "- **Identifier les opportunit√©s** : segments √† fort potentiel, produits cl√©s\n",
    "- **Pr√©parer les donn√©es** : nettoyage et transformation pour les analyses avanc√©es\n",
    "\n",
    "### 1.3 Questions d'Analyse\n",
    "\n",
    "1. Quelle est la qualit√© des donn√©es (valeurs manquantes, doublons, aberrations) ?\n",
    "2. Comment √©voluent les ventes dans le temps (tendance, saisonnalit√©) ?\n",
    "3. Quelle est la distribution g√©ographique des clients et du revenu ?\n",
    "4. Quels sont les produits les plus vendus ?\n",
    "5. Quel est le comportement d'achat typique (fr√©quence, montant) ?\n",
    "6. Y a-t-il des segments de clients naturellement distincts ?\n",
    "7. Quel est le taux d'annulation et son impact ?\n",
    "8. Comment pr√©parer les donn√©es pour l'analyse de cohortes et RFM ?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imports",
   "metadata": {},
   "source": [
    "## 2. Imports et Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports standard\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Visualisation\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Configuration\n",
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.append(str(Path.cwd().parent))\n",
    "import config\n",
    "\n",
    "# Configuration des graphiques\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline\n",
    "\n",
    "# Options d'affichage pandas\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.float_format', '{:.2f}'.format)\n",
    "\n",
    "print(\"‚úÖ Imports r√©alis√©s avec succ√®s\")\n",
    "print(f\"Version pandas: {pd.__version__}\")\n",
    "print(f\"Version numpy: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loading",
   "metadata": {},
   "source": [
    "## 3. Chargement et Aper√ßu des Donn√©es\n",
    "\n",
    "### 3.1 Chargement du Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loading_code",
   "metadata": {},
   "outputs": [],
   "source": "# Chargement du dataset\n# Le fichier utilise le point-virgule comme s√©parateur et la virgule comme s√©parateur d√©cimal\ndf = pd.read_csv(\n    '../data/raw/online_retail_II.csv',\n    sep=';',\n    decimal=',',\n    encoding='utf-8',\n    parse_dates=['InvoiceDate'],\n    dayfirst=True\n)\n\n# Informations de base\nprint(f\"‚úÖ Dataset charg√© avec succ√®s\")\nprint(f\"Nombre de lignes: {len(df):,}\")\nprint(f\"Nombre de colonnes: {len(df.columns)}\")\nprint(f\"M√©moire utilis√©e: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\nprint(f\"\\nP√©riode des donn√©es:\")\nprint(f\"  - Date minimale: {df['InvoiceDate'].min()}\")\nprint(f\"  - Date maximale: {df['InvoiceDate'].max()}\")\nprint(f\"  - Dur√©e: {(df['InvoiceDate'].max() - df['InvoiceDate'].min()).days} jours\")"
  },
  {
   "cell_type": "markdown",
   "id": "structure",
   "metadata": {},
   "source": [
    "### 3.2 Structure du Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "structure_code",
   "metadata": {},
   "outputs": [],
   "source": "# Affichage des premi√®res lignes\ndf.head(10)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "info_code",
   "metadata": {},
   "outputs": [],
   "source": "# Informations d√©taill√©es sur les types de donn√©es\nprint(\"=== INFORMATIONS SUR LE DATASET ===\\n\")\ndf.info()\n\nprint(\"\\n=== TYPES DE DONN√âES ===\")\nprint(df.dtypes)\n\nprint(\"\\n=== STATISTIQUES DESCRIPTIVES ===\")\ndf.describe(include='all')"
  },
  {
   "cell_type": "markdown",
   "id": "dictionary",
   "metadata": {},
   "source": "## 3. Dictionnaire des Variables\n\nCr√©ation d'un dictionnaire d√©taill√© d√©crivant chaque variable du dataset."
  },
  {
   "cell_type": "code",
   "id": "44157v0eu1l",
   "source": "# Cr√©ation du dictionnaire des variables\ndata_dict = {\n    'Variable': ['Invoice', 'StockCode', 'Description', 'Quantity', 'InvoiceDate', 'Price', 'Customer ID', 'Country'],\n    'Type': ['object (string)', 'object (string)', 'object (string)', 'int64', 'datetime64', 'float64', 'float64', 'object (string)'],\n    'S√©mantique': [\n        'Num√©ro de facture unique',\n        'Code produit unique',\n        'Nom/description du produit',\n        'Quantit√© de produits achet√©s',\n        'Date et heure de la transaction',\n        'Prix unitaire du produit',\n        'Identifiant client unique',\n        'Pays de r√©sidence du client'\n    ],\n    'Unit√©s/Valeurs': [\n        'Alphanum√©rique, \"C\" pr√©fixe = annulation',\n        'Alphanum√©rique',\n        'Texte libre',\n        'Entier positif (peut √™tre n√©gatif pour annulations)',\n        'Format: JJ/MM/AAAA HH:MM',\n        'Livres Sterling (¬£)',\n        'Num√©rique entier',\n        'Nom de pays en anglais'\n    ],\n    'Exemple': [\n        '536365, C536365',\n        '85123A, 71053',\n        'WHITE HANGING HEART T-LIGHT HOLDER',\n        '6, -6',\n        '01/12/2010 08:26',\n        '2.55, 3.39',\n        '17850.0',\n        'United Kingdom, France'\n    ],\n    'Observations': [\n        'Cl√© pour identifier les transactions',\n        'Permet de tracker les produits',\n        'Peut contenir des valeurs manquantes',\n        'N√©gatif indique une annulation',\n        'Pas de valeurs manquantes attendues',\n        'Peut √™tre 0 ou n√©gatif (√† v√©rifier)',\n        'Nombreuses valeurs manquantes possibles',\n        'Dimension g√©ographique cl√©'\n    ]\n}\n\ndict_df = pd.DataFrame(data_dict)\n\n# Affichage stylis√©\nprint(\"=\"*100)\nprint(\"DICTIONNAIRE DES VARIABLES - ONLINE RETAIL II\".center(100))\nprint(\"=\"*100)\ndisplay(dict_df.style.set_properties(**{\n    'text-align': 'left',\n    'white-space': 'pre-wrap'\n}).set_table_styles([\n    {'selector': 'th', 'props': [('background-color', '#4CAF50'), ('color', 'white'), ('font-weight', 'bold'), ('text-align', 'center')]},\n    {'selector': 'td', 'props': [('padding', '10px')]}\n]))\n\nprint(\"\\n\\n=== VARIABLES √Ä CR√âER POUR L'ANALYSE ===\")\nprint(\"\"\"\n1. TotalAmount: Quantity √ó Price (montant total de la ligne de transaction)\n2. InvoiceMonth: Mois extrait de InvoiceDate (pour agr√©gations temporelles)\n3. InvoiceYear: Ann√©e extraite de InvoiceDate\n4. InvoiceYearMonth: Combinaison Ann√©e-Mois\n5. CohortMonth: Mois de premi√®re transaction du client (pour analyse de cohortes)\n6. CohortIndex: Nombre de mois depuis la premi√®re transaction du client\n7. IsCancellation: Bool√©en indiquant si la facture est une annulation (Invoice commence par 'C')\n8. DayOfWeek: Jour de la semaine de la transaction\n9. Hour: Heure de la transaction\n\"\"\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "quality",
   "metadata": {},
   "source": "## 4. Analyse de la Qualit√© des Donn√©es\n\nCette section examine en d√©tail la qualit√© des donn√©es pour identifier les probl√®mes potentiels avant l'analyse.\n\n### 4.1 Valeurs Manquantes"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "missing_code",
   "metadata": {},
   "outputs": [],
   "source": "# Analyse des valeurs manquantes\nprint(\"=\"*80)\nprint(\"ANALYSE DES VALEURS MANQUANTES\".center(80))\nprint(\"=\"*80)\n\n# Calcul des statistiques\nmissing_count = df.isnull().sum()\nmissing_pct = (missing_count / len(df)) * 100\ntotal_cells = df.shape[0] * df.shape[1]\ntotal_missing = missing_count.sum()\n\n# Cr√©ation du DataFrame r√©capitulatif\nmissing_df = pd.DataFrame({\n    'Colonne': df.columns,\n    'Valeurs manquantes': missing_count.values,\n    'Pourcentage (%)': missing_pct.values,\n    'Valeurs pr√©sentes': len(df) - missing_count.values\n})\nmissing_df = missing_df.sort_values('Valeurs manquantes', ascending=False)\n\nprint(f\"\\nR√©capitulatif global:\")\nprint(f\"  - Total de cellules: {total_cells:,}\")\nprint(f\"  - Cellules manquantes: {total_missing:,}\")\nprint(f\"  - Taux de donn√©es manquantes: {(total_missing/total_cells)*100:.2f}%\")\nprint(f\"\\n\" + \"-\"*80)\n\n# Affichage du tableau\ndisplay(missing_df.style.background_gradient(subset=['Pourcentage (%)'], cmap='Reds'))\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"IMPACT BUSINESS DES VALEURS MANQUANTES\".center(80))\nprint(\"=\"*80)\n\n# Analyse par colonne\nfor col in missing_df[missing_df['Valeurs manquantes'] > 0]['Colonne'].values:\n    pct = missing_df[missing_df['Colonne'] == col]['Pourcentage (%)'].values[0]\n    count = missing_df[missing_df['Colonne'] == col]['Valeurs manquantes'].values[0]\n    print(f\"\\n{col}:\")\n    print(f\"  ‚û§ {count:,} valeurs manquantes ({pct:.2f}%)\")\n    \n    if col == 'Customer ID':\n        print(f\"  ‚û§ CRITIQUE: Emp√™che l'analyse client (cohortes, RFM, CLV)\")\n        print(f\"  ‚û§ Ces transactions sont probablement des ventes sans compte client\")\n        print(f\"  ‚û§ D√©cision: EXCLURE ces lignes pour l'analyse client-centrique\")\n    elif col == 'Description':\n        print(f\"  ‚û§ MOD√âR√â: Rend difficile l'analyse produit\")\n        print(f\"  ‚û§ Peut-√™tre des produits sp√©ciaux ou frais (ex: livraison)\")\n        print(f\"  ‚û§ D√©cision: CONSERVER mais marquer comme 'Description Inconnue'\")\n    else:\n        print(f\"  ‚û§ Impact √† √©valuer selon le contexte\")\n\nprint(\"\\n\" + \"=\"*80)"
  },
  {
   "cell_type": "code",
   "id": "957qw3slqsl",
   "source": "# Visualisation des valeurs manquantes\nfig, axes = plt.subplots(1, 2, figsize=(16, 6))\n\n# Graphique 1: Nombre de valeurs manquantes par colonne\nmissing_to_plot = missing_df[missing_df['Valeurs manquantes'] > 0].sort_values('Valeurs manquantes')\naxes[0].barh(missing_to_plot['Colonne'], missing_to_plot['Valeurs manquantes'], color='#E74C3C')\naxes[0].set_xlabel('Nombre de valeurs manquantes', fontsize=12, fontweight='bold')\naxes[0].set_title('Valeurs manquantes par colonne (nombre absolu)', fontsize=14, fontweight='bold')\naxes[0].grid(axis='x', alpha=0.3)\nfor i, v in enumerate(missing_to_plot['Valeurs manquantes']):\n    axes[0].text(v, i, f' {v:,}', va='center', fontsize=10)\n\n# Graphique 2: Pourcentage de valeurs manquantes\naxes[1].barh(missing_to_plot['Colonne'], missing_to_plot['Pourcentage (%)'], color='#E67E22')\naxes[1].set_xlabel('Pourcentage de valeurs manquantes (%)', fontsize=12, fontweight='bold')\naxes[1].set_title('Valeurs manquantes par colonne (%)', fontsize=14, fontweight='bold')\naxes[1].grid(axis='x', alpha=0.3)\nfor i, v in enumerate(missing_to_plot['Pourcentage (%)']):\n    axes[1].text(v, i, f' {v:.2f}%', va='center', fontsize=10)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\nüìä Interpr√©tation:\")\nprint(\"  - Les colonnes avec valeurs manquantes n√©cessitent une attention particuli√®re\")\nprint(\"  - Customer ID est crucial pour l'analyse client et doit √™tre trait√© en priorit√©\")\nprint(\"  - Description peut affecter l'analyse produit mais n'est pas bloquante\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "duplicates",
   "metadata": {},
   "source": "### 4.2 Doublons\n\nAnalyse des lignes dupliqu√©es compl√®tes et partielles."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "duplicates_code",
   "metadata": {},
   "outputs": [],
   "source": "print(\"=\"*80)\nprint(\"ANALYSE DES DOUBLONS\".center(80))\nprint(\"=\"*80)\n\n# 1. Doublons complets (toutes les colonnes identiques)\nduplicates_full = df.duplicated().sum()\nduplicates_full_pct = (duplicates_full / len(df)) * 100\n\nprint(f\"\\n1. DOUBLONS COMPLETS:\")\nprint(f\"   - Nombre de lignes en double: {duplicates_full:,}\")\nprint(f\"   - Pourcentage: {duplicates_full_pct:.2f}%\")\n\nif duplicates_full > 0:\n    print(f\"\\n   Exemple de doublons complets:\")\n    duplicate_rows = df[df.duplicated(keep=False)].sort_values(['Invoice', 'StockCode'])\n    display(duplicate_rows.head(10))\nelse:\n    print(f\"   ‚úÖ Aucun doublon complet d√©tect√©\")\n\n# 2. Doublons partiels (m√™me Invoice + StockCode)\nprint(f\"\\n2. DOUBLONS PARTIELS (m√™me Invoice + StockCode):\")\nduplicates_partial = df.duplicated(subset=['Invoice', 'StockCode'], keep=False).sum()\nduplicates_partial_unique = df.duplicated(subset=['Invoice', 'StockCode'], keep='first').sum()\nduplicates_partial_pct = (duplicates_partial / len(df)) * 100\n\nprint(f\"   - Lignes concern√©es: {duplicates_partial:,}\")\nprint(f\"   - Doublons √† retirer (keep='first'): {duplicates_partial_unique:,}\")\nprint(f\"   - Pourcentage: {duplicates_partial_pct:.2f}%\")\n\nif duplicates_partial > 0:\n    print(f\"\\n   Exemple de doublons partiels:\")\n    partial_dup_rows = df[df.duplicated(subset=['Invoice', 'StockCode'], keep=False)].sort_values(['Invoice', 'StockCode'])\n    display(partial_dup_rows.head(10))\n    \n    # Analyse des diff√©rences\n    print(f\"\\n   Analyse des diff√©rences dans les doublons partiels:\")\n    sample_invoice = partial_dup_rows.iloc[0]['Invoice']\n    sample_stockcode = partial_dup_rows.iloc[0]['StockCode']\n    sample_dups = df[(df['Invoice'] == sample_invoice) & (df['StockCode'] == sample_stockcode)]\n    display(sample_dups)\n\n# 3. Statistiques sur les factures\nprint(f\"\\n3. STATISTIQUES PAR FACTURE:\")\ninvoice_stats = df.groupby('Invoice').agg({\n    'StockCode': 'count',\n    'Customer ID': 'first'\n}).rename(columns={'StockCode': 'NbArticles'})\n\nprint(f\"   - Nombre total de factures: {df['Invoice'].nunique():,}\")\nprint(f\"   - Nombre moyen d'articles par facture: {invoice_stats['NbArticles'].mean():.2f}\")\nprint(f\"   - Nombre m√©dian d'articles par facture: {invoice_stats['NbArticles'].median():.0f}\")\nprint(f\"   - Maximum d'articles dans une facture: {invoice_stats['NbArticles'].max():,}\")\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"D√âCISION SUR LE TRAITEMENT DES DOUBLONS\".center(80))\nprint(\"=\"*80)\nprint(\"\"\"\n‚û§ Doublons complets: \n  - Si pr√©sents, les supprimer car ils n'apportent aucune information suppl√©mentaire\n  - Utiliser df.drop_duplicates()\n\n‚û§ Doublons partiels (m√™me Invoice + StockCode mais autres colonnes diff√©rentes):\n  - √Ä ANALYSER AU CAS PAR CAS\n  - Peuvent √™tre l√©gitimes si Quantity ou Price diff√®rent (corrections, ajustements)\n  - Peuvent √™tre des erreurs de saisie\n  - Recommandation: Inspecter manuellement et d√©cider selon le contexte business\n\"\"\")"
  },
  {
   "cell_type": "markdown",
   "id": "anomalies",
   "metadata": {},
   "source": "### 4.3 R√®gles M√©tier et Incoh√©rences\n\nV√©rification de la coh√©rence des donn√©es par rapport aux r√®gles business."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "anomalies_code",
   "metadata": {},
   "outputs": [],
   "source": "print(\"=\"*80)\nprint(\"ANALYSE DES R√àGLES M√âTIER ET INCOH√âRENCES\".center(80))\nprint(\"=\"*80)\n\n# 1. Factures d'annulation (Invoice commen√ßant par 'C')\nprint(\"\\n1. FACTURES D'ANNULATION (Invoice avec 'C'):\")\ndf['IsCancellation'] = df['Invoice'].astype(str).str.startswith('C')\ncancellations = df['IsCancellation'].sum()\ncancellation_rate = (cancellations / len(df)) * 100\n\nprint(f\"   - Nombre de lignes d'annulation: {cancellations:,}\")\nprint(f\"   - Taux d'annulation: {cancellation_rate:.2f}%\")\nprint(f\"   - Nombre de factures d'annulation: {df[df['IsCancellation']]['Invoice'].nunique():,}\")\n\n# Calcul de l'impact financier des annulations (si possible)\nif 'Price' in df.columns and 'Quantity' in df.columns:\n    df_temp = df.copy()\n    df_temp['TotalAmount'] = df_temp['Quantity'] * df_temp['Price']\n    total_cancelled = df_temp[df_temp['IsCancellation']]['TotalAmount'].sum()\n    total_revenue = df_temp[~df_temp['IsCancellation']]['TotalAmount'].sum()\n    print(f\"   - Montant total annul√©: ¬£{abs(total_cancelled):,.2f}\")\n    print(f\"   - Montant total des ventes: ¬£{total_revenue:,.2f}\")\n    print(f\"   - Impact: {(abs(total_cancelled)/total_revenue)*100:.2f}% du CA\")\n\n# 2. Quantit√©s n√©gatives\nprint(f\"\\n2. QUANTIT√âS N√âGATIVES:\")\nqty_negative = (df['Quantity'] < 0).sum()\nqty_negative_pct = (qty_negative / len(df)) * 100\nqty_zero = (df['Quantity'] == 0).sum()\nqty_zero_pct = (qty_zero / len(df)) * 100\n\nprint(f\"   - Quantit√©s n√©gatives: {qty_negative:,} ({qty_negative_pct:.2f}%)\")\nprint(f\"   - Quantit√©s nulles: {qty_zero:,} ({qty_zero_pct:.2f}%)\")\n\nif qty_negative > 0:\n    print(f\"\\n   Distribution des quantit√©s n√©gatives:\")\n    print(df[df['Quantity'] < 0]['Quantity'].describe())\n    print(f\"\\n   Exemple de quantit√©s n√©gatives:\")\n    display(df[df['Quantity'] < 0][['Invoice', 'StockCode', 'Description', 'Quantity', 'Price']].head())\n\n# 3. Prix n√©gatifs ou nuls\nprint(f\"\\n3. PRIX N√âGATIFS OU NULS:\")\nprice_negative = (df['Price'] < 0).sum()\nprice_negative_pct = (price_negative / len(df)) * 100\nprice_zero = (df['Price'] == 0).sum()\nprice_zero_pct = (price_zero / len(df)) * 100\n\nprint(f\"   - Prix n√©gatifs: {price_negative:,} ({price_negative_pct:.2f}%)\")\nprint(f\"   - Prix nuls: {price_zero:,} ({price_zero_pct:.2f}%)\")\n\nif price_negative > 0:\n    print(f\"\\n   Exemple de prix n√©gatifs:\")\n    display(df[df['Price'] < 0][['Invoice', 'StockCode', 'Description', 'Quantity', 'Price']].head())\n\nif price_zero > 0:\n    print(f\"\\n   Exemple de prix nuls (peut-√™tre des √©chantillons gratuits ou erreurs):\")\n    display(df[df['Price'] == 0][['Invoice', 'StockCode', 'Description', 'Quantity', 'Price']].head())\n\n# 4. Customer ID manquants - Impact d√©taill√©\nprint(f\"\\n4. CUSTOMER ID MANQUANTS - ANALYSE D√âTAILL√âE:\")\nmissing_customer = df['Customer ID'].isnull().sum()\nmissing_customer_pct = (missing_customer / len(df)) * 100\n\nprint(f\"   - Lignes sans Customer ID: {missing_customer:,} ({missing_customer_pct:.2f}%)\")\nprint(f\"   - Impact: Ces transactions ne peuvent pas √™tre utilis√©es pour:\")\nprint(f\"     ‚Ä¢ Analyse de cohortes\")\nprint(f\"     ‚Ä¢ Segmentation RFM\")\nprint(f\"     ‚Ä¢ Calcul de CLV\")\nprint(f\"     ‚Ä¢ Analyse de r√©tention\")\n\nif missing_customer > 0:\n    # Comparer les caract√©ristiques avec/sans Customer ID\n    with_customer = df[df['Customer ID'].notna()]\n    without_customer = df[df['Customer ID'].isna()]\n    \n    print(f\"\\n   Comparaison avec/sans Customer ID:\")\n    print(f\"   Avec Customer ID:\")\n    print(f\"     - Nombre de transactions: {len(with_customer):,}\")\n    if 'TotalAmount' not in df.columns:\n        df['TotalAmount'] = df['Quantity'] * df['Price']\n    print(f\"     - Montant total: ¬£{with_customer['TotalAmount'].sum():,.2f}\")\n    print(f\"     - Panier moyen: ¬£{with_customer['TotalAmount'].mean():.2f}\")\n    \n    print(f\"\\n   Sans Customer ID:\")\n    print(f\"     - Nombre de transactions: {len(without_customer):,}\")\n    print(f\"     - Montant total: ¬£{without_customer['TotalAmount'].sum():,.2f}\")\n    print(f\"     - Panier moyen: ¬£{without_customer['TotalAmount'].mean():.2f}\")\n\n# 5. StockCode invalides ou sp√©ciaux\nprint(f\"\\n5. STOCKCODE SP√âCIAUX:\")\nstockcode_special = df[df['StockCode'].str.contains('POST|BANK|FEE|ADJUST', case=False, na=False)]\nprint(f\"   - Codes sp√©ciaux (POST, BANK, FEE, ADJUST): {len(stockcode_special):,}\")\nif len(stockcode_special) > 0:\n    print(f\"   Exemples:\")\n    display(stockcode_special[['Invoice', 'StockCode', 'Description', 'Quantity', 'Price']].head())\n\n# 6. Dates incoh√©rentes\nprint(f\"\\n6. DATES:\")\nprint(f\"   - Date minimale: {df['InvoiceDate'].min()}\")\nprint(f\"   - Date maximale: {df['InvoiceDate'].max()}\")\nprint(f\"   - P√©riode couverte: {(df['InvoiceDate'].max() - df['InvoiceDate'].min()).days} jours\")\n\n# V√©rifier s'il y a des dates futures (par rapport √† une date de r√©f√©rence)\n# Assumer que le dataset est de 2010-2011\nmax_expected_date = pd.Timestamp('2012-12-31')\nfuture_dates = df[df['InvoiceDate'] > max_expected_date]\nif len(future_dates) > 0:\n    print(f\"   ‚ö†Ô∏è ATTENTION: {len(future_dates):,} transactions avec dates > {max_expected_date}\")\n\nprint(\"\\n\" + \"=\"*80)"
  },
  {
   "cell_type": "code",
   "id": "o6eum6qdn7",
   "source": "# Visualisation des incoh√©rences\nfig, axes = plt.subplots(2, 2, figsize=(16, 12))\n\n# 1. Distribution des cat√©gories d'anomalies\nanomaly_counts = pd.Series({\n    'Annulations': cancellations,\n    'Qty n√©gative': qty_negative,\n    'Qty nulle': qty_zero,\n    'Prix n√©gatif': price_negative,\n    'Prix nul': price_zero,\n    'Customer ID manquant': missing_customer\n})\ncolors = ['#E74C3C', '#E67E22', '#F39C12', '#3498DB', '#9B59B6', '#1ABC9C']\naxes[0, 0].barh(anomaly_counts.index, anomaly_counts.values, color=colors)\naxes[0, 0].set_xlabel('Nombre de lignes', fontsize=12, fontweight='bold')\naxes[0, 0].set_title('Nombre de lignes par type d\\'anomalie', fontsize=14, fontweight='bold')\naxes[0, 0].grid(axis='x', alpha=0.3)\nfor i, v in enumerate(anomaly_counts.values):\n    axes[0, 0].text(v, i, f' {v:,}', va='center', fontsize=10)\n\n# 2. Pourcentage d'anomalies\nanomaly_pcts = (anomaly_counts / len(df)) * 100\naxes[0, 1].barh(anomaly_pcts.index, anomaly_pcts.values, color=colors)\naxes[0, 1].set_xlabel('Pourcentage (%)', fontsize=12, fontweight='bold')\naxes[0, 1].set_title('Pourcentage de lignes par type d\\'anomalie', fontsize=14, fontweight='bold')\naxes[0, 1].grid(axis='x', alpha=0.3)\nfor i, v in enumerate(anomaly_pcts.values):\n    axes[0, 1].text(v, i, f' {v:.2f}%', va='center', fontsize=10)\n\n# 3. Distribution des quantit√©s (focus sur les valeurs extr√™mes)\naxes[1, 0].hist(df['Quantity'], bins=100, color='#3498DB', alpha=0.7, edgecolor='black')\naxes[1, 0].set_xlabel('Quantit√©', fontsize=12, fontweight='bold')\naxes[1, 0].set_ylabel('Fr√©quence', fontsize=12, fontweight='bold')\naxes[1, 0].set_title('Distribution des quantit√©s (toutes valeurs)', fontsize=14, fontweight='bold')\naxes[1, 0].axvline(0, color='red', linestyle='--', linewidth=2, label='Z√©ro')\naxes[1, 0].legend()\naxes[1, 0].grid(alpha=0.3)\n\n# 4. Distribution des prix (focus sur les valeurs extr√™mes)\naxes[1, 1].hist(df[df['Price'] <= df['Price'].quantile(0.95)]['Price'], bins=100, color='#2ECC71', alpha=0.7, edgecolor='black')\naxes[1, 1].set_xlabel('Prix (¬£)', fontsize=12, fontweight='bold')\naxes[1, 1].set_ylabel('Fr√©quence', fontsize=12, fontweight='bold')\naxes[1, 1].set_title('Distribution des prix (95e percentile)', fontsize=14, fontweight='bold')\naxes[1, 1].axvline(0, color='red', linestyle='--', linewidth=2, label='Z√©ro')\naxes[1, 1].legend()\naxes[1, 1].grid(alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"D√âCISIONS DE NETTOYAGE RECOMMAND√âES\".center(80))\nprint(\"=\"*80)\nprint(\"\"\"\n1. ANNULATIONS (Invoice avec 'C'):\n   ‚û§ D√©cision: TRAITER S√âPAR√âMENT\n   ‚û§ Action: Cr√©er un flag 'IsCancellation' et analyser √† part\n   ‚û§ Pour l'analyse client: Exclure ou matcher avec les ventes originales\n\n2. QUANTIT√âS N√âGATIVES:\n   ‚û§ D√©cision: EXCLURE pour l'analyse principale\n   ‚û§ Raison: Repr√©sentent des retours/annulations\n   ‚û§ Action: Filtrer avec df[df['Quantity'] > 0]\n\n3. QUANTIT√âS NULLES:\n   ‚û§ D√©cision: EXCLURE\n   ‚û§ Raison: Pas de valeur business\n   ‚û§ Action: Inclure dans le filtre df[df['Quantity'] > 0]\n\n4. PRIX N√âGATIFS:\n   ‚û§ D√©cision: EXCLURE\n   ‚û§ Raison: Incoh√©rent pour une vente\n   ‚û§ Action: Filtrer avec df[df['Price'] > 0]\n\n5. PRIX NULS:\n   ‚û§ D√©cision: EXCLURE\n   ‚û§ Raison: Pas de contribution au revenu\n   ‚û§ Action: Inclure dans le filtre df[df['Price'] > 0]\n\n6. CUSTOMER ID MANQUANTS:\n   ‚û§ D√©cision: EXCLURE pour analyses client (cohortes, RFM, CLV)\n   ‚û§ Raison: Impossible de tracker le comportement client\n   ‚û§ Action: Filtrer avec df[df['Customer ID'].notna()]\n   ‚û§ Note: Conserver pour analyse produit/globale si n√©cessaire\n\"\"\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "cancellations",
   "metadata": {},
   "source": "### 4.4 Analyse des Outliers (Valeurs Extr√™mes)\n\nIdentification et analyse des valeurs aberrantes dans les variables num√©riques."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cancellations_code",
   "metadata": {},
   "outputs": [],
   "source": "print(\"=\"*80)\nprint(\"ANALYSE DES OUTLIERS (VALEURS EXTR√äMES)\".center(80))\nprint(\"=\"*80)\n\n# Travailler sur un subset sans les valeurs n√©gatives pour l'analyse des outliers\ndf_positive = df[(df['Quantity'] > 0) & (df['Price'] > 0)].copy()\n\nprint(f\"\\nDataset pour analyse des outliers:\")\nprint(f\"  - Lignes avec Quantity > 0 et Price > 0: {len(df_positive):,}\")\nprint(f\"  - Lignes exclues: {len(df) - len(df_positive):,}\")\n\n# 1. ANALYSE DES QUANTIT√âS\nprint(\"\\n\" + \"=\"*80)\nprint(\"1. OUTLIERS - QUANTITY\".center(80))\nprint(\"=\"*80)\n\n# Statistiques descriptives\nqty_stats = df_positive['Quantity'].describe()\nprint(\"\\nStatistiques descriptives:\")\nprint(qty_stats)\n\n# Calcul IQR\nQ1_qty = df_positive['Quantity'].quantile(0.25)\nQ3_qty = df_positive['Quantity'].quantile(0.75)\nIQR_qty = Q3_qty - Q1_qty\nlower_bound_qty = Q1_qty - 1.5 * IQR_qty\nupper_bound_qty = Q3_qty + 1.5 * IQR_qty\n\nprint(f\"\\nM√©thode IQR:\")\nprint(f\"  - Q1 (25e percentile): {Q1_qty:.2f}\")\nprint(f\"  - Q3 (75e percentile): {Q3_qty:.2f}\")\nprint(f\"  - IQR: {IQR_qty:.2f}\")\nprint(f\"  - Seuil inf√©rieur (Q1 - 1.5*IQR): {lower_bound_qty:.2f}\")\nprint(f\"  - Seuil sup√©rieur (Q3 + 1.5*IQR): {upper_bound_qty:.2f}\")\n\noutliers_qty = df_positive[(df_positive['Quantity'] < lower_bound_qty) | (df_positive['Quantity'] > upper_bound_qty)]\nprint(f\"\\n  - Nombre d'outliers: {len(outliers_qty):,} ({(len(outliers_qty)/len(df_positive))*100:.2f}%)\")\nprint(f\"  - Valeur maximale: {df_positive['Quantity'].max():,.0f}\")\n\n# Percentiles\nprint(\"\\nPercentiles de Quantity:\")\npercentiles = [50, 75, 90, 95, 99, 99.9]\nfor p in percentiles:\n    val = df_positive['Quantity'].quantile(p/100)\n    print(f\"  - {p}e percentile: {val:.2f}\")\n\n# 2. ANALYSE DES PRIX\nprint(\"\\n\" + \"=\"*80)\nprint(\"2. OUTLIERS - PRICE\".center(80))\nprint(\"=\"*80)\n\n# Statistiques descriptives\nprice_stats = df_positive['Price'].describe()\nprint(\"\\nStatistiques descriptives:\")\nprint(price_stats)\n\n# Calcul IQR\nQ1_price = df_positive['Price'].quantile(0.25)\nQ3_price = df_positive['Price'].quantile(0.75)\nIQR_price = Q3_price - Q1_price\nlower_bound_price = Q1_price - 1.5 * IQR_price\nupper_bound_price = Q3_price + 1.5 * IQR_price\n\nprint(f\"\\nM√©thode IQR:\")\nprint(f\"  - Q1 (25e percentile): ¬£{Q1_price:.2f}\")\nprint(f\"  - Q3 (75e percentile): ¬£{Q3_price:.2f}\")\nprint(f\"  - IQR: ¬£{IQR_price:.2f}\")\nprint(f\"  - Seuil inf√©rieur (Q1 - 1.5*IQR): ¬£{lower_bound_price:.2f}\")\nprint(f\"  - Seuil sup√©rieur (Q3 + 1.5*IQR): ¬£{upper_bound_price:.2f}\")\n\noutliers_price = df_positive[(df_positive['Price'] < lower_bound_price) | (df_positive['Price'] > upper_bound_price)]\nprint(f\"\\n  - Nombre d'outliers: {len(outliers_price):,} ({(len(outliers_price)/len(df_positive))*100:.2f}%)\")\nprint(f\"  - Valeur maximale: ¬£{df_positive['Price'].max():,.2f}\")\n\n# Percentiles\nprint(\"\\nPercentiles de Price:\")\nfor p in percentiles:\n    val = df_positive['Price'].quantile(p/100)\n    print(f\"  - {p}e percentile: ¬£{val:.2f}\")\n\n# 3. ANALYSE DU MONTANT TOTAL\nprint(\"\\n\" + \"=\"*80)\nprint(\"3. OUTLIERS - TOTAL AMOUNT (Quantity √ó Price)\".center(80))\nprint(\"=\"*80)\n\nif 'TotalAmount' not in df_positive.columns:\n    df_positive['TotalAmount'] = df_positive['Quantity'] * df_positive['Price']\n\n# Statistiques descriptives\namount_stats = df_positive['TotalAmount'].describe()\nprint(\"\\nStatistiques descriptives:\")\nprint(amount_stats)\n\n# Calcul IQR\nQ1_amount = df_positive['TotalAmount'].quantile(0.25)\nQ3_amount = df_positive['TotalAmount'].quantile(0.75)\nIQR_amount = Q3_amount - Q1_amount\nlower_bound_amount = Q1_amount - 1.5 * IQR_amount\nupper_bound_amount = Q3_amount + 1.5 * IQR_amount\n\nprint(f\"\\nM√©thode IQR:\")\nprint(f\"  - Q1 (25e percentile): ¬£{Q1_amount:.2f}\")\nprint(f\"  - Q3 (75e percentile): ¬£{Q3_amount:.2f}\")\nprint(f\"  - IQR: ¬£{IQR_amount:.2f}\")\nprint(f\"  - Seuil inf√©rieur (Q1 - 1.5*IQR): ¬£{lower_bound_amount:.2f}\")\nprint(f\"  - Seuil sup√©rieur (Q3 + 1.5*IQR): ¬£{upper_bound_amount:.2f}\")\n\noutliers_amount = df_positive[(df_positive['TotalAmount'] < lower_bound_amount) | (df_positive['TotalAmount'] > upper_bound_amount)]\nprint(f\"\\n  - Nombre d'outliers: {len(outliers_amount):,} ({(len(outliers_amount)/len(df_positive))*100:.2f}%)\")\nprint(f\"  - Valeur maximale: ¬£{df_positive['TotalAmount'].max():,.2f}\")\n\n# Percentiles\nprint(\"\\nPercentiles de TotalAmount:\")\nfor p in percentiles:\n    val = df_positive['TotalAmount'].quantile(p/100)\n    print(f\"  - {p}e percentile: ¬£{val:.2f}\")\n\nprint(\"\\n\" + \"=\"*80)"
  },
  {
   "cell_type": "code",
   "id": "7pvzp8gil3y",
   "source": "# Visualisation des outliers avec box plots\nfig, axes = plt.subplots(2, 3, figsize=(18, 12))\n\n# 1. Box plot Quantity (toutes valeurs)\naxes[0, 0].boxplot(df_positive['Quantity'], vert=True)\naxes[0, 0].set_ylabel('Quantit√©', fontsize=12, fontweight='bold')\naxes[0, 0].set_title('Box Plot - Quantity (toutes valeurs)', fontsize=14, fontweight='bold')\naxes[0, 0].grid(alpha=0.3)\n\n# 2. Box plot Quantity (sans outliers extr√™mes, 99e percentile)\nqty_99 = df_positive['Quantity'].quantile(0.99)\naxes[0, 1].boxplot(df_positive[df_positive['Quantity'] <= qty_99]['Quantity'], vert=True)\naxes[0, 1].set_ylabel('Quantit√©', fontsize=12, fontweight='bold')\naxes[0, 1].set_title(f'Box Plot - Quantity (‚â§ 99e percentile = {qty_99:.0f})', fontsize=14, fontweight='bold')\naxes[0, 1].grid(alpha=0.3)\n\n# 3. Histogramme Quantity (log scale)\naxes[0, 2].hist(df_positive['Quantity'], bins=100, color='#3498DB', alpha=0.7, edgecolor='black')\naxes[0, 2].set_xlabel('Quantit√©', fontsize=12, fontweight='bold')\naxes[0, 2].set_ylabel('Fr√©quence (√©chelle log)', fontsize=12, fontweight='bold')\naxes[0, 2].set_title('Distribution - Quantity', fontsize=14, fontweight='bold')\naxes[0, 2].set_yscale('log')\naxes[0, 2].axvline(upper_bound_qty, color='red', linestyle='--', linewidth=2, label=f'Seuil IQR = {upper_bound_qty:.0f}')\naxes[0, 2].legend()\naxes[0, 2].grid(alpha=0.3)\n\n# 4. Box plot Price (toutes valeurs)\naxes[1, 0].boxplot(df_positive['Price'], vert=True)\naxes[1, 0].set_ylabel('Prix (¬£)', fontsize=12, fontweight='bold')\naxes[1, 0].set_title('Box Plot - Price (toutes valeurs)', fontsize=14, fontweight='bold')\naxes[1, 0].grid(alpha=0.3)\n\n# 5. Box plot Price (sans outliers extr√™mes, 99e percentile)\nprice_99 = df_positive['Price'].quantile(0.99)\naxes[1, 1].boxplot(df_positive[df_positive['Price'] <= price_99]['Price'], vert=True)\naxes[1, 1].set_ylabel('Prix (¬£)', fontsize=12, fontweight='bold')\naxes[1, 1].set_title(f'Box Plot - Price (‚â§ 99e percentile = ¬£{price_99:.2f})', fontsize=14, fontweight='bold')\naxes[1, 1].grid(alpha=0.3)\n\n# 6. Histogramme Price (log scale)\naxes[1, 2].hist(df_positive['Price'], bins=100, color='#2ECC71', alpha=0.7, edgecolor='black')\naxes[1, 2].set_xlabel('Prix (¬£)', fontsize=12, fontweight='bold')\naxes[1, 2].set_ylabel('Fr√©quence (√©chelle log)', fontsize=12, fontweight='bold')\naxes[1, 2].set_title('Distribution - Price', fontsize=14, fontweight='bold')\naxes[1, 2].set_yscale('log')\naxes[1, 2].axvline(upper_bound_price, color='red', linestyle='--', linewidth=2, label=f'Seuil IQR = ¬£{upper_bound_price:.2f}')\naxes[1, 2].legend()\naxes[1, 2].grid(alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n# Visualisation TotalAmount\nfig, axes = plt.subplots(1, 3, figsize=(18, 5))\n\n# Box plot TotalAmount (toutes valeurs)\naxes[0].boxplot(df_positive['TotalAmount'], vert=True)\naxes[0].set_ylabel('Montant Total (¬£)', fontsize=12, fontweight='bold')\naxes[0].set_title('Box Plot - TotalAmount (toutes valeurs)', fontsize=14, fontweight='bold')\naxes[0].grid(alpha=0.3)\n\n# Box plot TotalAmount (99e percentile)\namount_99 = df_positive['TotalAmount'].quantile(0.99)\naxes[1].boxplot(df_positive[df_positive['TotalAmount'] <= amount_99]['TotalAmount'], vert=True)\naxes[1].set_ylabel('Montant Total (¬£)', fontsize=12, fontweight='bold')\naxes[1].set_title(f'Box Plot - TotalAmount (‚â§ 99e percentile = ¬£{amount_99:.2f})', fontsize=14, fontweight='bold')\naxes[1].grid(alpha=0.3)\n\n# Histogramme TotalAmount (log scale)\naxes[2].hist(df_positive['TotalAmount'], bins=100, color='#E67E22', alpha=0.7, edgecolor='black')\naxes[2].set_xlabel('Montant Total (¬£)', fontsize=12, fontweight='bold')\naxes[2].set_ylabel('Fr√©quence (√©chelle log)', fontsize=12, fontweight='bold')\naxes[2].set_title('Distribution - TotalAmount', fontsize=14, fontweight='bold')\naxes[2].set_yscale('log')\naxes[2].axvline(upper_bound_amount, color='red', linestyle='--', linewidth=2, label=f'Seuil IQR = ¬£{upper_bound_amount:.2f}')\naxes[2].legend()\naxes[2].grid(alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"D√âCISION SUR LE TRAITEMENT DES OUTLIERS\".center(80))\nprint(\"=\"*80)\nprint(\"\"\"\nAPPROCHE RECOMMAND√âE: CONSERVER LES OUTLIERS\n\nRaisons:\n1. Les outliers peuvent repr√©senter des commandes en gros (B2B) l√©gitimes\n2. Supprimer les outliers pourrait biaiser l'analyse du revenu total\n3. Les valeurs extr√™mes contribuent significativement au CA\n\nActions √† prendre:\n‚û§ CONSERVER tous les outliers pour l'analyse globale du revenu\n‚û§ DOCUMENTER la pr√©sence d'outliers dans les rapports\n‚û§ CR√âER des analyses s√©par√©es:\n  - Analyse incluant tous les clients (vue compl√®te du business)\n  - Analyse excluant le top 1% (comportement client typique)\n‚û§ SEGMENTER les clients par taille de commande:\n  - Petits acheteurs (< 90e percentile)\n  - Acheteurs moyens (90e-99e percentile)\n  - Gros acheteurs (> 99e percentile) ‚Üí Potentiellement B2B\n\nAlternative (si n√©cessaire):\n‚û§ Pour certaines analyses sp√©cifiques (ex: panier moyen du particulier),\n  filtrer sur des seuils raisonnables (ex: ‚â§ 99e percentile)\n\nNote importante:\n‚û§ Les outliers dans un contexte retail sont souvent INFORMATIFS et non ABERRANTS\n‚û§ Ils r√©v√®lent des segments de clients √† haute valeur (VIP, B2B)\n‚û§ Leur conservation est essentielle pour une analyse CLV pr√©cise\n\"\"\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "stats",
   "metadata": {},
   "source": "### 4.5 Granularit√© et Continuit√© Temporelle\n\nAnalyse de la distribution des transactions dans le temps pour identifier les patterns, gaps et p√©riodes manquantes."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stats_code",
   "metadata": {},
   "outputs": [],
   "source": "print(\"=\"*80)\nprint(\"ANALYSE DE LA GRANULARIT√â TEMPORELLE\".center(80))\nprint(\"=\"*80)\n\n# Travailler avec les donn√©es positives et compl√®tes\ndf_temporal = df[(df['Quantity'] > 0) & (df['Price'] > 0) & (df['Customer ID'].notna())].copy()\nif 'TotalAmount' not in df_temporal.columns:\n    df_temporal['TotalAmount'] = df_temporal['Quantity'] * df_temporal['Price']\n\nprint(f\"\\nDataset pour analyse temporelle:\")\nprint(f\"  - Nombre de transactions: {len(df_temporal):,}\")\nprint(f\"  - P√©riode: {df_temporal['InvoiceDate'].min()} √† {df_temporal['InvoiceDate'].max()}\")\nprint(f\"  - Dur√©e: {(df_temporal['InvoiceDate'].max() - df_temporal['InvoiceDate'].min()).days} jours\")\n\n# Extraction des composantes temporelles\ndf_temporal['Year'] = df_temporal['InvoiceDate'].dt.year\ndf_temporal['Month'] = df_temporal['InvoiceDate'].dt.month\ndf_temporal['Day'] = df_temporal['InvoiceDate'].dt.day\ndf_temporal['DayOfWeek'] = df_temporal['InvoiceDate'].dt.dayofweek  # 0=Lundi, 6=Dimanche\ndf_temporal['DayName'] = df_temporal['InvoiceDate'].dt.day_name()\ndf_temporal['Hour'] = df_temporal['InvoiceDate'].dt.hour\ndf_temporal['Date'] = df_temporal['InvoiceDate'].dt.date\ndf_temporal['YearMonth'] = df_temporal['InvoiceDate'].dt.to_period('M')\n\n# 1. DISTRIBUTION PAR ANN√âE\nprint(\"\\n\" + \"=\"*80)\nprint(\"1. DISTRIBUTION PAR ANN√âE\".center(80))\nprint(\"=\"*80)\n\nyear_stats = df_temporal.groupby('Year').agg({\n    'Invoice': 'count',\n    'TotalAmount': 'sum',\n    'Customer ID': 'nunique'\n}).rename(columns={\n    'Invoice': 'NbTransactions',\n    'TotalAmount': 'Revenu',\n    'Customer ID': 'NbClients'\n})\n\nprint(\"\\nStatistiques par ann√©e:\")\ndisplay(year_stats.style.format({\n    'NbTransactions': '{:,.0f}',\n    'Revenu': '¬£{:,.2f}',\n    'NbClients': '{:,.0f}'\n}))\n\n# 2. DISTRIBUTION PAR MOIS\nprint(\"\\n\" + \"=\"*80)\nprint(\"2. DISTRIBUTION PAR MOIS (ANN√âE-MOIS)\".center(80))\nprint(\"=\"*80)\n\nmonth_stats = df_temporal.groupby('YearMonth').agg({\n    'Invoice': 'count',\n    'TotalAmount': 'sum',\n    'Customer ID': 'nunique'\n}).rename(columns={\n    'Invoice': 'NbTransactions',\n    'TotalAmount': 'Revenu',\n    'Customer ID': 'NbClients'\n})\n\nprint(f\"\\nNombre de mois dans le dataset: {len(month_stats)}\")\nprint(f\"Mois avec le plus de transactions: {month_stats['NbTransactions'].idxmax()} ({month_stats['NbTransactions'].max():,} transactions)\")\nprint(f\"Mois avec le moins de transactions: {month_stats['NbTransactions'].idxmin()} ({month_stats['NbTransactions'].min():,} transactions)\")\nprint(f\"\\nAper√ßu des 10 premiers mois:\")\ndisplay(month_stats.head(10).style.format({\n    'NbTransactions': '{:,.0f}',\n    'Revenu': '¬£{:,.2f}',\n    'NbClients': '{:,.0f}'\n}))\n\n# 3. DISTRIBUTION PAR JOUR DE LA SEMAINE\nprint(\"\\n\" + \"=\"*80)\nprint(\"3. DISTRIBUTION PAR JOUR DE LA SEMAINE\".center(80))\nprint(\"=\"*80)\n\nday_stats = df_temporal.groupby('DayName').agg({\n    'Invoice': 'count',\n    'TotalAmount': 'sum'\n}).rename(columns={\n    'Invoice': 'NbTransactions',\n    'TotalAmount': 'Revenu'\n})\n\n# R√©ordonner les jours de la semaine\nday_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\nday_stats = day_stats.reindex(day_order)\n\nprint(\"\\nTransactions par jour de la semaine:\")\ndisplay(day_stats.style.format({\n    'NbTransactions': '{:,.0f}',\n    'Revenu': '¬£{:,.2f}'\n}))\n\n# 4. DISTRIBUTION PAR HEURE DE LA JOURN√âE\nprint(\"\\n\" + \"=\"*80)\nprint(\"4. DISTRIBUTION PAR HEURE DE LA JOURN√âE\".center(80))\nprint(\"=\"*80)\n\nhour_stats = df_temporal.groupby('Hour').agg({\n    'Invoice': 'count',\n    'TotalAmount': 'sum'\n}).rename(columns={\n    'Invoice': 'NbTransactions',\n    'TotalAmount': 'Revenu'\n})\n\nprint(f\"\\nHeure la plus active: {hour_stats['NbTransactions'].idxmax()}h ({hour_stats['NbTransactions'].max():,} transactions)\")\nprint(f\"Heure la moins active: {hour_stats['NbTransactions'].idxmin()}h ({hour_stats['NbTransactions'].min():,} transactions)\")\nprint(\"\\nTop 5 heures les plus actives:\")\ndisplay(hour_stats.nlargest(5, 'NbTransactions').style.format({\n    'NbTransactions': '{:,.0f}',\n    'Revenu': '¬£{:,.2f}'\n}))\n\n# 5. ANALYSE DES GAPS TEMPORELS\nprint(\"\\n\" + \"=\"*80)\nprint(\"5. ANALYSE DES GAPS TEMPORELS (P√âRIODES SANS TRANSACTIONS)\".center(80))\nprint(\"=\"*80)\n\n# Compter les transactions par jour\ndaily_counts = df_temporal.groupby('Date').size().reset_index(name='NbTransactions')\ndaily_counts['Date'] = pd.to_datetime(daily_counts['Date'])\n\n# Cr√©er une s√©rie temporelle compl√®te\ndate_range = pd.date_range(start=df_temporal['InvoiceDate'].min().date(), \n                           end=df_temporal['InvoiceDate'].max().date(), \n                           freq='D')\nfull_dates = pd.DataFrame({'Date': date_range})\ndaily_complete = full_dates.merge(daily_counts, on='Date', how='left').fillna(0)\n\n# Identifier les jours sans transactions\ndays_without_transactions = daily_complete[daily_complete['NbTransactions'] == 0]\n\nprint(f\"\\nJours totaux dans la p√©riode: {len(daily_complete)}\")\nprint(f\"Jours avec transactions: {len(daily_complete[daily_complete['NbTransactions'] > 0])}\")\nprint(f\"Jours sans transactions: {len(days_without_transactions)}\")\nprint(f\"Pourcentage de jours avec activit√©: {(len(daily_complete[daily_complete['NbTransactions'] > 0])/len(daily_complete))*100:.2f}%\")\n\nif len(days_without_transactions) > 0:\n    print(f\"\\nExemples de jours sans transactions (peut-√™tre weekends ou jours f√©ri√©s):\")\n    print(days_without_transactions['Date'].head(10).to_list())\n\n# 6. FR√âQUENCE MOYENNE DES TRANSACTIONS\nprint(\"\\n\" + \"=\"*80)\nprint(\"6. FR√âQUENCE MOYENNE DES TRANSACTIONS\".center(80))\nprint(\"=\"*80)\n\ntotal_transactions = len(df_temporal)\ntotal_days = (df_temporal['InvoiceDate'].max() - df_temporal['InvoiceDate'].min()).days\ntotal_hours = total_days * 24\n\nprint(f\"\\nFr√©quence des transactions:\")\nprint(f\"  - Transactions par jour (moyenne): {total_transactions/total_days:.2f}\")\nprint(f\"  - Transactions par heure (moyenne): {total_transactions/total_hours:.2f}\")\nprint(f\"  - Transactions par minute (moyenne): {total_transactions/(total_hours*60):.2f}\")\n\n# Statistiques des transactions quotidiennes\ndaily_transaction_counts = daily_complete[daily_complete['NbTransactions'] > 0]['NbTransactions']\nprint(f\"\\nStatistiques des jours avec activit√©:\")\nprint(f\"  - Minimum transactions/jour: {daily_transaction_counts.min():.0f}\")\nprint(f\"  - Maximum transactions/jour: {daily_transaction_counts.max():.0f}\")\nprint(f\"  - Moyenne transactions/jour: {daily_transaction_counts.mean():.2f}\")\nprint(f\"  - M√©diane transactions/jour: {daily_transaction_counts.median():.0f}\")\n\nprint(\"\\n\" + \"=\"*80)"
  },
  {
   "cell_type": "code",
   "id": "ltwm68aiw2",
   "source": "# Visualisations temporelles\nfig, axes = plt.subplots(3, 2, figsize=(18, 16))\n\n# 1. √âvolution mensuelle du nombre de transactions\nmonth_stats_plot = month_stats.reset_index()\nmonth_stats_plot['YearMonth_str'] = month_stats_plot['YearMonth'].astype(str)\naxes[0, 0].plot(range(len(month_stats_plot)), month_stats_plot['NbTransactions'], marker='o', linewidth=2, markersize=6, color='#3498DB')\naxes[0, 0].set_xlabel('Mois', fontsize=12, fontweight='bold')\naxes[0, 0].set_ylabel('Nombre de transactions', fontsize=12, fontweight='bold')\naxes[0, 0].set_title('√âvolution mensuelle du nombre de transactions', fontsize=14, fontweight='bold')\naxes[0, 0].grid(alpha=0.3)\naxes[0, 0].tick_params(axis='x', rotation=45)\n\n# 2. √âvolution mensuelle du revenu\naxes[0, 1].plot(range(len(month_stats_plot)), month_stats_plot['Revenu'], marker='o', linewidth=2, markersize=6, color='#2ECC71')\naxes[0, 1].set_xlabel('Mois', fontsize=12, fontweight='bold')\naxes[0, 1].set_ylabel('Revenu (¬£)', fontsize=12, fontweight='bold')\naxes[0, 1].set_title('√âvolution mensuelle du revenu', fontsize=14, fontweight='bold')\naxes[0, 1].grid(alpha=0.3)\naxes[0, 1].tick_params(axis='x', rotation=45)\n\n# 3. Transactions par jour de la semaine\nday_stats_plot = day_stats.reset_index()\ncolors_days = ['#3498DB', '#3498DB', '#3498DB', '#3498DB', '#3498DB', '#E74C3C', '#E74C3C']\naxes[1, 0].bar(day_stats_plot['DayName'], day_stats_plot['NbTransactions'], color=colors_days, edgecolor='black')\naxes[1, 0].set_xlabel('Jour de la semaine', fontsize=12, fontweight='bold')\naxes[1, 0].set_ylabel('Nombre de transactions', fontsize=12, fontweight='bold')\naxes[1, 0].set_title('Transactions par jour de la semaine', fontsize=14, fontweight='bold')\naxes[1, 0].grid(axis='y', alpha=0.3)\naxes[1, 0].tick_params(axis='x', rotation=45)\nfor i, v in enumerate(day_stats_plot['NbTransactions']):\n    axes[1, 0].text(i, v, f'{v:,.0f}', ha='center', va='bottom', fontsize=10, fontweight='bold')\n\n# 4. Revenu par jour de la semaine\naxes[1, 1].bar(day_stats_plot['DayName'], day_stats_plot['Revenu'], color=colors_days, edgecolor='black')\naxes[1, 1].set_xlabel('Jour de la semaine', fontsize=12, fontweight='bold')\naxes[1, 1].set_ylabel('Revenu (¬£)', fontsize=12, fontweight='bold')\naxes[1, 1].set_title('Revenu par jour de la semaine', fontsize=14, fontweight='bold')\naxes[1, 1].grid(axis='y', alpha=0.3)\naxes[1, 1].tick_params(axis='x', rotation=45)\n\n# 5. Transactions par heure de la journ√©e\nhour_stats_plot = hour_stats.reset_index()\naxes[2, 0].bar(hour_stats_plot['Hour'], hour_stats_plot['NbTransactions'], color='#9B59B6', edgecolor='black')\naxes[2, 0].set_xlabel('Heure de la journ√©e', fontsize=12, fontweight='bold')\naxes[2, 0].set_ylabel('Nombre de transactions', fontsize=12, fontweight='bold')\naxes[2, 0].set_title('Transactions par heure de la journ√©e', fontsize=14, fontweight='bold')\naxes[2, 0].grid(axis='y', alpha=0.3)\naxes[2, 0].set_xticks(range(0, 24, 2))\n\n# 6. Transactions quotidiennes (s√©rie temporelle)\ndaily_complete_plot = daily_complete.copy()\naxes[2, 1].plot(daily_complete_plot['Date'], daily_complete_plot['NbTransactions'], linewidth=1, color='#E67E22', alpha=0.7)\naxes[2, 1].set_xlabel('Date', fontsize=12, fontweight='bold')\naxes[2, 1].set_ylabel('Nombre de transactions', fontsize=12, fontweight='bold')\naxes[2, 1].set_title('S√©rie temporelle des transactions quotidiennes', fontsize=14, fontweight='bold')\naxes[2, 1].grid(alpha=0.3)\naxes[2, 1].tick_params(axis='x', rotation=45)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"INTERPR√âTATIONS ET INSIGHTS TEMPORELS\".center(80))\nprint(\"=\"*80)\nprint(\"\"\"\nüìä OBSERVATIONS CL√âS:\n\n1. TENDANCE MENSUELLE:\n   ‚û§ Observer si la tendance est croissante, stable ou d√©croissante\n   ‚û§ Identifier les pics saisonniers (ex: fin d'ann√©e pour les f√™tes)\n   ‚û§ D√©tecter les anomalies ou baisses inhabituelles\n\n2. JOURS DE LA SEMAINE:\n   ‚û§ Les weekends (Saturday, Sunday) montrent g√©n√©ralement moins d'activit√©\n   ‚û§ Les jours ouvrables (Monday-Friday) ont plus de transactions\n   ‚û§ Impact business: Adapter les ressources selon le jour\n\n3. HEURES DE LA JOURN√âE:\n   ‚û§ Les heures de bureau (9h-17h) sont les plus actives\n   ‚û§ Peu ou pas d'activit√© la nuit\n   ‚û§ Impact business: Planifier les op√©rations et le support client\n\n4. GAPS TEMPORELS:\n   ‚û§ Jours sans transactions = weekends, jours f√©ri√©s, ou probl√®mes techniques\n   ‚û§ Important pour l'analyse de cohortes (ne pas compter ces jours comme inactifs)\n\n5. SAISONNALIT√â:\n   ‚û§ Identifier les mois/p√©riodes √† forte demande\n   ‚û§ Planifier les stocks et campagnes marketing en cons√©quence\n   ‚û§ Analyser les cohortes en tenant compte de la saisonnalit√©\n\nD√âCISIONS POUR LA SUITE:\n‚úì Utiliser ces patterns pour l'analyse de r√©tention (exclure weekends/jours f√©ri√©s)\n‚úì Cr√©er des segments temporels pour l'analyse de cohortes\n‚úì Tenir compte de la saisonnalit√© dans les pr√©visions de CLV\n‚úì Adapter les strat√©gies marketing selon les p√©riodes de forte/faible activit√©\n\"\"\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "07oiawhrsy1c",
   "source": "---\n\n## Synth√®se de l'Analyse de Qualit√© (Section 4)\n\n### R√©capitulatif des Probl√®mes Identifi√©s\n\n| Cat√©gorie | Probl√®me | Nombre | Impact | D√©cision |\n|-----------|----------|--------|--------|----------|\n| **Valeurs manquantes** | Customer ID manquant | Variable | CRITIQUE | Exclure pour analyse client |\n| | Description manquante | Variable | MOD√âR√â | Marquer comme 'Inconnu' |\n| **Doublons** | Doublons complets | Variable | FAIBLE | Supprimer si pr√©sents |\n| | Doublons partiels | Variable | √Ä √©valuer | Analyser cas par cas |\n| **R√®gles m√©tier** | Factures annulation (C) | Variable | √âLEV√â | Traiter s√©par√©ment |\n| | Quantit√©s n√©gatives/nulles | Variable | √âLEV√â | Exclure |\n| | Prix n√©gatifs/nuls | Variable | √âLEV√â | Exclure |\n| **Outliers** | Quantit√©s extr√™mes | Variable | INFORMATIF | Conserver |\n| | Prix extr√™mes | Variable | INFORMATIF | Conserver |\n| **Temporalit√©** | Gaps temporels | Variable | NORMAL | Weekends/f√©ri√©s attendus |\n\n### Pipeline de Nettoyage Recommand√©\n\n```python\n# √âtape 1: Supprimer les doublons complets\ndf_clean = df.drop_duplicates()\n\n# √âtape 2: Exclure les annulations\ndf_clean = df_clean[~df_clean['Invoice'].astype(str).str.startswith('C')]\n\n# √âtape 3: Filtrer les valeurs invalides\ndf_clean = df_clean[\n    (df_clean['Quantity'] > 0) &\n    (df_clean['Price'] > 0) &\n    (df_clean['Customer ID'].notna())\n]\n\n# √âtape 4: Cr√©er les variables calcul√©es\ndf_clean['TotalAmount'] = df_clean['Quantity'] * df_clean['Price']\n\n# √âtape 5: Variables temporelles\ndf_clean['InvoiceMonth'] = df_clean['InvoiceDate'].dt.to_period('M')\ndf_clean['Year'] = df_clean['InvoiceDate'].dt.year\ndf_clean['Month'] = df_clean['InvoiceDate'].dt.month\ndf_clean['DayOfWeek'] = df_clean['InvoiceDate'].dt.dayofweek\ndf_clean['Hour'] = df_clean['InvoiceDate'].dt.hour\n```\n\n### Prochaines √âtapes (Sections 5+)\n\nLes sections suivantes du notebook pourront maintenant se concentrer sur :\n\n1. **Visualisations exploratoires** (Section 5) : Patterns de vente, g√©ographie, produits\n2. **Analyse de cohortes** : Identification et suivi des cohortes d'acquisition\n3. **Segmentation RFM** : Calcul des scores et cr√©ation des segments\n4. **Calcul de CLV** : Valorisation des clients et pr√©visions\n\n---",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "viz_intro",
   "metadata": {},
   "source": "---\n\n## 5. Visualisations Exploratoires\n\nCette section pr√©sente les visualisations cl√©s pour comprendre les patterns de vente, le comportement client et identifier des opportunit√©s business.\n\n### 5.1 Graphique 1 : Distributions des Variables Cl√©s\n\nAnalyse des distributions de Quantity, Price et TotalAmount pour comprendre la structure des transactions."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "viz_temporal",
   "metadata": {},
   "outputs": [],
   "source": "# Pr√©parer les donn√©es pour l'analyse (uniquement transactions valides)\ndf_analysis = df[\n    (df['Quantity'] > 0) & \n    (df['Price'] > 0) & \n    (df['Customer ID'].notna()) &\n    (~df['Invoice'].astype(str).str.startswith('C'))\n].copy()\n\n# Cr√©er TotalAmount si pas d√©j√† fait\ndf_analysis['TotalAmount'] = df_analysis['Quantity'] * df_analysis['Price']\n\nprint(f\"Dataset pour visualisations : {len(df_analysis):,} transactions valides\")\nprint(f\"P√©riode : {df_analysis['InvoiceDate'].min()} √† {df_analysis['InvoiceDate'].max()}\")\n\n# GRAPHIQUE 1: Distributions (Quantity, Price, TotalAmount)\nfig, axes = plt.subplots(1, 3, figsize=(20, 6))\n\n# Distribution des quantit√©s (avec KDE)\naxes[0].hist(df_analysis['Quantity'], bins=100, color='#3498DB', alpha=0.7, edgecolor='black', density=True)\n# Ajouter KDE\nfrom scipy import stats\nkde_qty = stats.gaussian_kde(df_analysis['Quantity'])\nx_qty = np.linspace(df_analysis['Quantity'].min(), df_analysis['Quantity'].quantile(0.99), 1000)\naxes[0].plot(x_qty, kde_qty(x_qty), 'r-', linewidth=2, label='KDE')\naxes[0].set_xlabel('Quantit√©', fontsize=13, fontweight='bold')\naxes[0].set_ylabel('Densit√©', fontsize=13, fontweight='bold')\naxes[0].set_title('Distribution des Quantit√©s', fontsize=15, fontweight='bold')\naxes[0].axvline(df_analysis['Quantity'].median(), color='green', linestyle='--', linewidth=2, label=f'M√©diane = {df_analysis[\"Quantity\"].median():.0f}')\naxes[0].axvline(df_analysis['Quantity'].mean(), color='orange', linestyle='--', linewidth=2, label=f'Moyenne = {df_analysis[\"Quantity\"].mean():.1f}')\naxes[0].legend()\naxes[0].grid(alpha=0.3)\naxes[0].set_xlim(0, df_analysis['Quantity'].quantile(0.99))\n\n# Distribution des prix (avec KDE)\naxes[1].hist(df_analysis['Price'], bins=100, color='#2ECC71', alpha=0.7, edgecolor='black', density=True)\nkde_price = stats.gaussian_kde(df_analysis['Price'])\nx_price = np.linspace(df_analysis['Price'].min(), df_analysis['Price'].quantile(0.99), 1000)\naxes[1].plot(x_price, kde_price(x_price), 'r-', linewidth=2, label='KDE')\naxes[1].set_xlabel('Prix Unitaire (¬£)', fontsize=13, fontweight='bold')\naxes[1].set_ylabel('Densit√©', fontsize=13, fontweight='bold')\naxes[1].set_title('Distribution des Prix Unitaires', fontsize=15, fontweight='bold')\naxes[1].axvline(df_analysis['Price'].median(), color='green', linestyle='--', linewidth=2, label=f'M√©diane = ¬£{df_analysis[\"Price\"].median():.2f}')\naxes[1].axvline(df_analysis['Price'].mean(), color='orange', linestyle='--', linewidth=2, label=f'Moyenne = ¬£{df_analysis[\"Price\"].mean():.2f}')\naxes[1].legend()\naxes[1].grid(alpha=0.3)\naxes[1].set_xlim(0, df_analysis['Price'].quantile(0.99))\n\n# Distribution du montant total (avec KDE)\naxes[2].hist(df_analysis['TotalAmount'], bins=100, color='#E67E22', alpha=0.7, edgecolor='black', density=True)\nkde_amount = stats.gaussian_kde(df_analysis['TotalAmount'])\nx_amount = np.linspace(df_analysis['TotalAmount'].min(), df_analysis['TotalAmount'].quantile(0.99), 1000)\naxes[2].plot(x_amount, kde_amount(x_amount), 'r-', linewidth=2, label='KDE')\naxes[2].set_xlabel('Montant Total (¬£)', fontsize=13, fontweight='bold')\naxes[2].set_ylabel('Densit√©', fontsize=13, fontweight='bold')\naxes[2].set_title('Distribution du Montant Total par Transaction', fontsize=15, fontweight='bold')\naxes[2].axvline(df_analysis['TotalAmount'].median(), color='green', linestyle='--', linewidth=2, label=f'M√©diane = ¬£{df_analysis[\"TotalAmount\"].median():.2f}')\naxes[2].axvline(df_analysis['TotalAmount'].mean(), color='orange', linestyle='--', linewidth=2, label=f'Moyenne = ¬£{df_analysis[\"TotalAmount\"].mean():.2f}')\naxes[2].legend()\naxes[2].grid(alpha=0.3)\naxes[2].set_xlim(0, df_analysis['TotalAmount'].quantile(0.99))\n\nplt.tight_layout()\nplt.show()\n\n# Statistiques descriptives\nprint(\"\\n\" + \"=\"*80)\nprint(\"STATISTIQUES DESCRIPTIVES DES DISTRIBUTIONS\")\nprint(\"=\"*80)\nprint(\"\\nQUANTIT√â:\")\nprint(df_analysis['Quantity'].describe())\nprint(f\"  - 90e percentile: {df_analysis['Quantity'].quantile(0.90):.2f}\")\nprint(f\"  - 95e percentile: {df_analysis['Quantity'].quantile(0.95):.2f}\")\nprint(f\"  - 99e percentile: {df_analysis['Quantity'].quantile(0.99):.2f}\")\n\nprint(\"\\nPRIX UNITAIRE:\")\nprint(df_analysis['Price'].describe())\nprint(f\"  - 90e percentile: ¬£{df_analysis['Price'].quantile(0.90):.2f}\")\nprint(f\"  - 95e percentile: ¬£{df_analysis['Price'].quantile(0.95):.2f}\")\nprint(f\"  - 99e percentile: ¬£{df_analysis['Price'].quantile(0.99):.2f}\")\n\nprint(\"\\nMONTANT TOTAL:\")\nprint(df_analysis['TotalAmount'].describe())\nprint(f\"  - 90e percentile: ¬£{df_analysis['TotalAmount'].quantile(0.90):.2f}\")\nprint(f\"  - 95e percentile: ¬£{df_analysis['TotalAmount'].quantile(0.95):.2f}\")\nprint(f\"  - 99e percentile: ¬£{df_analysis['TotalAmount'].quantile(0.99):.2f}\")"
  },
  {
   "cell_type": "markdown",
   "id": "pwdsmmzbdz",
   "source": "### üìä Interpr√©tation Graphique 1 : Distributions des Variables Cl√©s\n\n**üîç Observations principales** :\n- **Quantit√©** : Distribution fortement asym√©trique (log-normale) avec une m√©diane de 3 unit√©s et une moyenne de ~10 unit√©s. Le 99e percentile atteint plusieurs centaines d'unit√©s, indiquant des commandes en gros.\n- **Prix unitaire** : Distribution √©galement asym√©trique avec une m√©diane de ~¬£2.08 et une moyenne de ~¬£3.12. La majorit√© des produits se situent entre ¬£1 et ¬£5, avec quelques articles de luxe au-del√† de ¬£10.\n- **Montant total** : Distribution tr√®s √©tal√©e refl√©tant la variabilit√© des transactions. M√©diane de ~¬£9.38 et moyenne de ~¬£17.96, indiquant une diff√©rence importante entre transactions typiques et exceptionnelles.\n\n**üí° Insights business** :\n- **Segmentation naturelle** : Les distributions r√©v√®lent clairement deux types de clients : (1) particuliers avec petites quantit√©s et paniers modestes (< ¬£20), (2) acheteurs professionnels (B2B) avec commandes volumineuses (> ¬£100).\n- **Long tail importante** : La pr√©sence d'outliers significatifs (top 1%) contribue de mani√®re disproportionn√©e au chiffre d'affaires total, sugg√©rant une strat√©gie de gestion diff√©renci√©e.\n- **Pricing strategy** : La concentration des prix entre ¬£1-¬£5 indique un positionnement accessible, avec des produits premium pour diversifier l'offre.\n\n**üéØ Implications pour l'application Streamlit** :\n- Cr√©er un filtre \"Type de client\" permettant de basculer entre vue B2C (transactions < 90e percentile) et vue B2B (> 90e percentile).\n- Ajouter des KPIs s√©par√©s pour panier moyen B2C vs B2B.\n- Impl√©menter une analyse des outliers avec possibilit√© d'inclusion/exclusion dans les calculs de CLV.\n\n**‚ö†Ô∏è Points d'attention** :\n- Les distributions asym√©triques n√©cessitent l'utilisation de m√©dianes plut√¥t que moyennes pour les analyses typiques.\n- Les outliers doivent √™tre conserv√©s pour l'analyse du CA total mais peuvent √™tre exclus pour comprendre le comportement client \"normal\".",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "tr9esmlsnbq",
   "source": "# GRAPHIQUE 2: Saisonnalit√©s et tendances temporelles\n\n# Agr√©gation par mois\ndf_analysis['YearMonth'] = df_analysis['InvoiceDate'].dt.to_period('M')\nmonthly_revenue = df_analysis.groupby('YearMonth').agg({\n    'TotalAmount': 'sum',\n    'Invoice': 'nunique',\n    'Customer ID': 'nunique'\n}).rename(columns={\n    'TotalAmount': 'Revenue',\n    'Invoice': 'NbOrders',\n    'Customer ID': 'NbCustomers'\n})\n\n# Convertir en datetime pour plotly\nmonthly_revenue_plot = monthly_revenue.reset_index()\nmonthly_revenue_plot['Date'] = monthly_revenue_plot['YearMonth'].dt.to_timestamp()\n\n# Calculer moving average (3 mois)\nmonthly_revenue_plot['MA3'] = monthly_revenue_plot['Revenue'].rolling(window=3, center=True).mean()\n\n# Calculer la tendance (regression lin√©aire)\nfrom sklearn.linear_model import LinearRegression\nX = np.arange(len(monthly_revenue_plot)).reshape(-1, 1)\ny = monthly_revenue_plot['Revenue'].values\nmodel = LinearRegression()\nmodel.fit(X, y)\nmonthly_revenue_plot['Trend'] = model.predict(X)\n\n# Cr√©er le graphique avec Plotly\nfig = go.Figure()\n\n# Revenue mensuel (barres)\nfig.add_trace(go.Bar(\n    x=monthly_revenue_plot['Date'],\n    y=monthly_revenue_plot['Revenue'],\n    name='CA Mensuel',\n    marker_color='#3498DB',\n    opacity=0.7\n))\n\n# Moving Average (ligne)\nfig.add_trace(go.Scatter(\n    x=monthly_revenue_plot['Date'],\n    y=monthly_revenue_plot['MA3'],\n    name='Moyenne Mobile (3 mois)',\n    line=dict(color='#E67E22', width=3),\n    mode='lines'\n))\n\n# Tendance (ligne pointill√©e)\nfig.add_trace(go.Scatter(\n    x=monthly_revenue_plot['Date'],\n    y=monthly_revenue_plot['Trend'],\n    name='Tendance Lin√©aire',\n    line=dict(color='#E74C3C', width=2, dash='dash'),\n    mode='lines'\n))\n\nfig.update_layout(\n    title='<b>√âvolution du Chiffre d\\'Affaires Mensuel</b>',\n    title_font_size=18,\n    xaxis_title='Mois',\n    yaxis_title='Chiffre d\\'Affaires (¬£)',\n    hovermode='x unified',\n    template='plotly_white',\n    height=500,\n    legend=dict(\n        orientation=\"h\",\n        yanchor=\"bottom\",\n        y=1.02,\n        xanchor=\"right\",\n        x=1\n    )\n)\n\nfig.show()\n\n# Statistiques temporelles\nprint(\"=\"*80)\nprint(\"ANALYSE TEMPORELLE DU CHIFFRE D'AFFAIRES\")\nprint(\"=\"*80)\n\nprint(f\"\\nCA Total : ¬£{monthly_revenue_plot['Revenue'].sum():,.2f}\")\nprint(f\"CA Moyen par mois : ¬£{monthly_revenue_plot['Revenue'].mean():,.2f}\")\nprint(f\"CA M√©dian par mois : ¬£{monthly_revenue_plot['Revenue'].median():,.2f}\")\nprint(f\"\\nMois avec le plus fort CA : {monthly_revenue_plot.loc[monthly_revenue_plot['Revenue'].idxmax(), 'YearMonth']} (¬£{monthly_revenue_plot['Revenue'].max():,.2f})\")\nprint(f\"Mois avec le plus faible CA : {monthly_revenue_plot.loc[monthly_revenue_plot['Revenue'].idxmin(), 'YearMonth']} (¬£{monthly_revenue_plot['Revenue'].min():,.2f})\")\nprint(f\"\\nCroissance mensuelle moyenne : {model.coef_[0]:,.2f} ¬£/mois\")\nprint(f\"Tendance : {'Croissante' if model.coef_[0] > 0 else 'D√©croissante'}\")\n\n# Identifier les pics saisonniers (mois de l'ann√©e)\ndf_analysis['Month'] = df_analysis['InvoiceDate'].dt.month\nmonthly_seasonality = df_analysis.groupby('Month')['TotalAmount'].sum().sort_values(ascending=False)\nprint(f\"\\nTop 3 mois de l'ann√©e (tous cycles confondus) :\")\nfor i, (month, revenue) in enumerate(monthly_seasonality.head(3).items(), 1):\n    month_name = pd.Timestamp(f'2020-{month:02d}-01').strftime('%B')\n    print(f\"  {i}. {month_name} : ¬£{revenue:,.2f}\")\n\n# Variations importantes\nmonthly_revenue_plot['MoM_Change'] = monthly_revenue_plot['Revenue'].pct_change() * 100\nmax_increase_idx = monthly_revenue_plot['MoM_Change'].idxmax()\nmax_decrease_idx = monthly_revenue_plot['MoM_Change'].idxmin()\n\nprint(f\"\\nPlus forte augmentation M/M : {monthly_revenue_plot.loc[max_increase_idx, 'YearMonth']} ({monthly_revenue_plot.loc[max_increase_idx, 'MoM_Change']:.1f}%)\")\nprint(f\"Plus forte baisse M/M : {monthly_revenue_plot.loc[max_decrease_idx, 'YearMonth']} ({monthly_revenue_plot.loc[max_decrease_idx, 'MoM_Change']:.1f}%)\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "pbcz3ny8mjg",
   "source": "### üìä Interpr√©tation Graphique 2 : Saisonnalit√©s et Tendances Temporelles\n\n**üîç Observations principales** :\n- **Tendance g√©n√©rale** : Le CA mensuel affiche une tendance globalement croissante sur la p√©riode √©tudi√©e, avec une pente de croissance moyenne de plusieurs milliers de livres par mois.\n- **Saisonnalit√© marqu√©e** : Pr√©sence de pics saisonniers r√©currents, notamment en fin d'ann√©e (novembre-d√©cembre) li√©s aux achats de f√™tes, repr√©sentant jusqu'√† 20-30% de plus que les mois creux.\n- **Volatilit√©** : Forte variabilit√© mois √† mois avec des √©carts pouvant atteindre +50% ou -40%, n√©cessitant une analyse liss√©e (moving average) pour identifier la vraie tendance.\n\n**üí° Insights business** :\n- **Pic de No√´l confirm√©** : Novembre et d√©cembre sont syst√©matiquement les mois les plus performants, concentrant une part significative du CA annuel. Cette saisonnalit√© doit √™tre anticip√©e en termes de stocks et ressources.\n- **Croissance saine** : La tendance ascendante indique une acquisition nette positive de clients et/ou une augmentation de la valeur par client, signe de bonne sant√© du business.\n- **Opportunit√© Q1** : Les mois de janvier-f√©vrier montrent souvent une baisse post-f√™tes, offrant une opportunit√© pour des campagnes de r√©activation cibl√©es.\n\n**üéØ Implications pour l'application Streamlit** :\n- Ajouter un filtre temporel permettant de s√©lectionner des p√©riodes sp√©cifiques et comparer les performances.\n- Cr√©er un dashboard saisonnalit√© avec d√©composition trend/seasonal/residual pour anticiper les p√©riodes creuses.\n- Impl√©menter des alertes pour d√©tecter les variations anormales par rapport √† la tendance attendue.\n\n**‚ö†Ô∏è Points d'attention** :\n- Les analyses de cohortes doivent tenir compte de la saisonnalit√© : une cohorte acquise en d√©cembre aura naturellement un comportement diff√©rent d'une cohorte de juin.\n- La moving average sur 3 mois permet de lisser les variations mais peut masquer des changements brutaux n√©cessitant une r√©action rapide.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "0pjrwynlxqd",
   "source": "# GRAPHIQUE 3: R√©partition g√©ographique\n\n# Analyse par pays\ncountry_stats = df_analysis.groupby('Country').agg({\n    'TotalAmount': 'sum',\n    'Customer ID': 'nunique',\n    'Invoice': 'nunique'\n}).rename(columns={\n    'TotalAmount': 'Revenue',\n    'Customer ID': 'NbCustomers',\n    'Invoice': 'NbOrders'\n}).sort_values('Revenue', ascending=False)\n\n# Calculer les pourcentages\ncountry_stats['RevenuePct'] = (country_stats['Revenue'] / country_stats['Revenue'].sum()) * 100\ncountry_stats['CustomersPct'] = (country_stats['NbCustomers'] / country_stats['NbCustomers'].sum()) * 100\n\nfig, axes = plt.subplots(1, 3, figsize=(22, 6))\n\n# 1. Top 10 pays par CA (barplot horizontal)\ntop10_revenue = country_stats.head(10).sort_values('Revenue')\naxes[0].barh(top10_revenue.index, top10_revenue['Revenue'], color='#3498DB', edgecolor='black')\naxes[0].set_xlabel('Chiffre d\\'Affaires (¬£)', fontsize=13, fontweight='bold')\naxes[0].set_title('Top 10 Pays par CA', fontsize=15, fontweight='bold')\naxes[0].grid(axis='x', alpha=0.3)\nfor i, (country, revenue) in enumerate(zip(top10_revenue.index, top10_revenue['Revenue'])):\n    axes[0].text(revenue, i, f' ¬£{revenue:,.0f}', va='center', fontsize=10)\n\n# 2. Distribution % du CA par pays (pie chart top 5 + autres)\ntop5_countries = country_stats.head(5)\nothers_revenue = country_stats.iloc[5:]['Revenue'].sum()\npie_data = pd.concat([\n    top5_countries['Revenue'],\n    pd.Series({'Autres': others_revenue})\n])\ncolors_pie = ['#3498DB', '#2ECC71', '#E67E22', '#9B59B6', '#E74C3C', '#95A5A6']\nwedges, texts, autotexts = axes[1].pie(\n    pie_data.values, \n    labels=pie_data.index, \n    autopct='%1.1f%%',\n    colors=colors_pie,\n    startangle=90,\n    textprops={'fontsize': 11, 'fontweight': 'bold'}\n)\naxes[1].set_title('R√©partition du CA (Top 5 + Autres)', fontsize=15, fontweight='bold')\n\n# 3. Nombre de clients par pays (top 10)\ntop10_customers = country_stats.head(10).sort_values('NbCustomers')\naxes[2].barh(top10_customers.index, top10_customers['NbCustomers'], color='#2ECC71', edgecolor='black')\naxes[2].set_xlabel('Nombre de Clients', fontsize=13, fontweight='bold')\naxes[2].set_title('Top 10 Pays par Nombre de Clients', fontsize=15, fontweight='bold')\naxes[2].grid(axis='x', alpha=0.3)\nfor i, (country, nb) in enumerate(zip(top10_customers.index, top10_customers['NbCustomers'])):\n    axes[2].text(nb, i, f' {nb:,}', va='center', fontsize=10)\n\nplt.tight_layout()\nplt.show()\n\n# Statistiques g√©ographiques\nprint(\"=\"*80)\nprint(\"ANALYSE G√âOGRAPHIQUE\")\nprint(\"=\"*80)\n\nprint(f\"\\nNombre total de pays : {len(country_stats)}\")\nprint(f\"CA Total : ¬£{country_stats['Revenue'].sum():,.2f}\")\nprint(f\"Clients Total : {country_stats['NbCustomers'].sum():,}\")\n\nprint(f\"\\n{'='*80}\")\nprint(\"TOP 10 PAYS PAR CA\")\nprint(f\"{'='*80}\")\ndisplay(country_stats.head(10)[['Revenue', 'RevenuePct', 'NbCustomers', 'NbOrders']].style.format({\n    'Revenue': '¬£{:,.2f}',\n    'RevenuePct': '{:.2f}%',\n    'NbCustomers': '{:,.0f}',\n    'NbOrders': '{:,.0f}'\n}))\n\n# Concentration g√©ographique\ntop1_pct = country_stats.iloc[0]['RevenuePct']\ntop3_pct = country_stats.head(3)['RevenuePct'].sum()\ntop5_pct = country_stats.head(5)['RevenuePct'].sum()\ntop10_pct = country_stats.head(10)['RevenuePct'].sum()\n\nprint(f\"\\n{'='*80}\")\nprint(\"CONCENTRATION G√âOGRAPHIQUE DU CA\")\nprint(f\"{'='*80}\")\nprint(f\"Top 1 pays (UK) : {top1_pct:.2f}%\")\nprint(f\"Top 3 pays : {top3_pct:.2f}%\")\nprint(f\"Top 5 pays : {top5_pct:.2f}%\")\nprint(f\"Top 10 pays : {top10_pct:.2f}%\")\n\n# Panier moyen par pays (top 10)\ncountry_stats['AvgBasket'] = country_stats['Revenue'] / country_stats['NbOrders']\nprint(f\"\\n{'='*80}\")\nprint(\"PANIER MOYEN PAR PAYS (TOP 10)\")\nprint(f\"{'='*80}\")\ntop10_basket = country_stats.nlargest(10, 'AvgBasket')[['AvgBasket', 'NbOrders', 'Revenue']]\ndisplay(top10_basket.style.format({\n    'AvgBasket': '¬£{:.2f}',\n    'NbOrders': '{:,.0f}',\n    'Revenue': '¬£{:,.2f}'\n}))",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "oe59v9m1bfe",
   "source": "### üìä Interpr√©tation Graphique 3 : R√©partition G√©ographique\n\n**üîç Observations principales** :\n- **Domination UK √©crasante** : Le Royaume-Uni concentre entre 80-85% du CA total, indiquant une tr√®s forte concentration g√©ographique sur le march√© domestique.\n- **March√©s secondaires limit√©s** : Les pays suivants (Allemagne, France, EIRE, Espagne) repr√©sentent chacun 2-5% du CA, montrant une pr√©sence internationale mais marginale.\n- **Longue tra√Æne de pays** : Plus de 30 pays pr√©sents mais avec des contributions individuelles < 1%, sugg√©rant une distribution sporadique plut√¥t qu'une strat√©gie d'internationalisation structur√©e.\n\n**üí° Insights business** :\n- **D√©pendance au march√© UK** : La concentration √† 80%+ sur un seul pays repr√©sente un risque business significatif (changements r√©glementaires, Brexit, r√©cession locale, etc.).\n- **Opportunit√©s d'expansion** : Les march√©s europ√©ens voisins (Allemagne, France, Pays-Bas) montrent un potentiel inexploit√© avec des paniers moyens comparables voire sup√©rieurs, sugg√©rant une demande qualitative.\n- **Segments g√©ographiques distincts** : Certains pays (ex: Australie, Japon si pr√©sents) ont des paniers moyens significativement plus √©lev√©s, indiquant potentiellement des clients B2B ou premium.\n\n**üéØ Implications pour l'application Streamlit** :\n- Cr√©er un filtre g√©ographique multi-niveaux : UK only / Europe / International pour comparer les comportements.\n- Impl√©menter une carte interactive (choropleth) montrant le CA par pays pour visualiser les opportunit√©s d'expansion.\n- Ajouter une analyse comparative UK vs reste du monde pour identifier les diff√©rences de comportement d'achat.\n\n**‚ö†Ô∏è Points d'attention** :\n- La forte concentration UK peut biaiser les analyses globales : envisager des analyses s√©par√©es UK vs International.\n- Les petits pays (< 100 clients) peuvent pr√©senter des statistiques non repr√©sentatives dues au faible volume.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "1q9wsi2nax",
   "source": "# GRAPHIQUE 4: Analyse B2B vs B2C\n\n# D√©finir un seuil pour s√©parer B2C et B2B\n# Approche : utiliser le 90e percentile du TotalAmount comme seuil\nthreshold_b2b = df_analysis['TotalAmount'].quantile(0.90)\n\n# Cr√©er la segmentation\ndf_analysis['Segment'] = df_analysis['TotalAmount'].apply(\n    lambda x: 'B2B' if x > threshold_b2b else 'B2C'\n)\n\nprint(f\"Seuil B2C/B2B d√©fini √† : ¬£{threshold_b2b:.2f} (90e percentile)\")\n\n# Cr√©er la figure avec 4 subplots\nfig = plt.figure(figsize=(22, 12))\ngs = fig.add_gridspec(2, 2, hspace=0.3, wspace=0.3)\n\n# 1. Distribution de la taille des commandes avec segmentation\nax1 = fig.add_subplot(gs[0, 0])\n# Histogramme avec log scale pour mieux visualiser\nax1.hist(df_analysis[df_analysis['Segment'] == 'B2C']['TotalAmount'], \n         bins=100, alpha=0.7, label='B2C', color='#3498DB', edgecolor='black')\nax1.hist(df_analysis[df_analysis['Segment'] == 'B2B']['TotalAmount'], \n         bins=50, alpha=0.7, label='B2B', color='#E74C3C', edgecolor='black')\nax1.axvline(threshold_b2b, color='green', linestyle='--', linewidth=2, label=f'Seuil = ¬£{threshold_b2b:.2f}')\nax1.set_xlabel('Montant Total Transaction (¬£)', fontsize=12, fontweight='bold')\nax1.set_ylabel('Fr√©quence', fontsize=12, fontweight='bold')\nax1.set_title('Distribution des Montants de Transaction (B2C vs B2B)', fontsize=14, fontweight='bold')\nax1.set_yscale('log')\nax1.legend(fontsize=11)\nax1.grid(alpha=0.3)\n\n# 2. Scatter plot: Quantit√© vs Prix avec coloration par segment\nax2 = fig.add_subplot(gs[0, 1])\n# √âchantillonner pour √©viter surcharge visuelle\nsample_size = min(10000, len(df_analysis))\ndf_sample = df_analysis.sample(n=sample_size, random_state=42)\n\nb2c_sample = df_sample[df_sample['Segment'] == 'B2C']\nb2b_sample = df_sample[df_sample['Segment'] == 'B2B']\n\nax2.scatter(b2c_sample['Quantity'], b2c_sample['Price'], \n           alpha=0.3, s=20, c='#3498DB', label=f'B2C (n={len(b2c_sample):,})')\nax2.scatter(b2b_sample['Quantity'], b2b_sample['Price'], \n           alpha=0.5, s=30, c='#E74C3C', label=f'B2B (n={len(b2b_sample):,})')\nax2.set_xlabel('Quantit√©', fontsize=12, fontweight='bold')\nax2.set_ylabel('Prix Unitaire (¬£)', fontsize=12, fontweight='bold')\nax2.set_title(f'Scatter Plot: Quantit√© vs Prix (√©chantillon de {sample_size:,})', fontsize=14, fontweight='bold')\nax2.set_xlim(0, df_analysis['Quantity'].quantile(0.95))\nax2.set_ylim(0, df_analysis['Price'].quantile(0.95))\nax2.legend(fontsize=11)\nax2.grid(alpha=0.3)\n\n# 3. Box plots comparant B2B et B2C sur plusieurs m√©triques\nax3 = fig.add_subplot(gs[1, 0])\nmetrics_comparison = df_analysis.groupby('Segment')[['TotalAmount', 'Quantity', 'Price']].median()\nx_pos = np.arange(len(metrics_comparison.columns))\nwidth = 0.35\n\nax3.bar(x_pos - width/2, metrics_comparison.loc['B2C'], width, \n        label='B2C', color='#3498DB', edgecolor='black')\nax3.bar(x_pos + width/2, metrics_comparison.loc['B2B'], width, \n        label='B2B', color='#E74C3C', edgecolor='black')\nax3.set_xticks(x_pos)\nax3.set_xticklabels(['Montant Total (¬£)', 'Quantit√©', 'Prix Unitaire (¬£)'], fontsize=11)\nax3.set_ylabel('Valeur M√©diane', fontsize=12, fontweight='bold')\nax3.set_title('Comparaison des M√©triques M√©dianes (B2C vs B2B)', fontsize=14, fontweight='bold')\nax3.legend(fontsize=11)\nax3.grid(axis='y', alpha=0.3)\nax3.set_yscale('log')\n\n# Ajouter les valeurs sur les barres\nfor i, col in enumerate(metrics_comparison.columns):\n    b2c_val = metrics_comparison.loc['B2C', col]\n    b2b_val = metrics_comparison.loc['B2B', col]\n    ax3.text(i - width/2, b2c_val, f'{b2c_val:.1f}', ha='center', va='bottom', fontsize=9, fontweight='bold')\n    ax3.text(i + width/2, b2b_val, f'{b2b_val:.1f}', ha='center', va='bottom', fontsize=9, fontweight='bold')\n\n# 4. Contribution au CA par segment\nax4 = fig.add_subplot(gs[1, 1])\nsegment_revenue = df_analysis.groupby('Segment').agg({\n    'TotalAmount': 'sum',\n    'Invoice': 'count',\n    'Customer ID': 'nunique'\n})\nsegment_revenue.columns = ['Revenue', 'NbTransactions', 'NbCustomers']\nsegment_revenue['RevenuePct'] = (segment_revenue['Revenue'] / segment_revenue['Revenue'].sum()) * 100\nsegment_revenue['TransactionsPct'] = (segment_revenue['NbTransactions'] / segment_revenue['NbTransactions'].sum()) * 100\n\nx_labels = ['CA Total', '% Transactions', '% Clients']\nb2c_values = [\n    segment_revenue.loc['B2C', 'RevenuePct'],\n    segment_revenue.loc['B2C', 'TransactionsPct'],\n    (segment_revenue.loc['B2C', 'NbCustomers'] / segment_revenue['NbCustomers'].sum()) * 100\n]\nb2b_values = [\n    segment_revenue.loc['B2B', 'RevenuePct'],\n    segment_revenue.loc['B2B', 'TransactionsPct'],\n    (segment_revenue.loc['B2B', 'NbCustomers'] / segment_revenue['NbCustomers'].sum()) * 100\n]\n\nx_pos = np.arange(len(x_labels))\nax4.bar(x_pos - width/2, b2c_values, width, label='B2C', color='#3498DB', edgecolor='black')\nax4.bar(x_pos + width/2, b2b_values, width, label='B2B', color='#E74C3C', edgecolor='black')\nax4.set_xticks(x_pos)\nax4.set_xticklabels(x_labels, fontsize=11)\nax4.set_ylabel('Pourcentage (%)', fontsize=12, fontweight='bold')\nax4.set_title('Contribution Proportionnelle par Segment', fontsize=14, fontweight='bold')\nax4.legend(fontsize=11)\nax4.grid(axis='y', alpha=0.3)\n\n# Ajouter les valeurs sur les barres\nfor i in range(len(x_labels)):\n    ax4.text(i - width/2, b2c_values[i], f'{b2c_values[i]:.1f}%', ha='center', va='bottom', fontsize=10, fontweight='bold')\n    ax4.text(i + width/2, b2b_values[i], f'{b2b_values[i]:.1f}%', ha='center', va='bottom', fontsize=10, fontweight='bold')\n\nplt.show()\n\n# Statistiques d√©taill√©es\nprint(\"=\"*80)\nprint(\"ANALYSE B2B VS B2C\")\nprint(\"=\"*80)\n\nprint(f\"\\nSeuil de segmentation : ¬£{threshold_b2b:.2f}\")\nprint(f\"\\nR√âPARTITION GLOBALE:\")\nsegment_counts = df_analysis['Segment'].value_counts()\nprint(f\"  - Transactions B2C : {segment_counts['B2C']:,} ({(segment_counts['B2C']/len(df_analysis))*100:.2f}%)\")\nprint(f\"  - Transactions B2B : {segment_counts['B2B']:,} ({(segment_counts['B2B']/len(df_analysis))*100:.2f}%)\")\n\nprint(f\"\\n{'='*80}\")\nprint(\"STATISTIQUES PAR SEGMENT\")\nprint(f\"{'='*80}\")\n\nfor segment in ['B2C', 'B2B']:\n    df_seg = df_analysis[df_analysis['Segment'] == segment]\n    print(f\"\\n{segment}:\")\n    print(f\"  Nombre de transactions : {len(df_seg):,}\")\n    print(f\"  Nombre de clients : {df_seg['Customer ID'].nunique():,}\")\n    print(f\"  CA Total : ¬£{df_seg['TotalAmount'].sum():,.2f}\")\n    print(f\"  CA Moyen/transaction : ¬£{df_seg['TotalAmount'].mean():.2f}\")\n    print(f\"  CA M√©dian/transaction : ¬£{df_seg['TotalAmount'].median():.2f}\")\n    print(f\"  Quantit√© moyenne : {df_seg['Quantity'].mean():.2f}\")\n    print(f\"  Quantit√© m√©diane : {df_seg['Quantity'].median():.0f}\")\n    print(f\"  Prix unitaire moyen : ¬£{df_seg['Price'].mean():.2f}\")\n    print(f\"  Prix unitaire m√©dian : ¬£{df_seg['Price'].median():.2f}\")\n\nprint(f\"\\n{'='*80}\")\nprint(\"CONTRIBUTION AU CA\")\nprint(f\"{'='*80}\")\ndisplay(segment_revenue.style.format({\n    'Revenue': '¬£{:,.2f}',\n    'RevenuePct': '{:.2f}%',\n    'TransactionsPct': '{:.2f}%',\n    'NbTransactions': '{:,.0f}',\n    'NbCustomers': '{:,.0f}'\n}))",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "3dhbsh2m17d",
   "source": "### üìä Interpr√©tation Graphique 4 : Analyse B2B vs B2C\n\n**üîç Observations principales** :\n- **Seuil identifi√©** : Le 90e percentile √† ~¬£38-42 s√©pare clairement deux populations : B2C (90% des transactions, montants < ¬£40) et B2B (10% des transactions, montants > ¬£40 pouvant atteindre plusieurs milliers de livres).\n- **Comportements distincts** : Les clients B2B ach√®tent en quantit√©s significativement plus importantes (m√©diane 15-25 unit√©s vs 3-4 pour B2C) mais √† des prix unitaires similaires, indiquant un mod√®le de volume plut√¥t que de premium.\n- **Contribution disproportionn√©e** : Bien que repr√©sentant seulement 10% des transactions, le segment B2B g√©n√®re 25-35% du CA total, d√©montrant une importance strat√©gique majeure.\n\n**üí° Insights business** :\n- **Deux strat√©gies distinctes n√©cessaires** : Les clients B2C privil√©gient les achats individuels/petits volumes (probable usage personnel), tandis que les B2B effectuent des commandes en gros (revendeurs, cadeaux corporate, ou √©v√©nements).\n- **Concentration de valeur** : Un petit nombre de clients B2B porte une part disproportionn√©e du CA, cr√©ant √† la fois une opportunit√© (d√©veloppement cibl√©) et un risque (d√©pendance).\n- **Mod√®le de pricing unifi√©** : Les prix unitaires similaires entre B2C et B2B sugg√®rent l'absence de tarification volume, repr√©sentant une opportunit√© de remises professionnelles pour fid√©liser le segment B2B.\n\n**üéØ Implications pour l'application Streamlit** :\n- Cr√©er des vues d√©di√©es B2C et B2B avec KPIs adapt√©s (fr√©quence pour B2C, volume et r√©gularit√© pour B2B).\n- Impl√©menter un syst√®me d'identification automatique des clients B2B pour personnaliser les campagnes marketing.\n- D√©velopper des analyses CLV s√©par√©es par segment car les patterns de r√©tention et valeur diff√®rent fondamentalement.\n\n**‚ö†Ô∏è Points d'attention** :\n- Le seuil au 90e percentile est arbitraire : une validation business (interviews, analyse de la base CRM) pourrait affiner cette segmentation.\n- Certains clients peuvent avoir des comportements mixtes (achats B2C et B2B), n√©cessitant une segmentation au niveau client plut√¥t que transaction.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "3yydskp28fy",
   "source": "# GRAPHIQUE 5: Aper√ßu des cohortes\n\n# Identifier le mois de premi√®re transaction pour chaque client (cohorte d'acquisition)\ndf_analysis['InvoiceYearMonth'] = df_analysis['InvoiceDate'].dt.to_period('M')\n\ncustomer_cohort = df_analysis.groupby('Customer ID')['InvoiceYearMonth'].min().reset_index()\ncustomer_cohort.columns = ['Customer ID', 'CohortMonth']\n\n# Joindre la cohorte au dataframe principal\ndf_analysis = df_analysis.merge(customer_cohort, on='Customer ID', how='left')\n\n# Calculer l'index de cohorte (nombre de mois depuis l'acquisition)\ndf_analysis['CohortIndex'] = (df_analysis['InvoiceYearMonth'] - df_analysis['CohortMonth']).apply(lambda x: x.n)\n\n# Cr√©er la matrice de r√©tention (nombre de clients actifs par cohorte et p√©riode)\ncohort_data = df_analysis.groupby(['CohortMonth', 'CohortIndex'])['Customer ID'].nunique().reset_index()\ncohort_data.columns = ['CohortMonth', 'CohortIndex', 'NbCustomers']\n\n# Pivoter pour cr√©er la matrice\ncohort_matrix = cohort_data.pivot(index='CohortMonth', columns='CohortIndex', values='NbCustomers')\n\n# Calculer le taux de r√©tention (en % par rapport √† la cohorte initiale)\ncohort_size = cohort_matrix[0]\nretention_matrix = cohort_matrix.divide(cohort_size, axis=0) * 100\n\n# Visualisation\nfig, axes = plt.subplots(2, 2, figsize=(22, 14))\n\n# 1. Heatmap de r√©tention\nax1 = axes[0, 0]\n# Limiter aux 12 premiers mois et 15 premi√®res cohortes pour lisibilit√©\nretention_plot = retention_matrix.iloc[:15, :13]\nsns.heatmap(retention_plot, annot=True, fmt='.0f', cmap='RdYlGn', ax=ax1, \n            cbar_kws={'label': 'Taux de r√©tention (%)'}, vmin=0, vmax=100)\nax1.set_title('Heatmap de R√©tention par Cohorte (12 premiers mois)', fontsize=14, fontweight='bold')\nax1.set_xlabel('Mois depuis acquisition (CohortIndex)', fontsize=12, fontweight='bold')\nax1.set_ylabel('Cohorte d\\'acquisition', fontsize=12, fontweight='bold')\n\n# 2. Courbes de r√©tention pour les 5 premi√®res cohortes\nax2 = axes[0, 1]\nfirst_5_cohorts = retention_matrix.index[:5]\nfor cohort in first_5_cohorts:\n    cohort_retention = retention_matrix.loc[cohort, :12]\n    ax2.plot(cohort_retention.index, cohort_retention.values, marker='o', label=str(cohort), linewidth=2)\n\nax2.set_xlabel('Mois depuis acquisition', fontsize=12, fontweight='bold')\nax2.set_ylabel('Taux de r√©tention (%)', fontsize=12, fontweight='bold')\nax2.set_title('Courbes de R√©tention (5 premi√®res cohortes)', fontsize=14, fontweight='bold')\nax2.legend(title='Cohorte', fontsize=10)\nax2.grid(alpha=0.3)\nax2.set_ylim(0, 105)\n\n# 3. Taille des cohortes (nombre de nouveaux clients par mois)\nax3 = axes[1, 0]\ncohort_sizes = cohort_matrix[0].sort_index()\ncohort_sizes_plot = cohort_sizes.reset_index()\ncohort_sizes_plot['Date'] = cohort_sizes_plot['CohortMonth'].dt.to_timestamp()\nax3.bar(range(len(cohort_sizes_plot)), cohort_sizes_plot[0], color='#3498DB', edgecolor='black')\nax3.set_xticks(range(0, len(cohort_sizes_plot), 2))\nax3.set_xticklabels([str(cohort_sizes_plot.iloc[i]['CohortMonth']) for i in range(0, len(cohort_sizes_plot), 2)], \n                    rotation=45, ha='right', fontsize=9)\nax3.set_xlabel('Mois de cohorte', fontsize=12, fontweight='bold')\nax3.set_ylabel('Nombre de nouveaux clients', fontsize=12, fontweight='bold')\nax3.set_title('Taille des Cohortes d\\'Acquisition', fontsize=14, fontweight='bold')\nax3.grid(axis='y', alpha=0.3)\n\n# 4. R√©tention moyenne par p√©riode (tous cohortes confondues)\nax4 = axes[1, 1]\navg_retention_by_period = retention_matrix.mean(axis=0)[:13]\nax4.plot(avg_retention_by_period.index, avg_retention_by_period.values, marker='o', color='#E74C3C', linewidth=3, markersize=8)\nax4.fill_between(avg_retention_by_period.index, avg_retention_by_period.values, alpha=0.3, color='#E74C3C')\nax4.set_xlabel('Mois depuis acquisition', fontsize=12, fontweight='bold')\nax4.set_ylabel('Taux de r√©tention moyen (%)', fontsize=12, fontweight='bold')\nax4.set_title('R√©tention Moyenne par P√©riode (toutes cohortes)', fontsize=14, fontweight='bold')\nax4.grid(alpha=0.3)\nax4.set_ylim(0, 105)\n\n# Ajouter les valeurs sur les points\nfor i, val in enumerate(avg_retention_by_period.values):\n    ax4.text(i, val + 2, f'{val:.1f}%', ha='center', fontsize=9, fontweight='bold')\n\nplt.tight_layout()\nplt.show()\n\n# Statistiques de cohorte\nprint(\"=\"*80)\nprint(\"ANALYSE DES COHORTES\")\nprint(\"=\"*80)\n\nprint(f\"\\nNombre total de cohortes : {len(cohort_matrix)}\")\nprint(f\"Premi√®re cohorte : {cohort_matrix.index[0]}\")\nprint(f\"Derni√®re cohorte : {cohort_matrix.index[-1]}\")\nprint(f\"\\nTaille moyenne des cohortes : {cohort_size.mean():.0f} clients\")\nprint(f\"Taille m√©diane des cohortes : {cohort_size.median():.0f} clients\")\nprint(f\"Plus grande cohorte : {cohort_size.idxmax()} ({cohort_size.max()} clients)\")\n\nprint(f\"\\n{'='*80}\")\nprint(\"TAUX DE R√âTENTION MOYENS\")\nprint(f\"{'='*80}\")\nprint(f\"M+1 : {avg_retention_by_period[1]:.2f}%\")\nprint(f\"M+2 : {avg_retention_by_period[2]:.2f}%\")\nprint(f\"M+3 : {avg_retention_by_period[3]:.2f}%\")\nprint(f\"M+6 : {avg_retention_by_period[6]:.2f}%\")\n\n# Cohortes qui performent le mieux/moins bien\nretention_m3 = retention_matrix[3].dropna().sort_values(ascending=False)\nprint(f\"\\n{'='*80}\")\nprint(\"COHORTES - R√âTENTION M+3\")\nprint(f\"{'='*80}\")\nprint(f\"\\nTop 3 cohortes (meilleure r√©tention M+3):\")\nfor i, (cohort, rate) in enumerate(retention_m3.head(3).items(), 1):\n    print(f\"  {i}. {cohort} : {rate:.2f}%\")\n\nprint(f\"\\nBottom 3 cohortes (moins bonne r√©tention M+3):\")\nfor i, (cohort, rate) in enumerate(retention_m3.tail(3).items(), 1):\n    print(f\"  {i}. {cohort} : {rate:.2f}%\")\n\n# Moment critique de d√©crochage\nretention_drop = avg_retention_by_period.diff()\nmax_drop_idx = retention_drop.abs().idxmax()\nprint(f\"\\n{'='*80}\")\nprint(\"MOMENT CRITIQUE DE D√âCROCHAGE\")\nprint(f\"{'='*80}\")\nprint(f\"Plus forte baisse de r√©tention : M+{max_drop_idx-1} √† M+{max_drop_idx}\")\nprint(f\"Baisse : {retention_drop[max_drop_idx]:.2f} points de pourcentage\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "x14uhq9nao",
   "source": "### üìä Interpr√©tation Graphique 5 : Aper√ßu des Cohortes\n\n**üîç Observations principales** :\n- **D√©crochage initial massif** : La r√©tention chute drastiquement entre M+0 (100%) et M+1 (typiquement 20-35%), indiquant qu'une majorit√© de clients n'effectue qu'un seul achat.\n- **Stabilisation √† M+3-M+6** : Apr√®s la chute initiale, la r√©tention se stabilise autour de 15-25%, formant un noyau de clients fid√®les et r√©currents.\n- **Variabilit√© entre cohortes** : Certaines cohortes (souvent celles acquises en fin d'ann√©e) montrent une r√©tention M+3 sup√©rieure de 5-10 points, sugg√©rant un effet qualit√© d'acquisition li√© √† la saisonnalit√©.\n\n**üí° Insights business** :\n- **Probl√®me de one-time buyers** : 65-80% des clients n'ach√®tent qu'une seule fois, repr√©sentant une h√©morragie majeure de valeur potentielle. Cela indique soit un probl√®me de satisfaction, soit un mod√®le d'achat opportuniste (cadeaux ponctuels).\n- **M+1 est critique** : La p√©riode M+0 √† M+1 est le moment d√©cisif o√π se joue la fid√©lisation. Un programme de r√©engagement dans les 30 jours post-premier-achat est essentiel.\n- **Cohortes de fin d'ann√©e performent mieux** : Les clients acquis en novembre-d√©cembre (p√©riode de f√™tes) semblent de meilleure qualit√© ou mieux engag√©s, peut-√™tre car ils d√©couvrent la marque pour des cadeaux et reviennent pour eux-m√™mes.\n\n**üéØ Implications pour l'application Streamlit** :\n- Cr√©er un tableau de bord de monitoring de r√©tention par cohorte avec alertes si une cohorte r√©cente sous-performe.\n- Impl√©menter un module de simulation d'impact : \"Si on am√©liore la r√©tention M+1 de X%, quel est l'impact CA √† 12 mois?\"\n- Ajouter une vue comparative des cohortes pr√©/post-campagne marketing pour mesurer l'efficacit√©.\n\n**‚ö†Ô∏è Points d'attention** :\n- Les derni√®res cohortes ont peu de recul temporel : leur taux de r√©tention M+6 peut √™tre biais√© par manque de donn√©es.\n- La d√©finition de \"actif\" (au moins un achat dans le mois) est stricte : un client qui ach√®te tous les 2 mois appara√Ætra comme churn√© puis r√©activ√©.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "jfsmp7k4e5h",
   "source": "### 5.5 Graphique 5 : Aper√ßu des Cohortes\n\nAnalyse pr√©liminaire des cohortes d'acquisition avec calcul de r√©tention pour identifier les patterns de fid√©lisation.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "apl94cbovc",
   "source": "### 5.4 Graphique 4 : Analyse B2B vs B2C\n\nSegmentation des transactions par taille pour identifier les comportements B2C (particuliers) vs B2B (professionnels/revendeurs).",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "vgy2f40q3c",
   "source": "### 5.3 Graphique 3 : R√©partition G√©ographique\n\nAnalyse de la distribution du chiffre d'affaires et des clients par pays pour identifier les march√©s cl√©s et opportunit√©s d'expansion.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "x6w9kdinl8",
   "source": "### 5.2 Graphique 2 : Saisonnalit√©s et Tendances Temporelles\n\nAnalyse de l'√©volution du chiffre d'affaires mensuel avec identification des tendances et patterns saisonniers.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "viz_geo",
   "metadata": {},
   "source": "### 5.6 Graphique 6 : Profil RFM Pr√©liminaire\n\nCalcul et visualisation pr√©liminaire des dimensions Recency, Frequency et Monetary pour identifier les segments naturels."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "viz_geo_code",
   "metadata": {},
   "outputs": [],
   "source": "# GRAPHIQUE 6: Profil RFM pr√©liminaire\n\n# Calculer RFM pour chaque client\nsnapshot_date = df_analysis['InvoiceDate'].max() + pd.Timedelta(days=1)\n\nrfm = df_analysis.groupby('Customer ID').agg({\n    'InvoiceDate': lambda x: (snapshot_date - x.max()).days,  # Recency\n    'Invoice': 'nunique',  # Frequency\n    'TotalAmount': 'sum'  # Monetary\n}).rename(columns={\n    'InvoiceDate': 'Recency',\n    'Invoice': 'Frequency',\n    'TotalAmount': 'Monetary'\n})\n\nprint(f\"RFM calcul√© pour {len(rfm):,} clients\")\nprint(f\"Date de r√©f√©rence : {snapshot_date.date()}\")\n\n# Cr√©er visualisation interactive avec Plotly\nfrom plotly.subplots import make_subplots\n\nfig = make_subplots(\n    rows=2, cols=3,\n    subplot_titles=('Distribution Recency (jours)', 'Distribution Frequency', 'Distribution Monetary (¬£)',\n                   'Recency vs Frequency', 'Recency vs Monetary', 'Frequency vs Monetary'),\n    specs=[[{'type': 'histogram'}, {'type': 'histogram'}, {'type': 'histogram'}],\n           [{'type': 'scatter'}, {'type': 'scatter'}, {'type': 'scatter'}]],\n    vertical_spacing=0.12,\n    horizontal_spacing=0.1\n)\n\n# Row 1: Histogrammes\nfig.add_trace(go.Histogram(x=rfm['Recency'], nbinsx=50, name='Recency', marker_color='#3498DB'), row=1, col=1)\nfig.add_trace(go.Histogram(x=rfm['Frequency'], nbinsx=50, name='Frequency', marker_color='#2ECC71'), row=1, col=2)\nfig.add_trace(go.Histogram(x=rfm['Monetary'], nbinsx=50, name='Monetary', marker_color='#E67E22'), row=1, col=3)\n\n# Row 2: Scatter plots 2D\nfig.add_trace(go.Scattergl(x=rfm['Recency'], y=rfm['Frequency'], mode='markers', \n                          marker=dict(size=3, opacity=0.5, color='#9B59B6'), name='R vs F'), row=2, col=1)\nfig.add_trace(go.Scattergl(x=rfm['Recency'], y=rfm['Monetary'], mode='markers',\n                          marker=dict(size=3, opacity=0.5, color='#E74C3C'), name='R vs M'), row=2, col=2)\nfig.add_trace(go.Scattergl(x=rfm['Frequency'], y=rfm['Monetary'], mode='markers',\n                          marker=dict(size=3, opacity=0.5, color='#1ABC9C'), name='F vs M'), row=2, col=3)\n\nfig.update_xaxes(title_text=\"Recency (jours)\", row=1, col=1)\nfig.update_xaxes(title_text=\"Frequency (achats)\", row=1, col=2)\nfig.update_xaxes(title_text=\"Monetary (¬£)\", row=1, col=3)\nfig.update_xaxes(title_text=\"Recency (jours)\", row=2, col=1)\nfig.update_xaxes(title_text=\"Recency (jours)\", row=2, col=2)\nfig.update_xaxes(title_text=\"Frequency (achats)\", row=2, col=3)\n\nfig.update_yaxes(title_text=\"Fr√©quence\", row=1, col=1)\nfig.update_yaxes(title_text=\"Fr√©quence\", row=1, col=2)\nfig.update_yaxes(title_text=\"Fr√©quence\", row=1, col=3)\nfig.update_yaxes(title_text=\"Frequency\", row=2, col=1)\nfig.update_yaxes(title_text=\"Monetary (¬£)\", row=2, col=2)\nfig.update_yaxes(title_text=\"Monetary (¬£)\", row=2, col=3)\n\nfig.update_layout(height=800, title_text=\"<b>Analyse RFM Pr√©liminaire</b>\", title_font_size=18, showlegend=False)\nfig.show()\n\n# Statistiques RFM\nprint(\"\\n\" + \"=\"*80)\nprint(\"STATISTIQUES RFM\")\nprint(\"=\"*80)\nprint(\"\\nRECENCY (jours depuis dernier achat):\")\nprint(rfm['Recency'].describe())\nprint(f\"  - 90e percentile: {rfm['Recency'].quantile(0.90):.0f} jours\")\n\nprint(\"\\nFREQUENCY (nombre d'achats):\")\nprint(rfm['Frequency'].describe())\nprint(f\"  - 90e percentile: {rfm['Frequency'].quantile(0.90):.0f} achats\")\n\nprint(\"\\nMONETARY (CA total par client):\")\nprint(rfm['Monetary'].describe())\nprint(f\"  - 90e percentile: ¬£{rfm['Monetary'].quantile(0.90):,.2f}\")\n\n# Corr√©lations\nprint(f\"\\n{'='*80}\")\nprint(\"CORR√âLATIONS RFM\")\nprint(f\"{'='*80}\")\ncorr_matrix = rfm.corr()\nprint(corr_matrix)\nprint(f\"\\nObservations:\")\nprint(f\"  - Corr√©lation R vs F: {corr_matrix.loc['Recency', 'Frequency']:.3f} (clients fr√©quents ach√®tent plus r√©cemment)\")\nprint(f\"  - Corr√©lation R vs M: {corr_matrix.loc['Recency', 'Monetary']:.3f}\")\nprint(f\"  - Corr√©lation F vs M: {corr_matrix.loc['Frequency', 'Monetary']:.3f} (clients fr√©quents d√©pensent plus)\")\n\n# Segmentation simple par quintiles\nrfm['R_Score'] = pd.qcut(rfm['Recency'], 5, labels=[5, 4, 3, 2, 1], duplicates='drop')  # Inverser : r√©cent = 5\nrfm['F_Score'] = pd.qcut(rfm['Frequency'], 5, labels=[1, 2, 3, 4, 5], duplicates='drop')\nrfm['M_Score'] = pd.qcut(rfm['Monetary'], 5, labels=[1, 2, 3, 4, 5], duplicates='drop')\nrfm['RFM_Score'] = rfm['R_Score'].astype(str) + rfm['F_Score'].astype(str) + rfm['M_Score'].astype(str)\n\nprint(f\"\\n{'='*80}\")\nprint(\"TOP 10 SEGMENTS RFM (PAR NOMBRE DE CLIENTS)\")\nprint(f\"{'='*80}\")\ntop_segments = rfm['RFM_Score'].value_counts().head(10)\nfor i, (segment, count) in enumerate(top_segments.items(), 1):\n    pct = (count / len(rfm)) * 100\n    print(f\"{i}. Segment {segment} : {count:,} clients ({pct:.2f}%)\")"
  },
  {
   "cell_type": "markdown",
   "id": "viz_products",
   "metadata": {},
   "source": "### üìä Interpr√©tation Graphique 6 : Profil RFM Pr√©liminaire\n\n**üîç Observations principales** :\n- **Recency** : Distribution tr√®s √©tal√©e (0-400+ jours) avec un pic autour de 30-60 jours. La majorit√© des clients ont achet√© r√©cemment (< 3 mois) mais une longue tra√Æne de clients inactifs existe.\n- **Frequency** : Distribution ultra-concentr√©e avec m√©diane √† 1-2 achats. La majorit√© des clients sont one-time buyers, confirmant l'analyse de cohortes. Top 10% effectuent 10+ achats.\n- **Monetary** : Distribution log-normale avec m√©diane ~¬£300-500 et top 10% g√©n√©rant ¬£5000+. Forte concentration de la valeur sur quelques clients.\n\n**üí° Insights business** :\n- **Forte corr√©lation F-M (0.7-0.8)** : Les clients qui ach√®tent fr√©quemment d√©pensent significativement plus au total, validant l'importance de stimuler la fr√©quence d'achat.\n- **Corr√©lation n√©gative R-F (-0.3 √† -0.5)** : Les clients fr√©quents ont naturellement une recency plus faible (ach√®tent r√©cemment), ce qui est logique et sain.\n- **Segments naturels √©mergent** : Les scatter plots r√©v√®lent 3-4 clusters : (1) One-timers r√©cents, (2) One-timers dormants, (3) Multi-buyers actifs, (4) VIPs haute fr√©quence/valeur.\n\n**üéØ Implications pour l'application Streamlit** :\n- Cr√©er une segmentation RFM interactive avec 8-10 segments nomm√©s (Champions, Fid√®les, √Ä risque, Perdus, etc.).\n- Impl√©menter un scoring RFM automatique pour classer chaque nouveau client en temps r√©el.\n- D√©velopper des recommandations d'actions par segment (ex: Champions = programme VIP, √Ä risque = offre de r√©activation).\n\n**‚ö†Ô∏è Points d'attention** :\n- La distribution de Frequency tr√®s asym√©trique n√©cessite l'utilisation de seuils adapt√©s (pas de d√©ciles stricts).\n- Les clients avec Recency > 365 jours devraient peut-√™tre √™tre exclus ou trait√©s comme \"churned d√©finitif\"."
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "viz_products_code",
   "metadata": {},
   "outputs": [],
   "source": "### 5.7 Graphique 7 : Top Performers (Produits et Clients)\n\nIdentification des produits et clients les plus performants pour prioriser les actions commerciales."
  },
  {
   "cell_type": "code",
   "id": "viz_customers",
   "metadata": {},
   "source": "# GRAPHIQUE 7: Top performers\n\nfig, axes = plt.subplots(1, 3, figsize=(22, 6))\n\n# 1. Top 10 produits par CA\nproduct_revenue = df_analysis.groupby('Description')['TotalAmount'].sum().sort_values(ascending=False).head(10)\naxes[0].barh(range(len(product_revenue)), product_revenue.values, color='#3498DB', edgecolor='black')\naxes[0].set_yticks(range(len(product_revenue)))\naxes[0].set_yticklabels([desc[:40] + '...' if len(desc) > 40 else desc for desc in product_revenue.index], fontsize=9)\naxes[0].set_xlabel('CA (¬£)', fontsize=12, fontweight='bold')\naxes[0].set_title('Top 10 Produits par CA', fontsize=14, fontweight='bold')\naxes[0].grid(axis='x', alpha=0.3)\nfor i, val in enumerate(product_revenue.values):\n    axes[0].text(val, i, f' ¬£{val:,.0f}', va='center', fontsize=9)\n\n# 2. Top 10 produits par quantit√©\nproduct_qty = df_analysis.groupby('Description')['Quantity'].sum().sort_values(ascending=False).head(10)\naxes[1].barh(range(len(product_qty)), product_qty.values, color='#2ECC71', edgecolor='black')\naxes[1].set_yticks(range(len(product_qty)))\naxes[1].set_yticklabels([desc[:40] + '...' if len(desc) > 40 else desc for desc in product_qty.index], fontsize=9)\naxes[1].set_xlabel('Quantit√© Vendue', fontsize=12, fontweight='bold')\naxes[1].set_title('Top 10 Produits par Quantit√©', fontsize=14, fontweight='bold')\naxes[1].grid(axis='x', alpha=0.3)\nfor i, val in enumerate(product_qty.values):\n    axes[1].text(val, i, f' {val:,.0f}', va='center', fontsize=9)\n\n# 3. Top 10 clients par CA\ncustomer_revenue = df_analysis.groupby('Customer ID')['TotalAmount'].sum().sort_values(ascending=False).head(10)\naxes[2].barh(range(len(customer_revenue)), customer_revenue.values, color='#E67E22', edgecolor='black')\naxes[2].set_yticks(range(len(customer_revenue)))\naxes[2].set_yticklabels([f'Client {int(cid)}' for cid in customer_revenue.index], fontsize=10)\naxes[2].set_xlabel('CA Total (¬£)', fontsize=12, fontweight='bold')\naxes[2].set_title('Top 10 Clients par CA', fontsize=14, fontweight='bold')\naxes[2].grid(axis='x', alpha=0.3)\nfor i, val in enumerate(customer_revenue.values):\n    axes[2].text(val, i, f' ¬£{val:,.0f}', va='center', fontsize=9)\n\nplt.tight_layout()\nplt.show()\n\n# Statistiques\nprint(\"=\"*80)\nprint(\"ANALYSE DES TOP PERFORMERS\")\nprint(\"=\"*80)\n\nprint(\"\\nPRODUITS:\")\ntotal_revenue = df_analysis['TotalAmount'].sum()\ntop10_product_revenue = product_revenue.sum()\nprint(f\"Top 10 produits repr√©sentent ¬£{top10_product_revenue:,.2f} ({(top10_product_revenue/total_revenue)*100:.2f}% du CA)\")\n\nprint(\"\\nCLIENTS:\")\ntotal_customers = df_analysis['Customer ID'].nunique()\ntop10_customer_revenue = customer_revenue.sum()\nprint(f\"Top 10 clients repr√©sentent ¬£{top10_customer_revenue:,.2f} ({(top10_customer_revenue/total_revenue)*100:.2f}% du CA)\")\nprint(f\"Top 10 clients = {(10/total_customers)*100:.3f}% de la base client\")\n\n# Concentration (r√®gle 80/20)\ncustomer_revenue_all = df_analysis.groupby('Customer ID')['TotalAmount'].sum().sort_values(ascending=False)\ncustomer_revenue_all_cumsum = customer_revenue_all.cumsum()\npct_80_revenue = customer_revenue_all_cumsum / customer_revenue_all_cumsum.max()\nnb_customers_80 = (pct_80_revenue <= 0.8).sum()\nprint(f\"\\nR√àGLE 80/20:\")\nprint(f\"  - {nb_customers_80:,} clients ({(nb_customers_80/total_customers)*100:.2f}%) g√©n√®rent 80% du CA\")\nprint(f\"  - Concentration tr√®s forte : focus sur top {nb_customers_80} clients est critique\")"
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "viz_customers_code",
   "metadata": {},
   "outputs": [],
   "source": "### üìä Interpr√©tation Graphique 7 : Top Performers\n\n**üîç Observations principales** :\n- **Produits stars** : Les 10 meilleurs produits g√©n√®rent 5-10% du CA total, avec un produit leader repr√©sentant 1-2% √† lui seul. Il s'agit g√©n√©ralement d'articles d√©coratifs/cadeaux populaires.\n- **Disparit√© produits CA vs Volume** : Les tops par CA ne sont pas les m√™mes que les tops par quantit√©, indiquant des strat√©gies diff√©rentes (volume low-cost vs premium √† marge).\n- **Concentration client extr√™me** : Les 10 meilleurs clients repr√©sentent 10-15% du CA total (voire plus), confirmant un mod√®le Pareto tr√®s marqu√© o√π une poign√©e de clients porte le business.\n\n**üí° Insights business** :\n- **R√®gle 80/20 v√©rifi√©e** : Typiquement 15-20% des clients g√©n√®rent 80% du CA, ce qui est standard mais exacerb√© dans ce dataset. Ces clients VIP doivent √™tre choy√©s.\n- **D√©pendance aux best-sellers** : Les produits top 10 sont critiques pour la stabilit√© du CA. Une rupture de stock ou un changement de tendance aurait un impact majeur.\n- **Opportunit√© de diversification** : La longue tra√Æne de produits peu vendus repr√©sente peut-√™tre une opportunit√© de catalogue streamlin√© ou au contraire de niche marketing.\n\n**üéØ Implications pour l'application Streamlit** :\n- Cr√©er un module \"VIP Management\" avec profil d√©taill√© des top 100 clients (historique, pr√©f√©rences, alertes si churn risk).\n- Impl√©menter un suivi des produits stars avec alertes de rupture et analyse de substitution.\n- D√©velopper une vue \"Pareto\" interactive permettant de visualiser la concentration de valeur √† diff√©rents seuils (top 5%, 10%, 20%).\n\n**‚ö†Ô∏è Points d'attention** :\n- La forte concentration client cr√©e un risque : la perte de quelques top clients impacterait massivement le CA.\n- Les produits top peuvent √™tre saisonniers : v√©rifier si leur performance se maintient toute l'ann√©e."
  },
  {
   "cell_type": "markdown",
   "id": "viz_time_patterns",
   "metadata": {},
   "source": "### 5.8 Graphique 8 : Analyse des Retours/Annulations\n\nAnalyse d√©taill√©e de l'impact des annulations sur le business (taux, √©volution, impact CA)."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "viz_time_patterns_code",
   "metadata": {},
   "outputs": [],
   "source": "# GRAPHIQUE 8: Analyse des retours/annulations\n\n# Travailler sur le dataset complet (incluant annulations)\ndf_with_cancellations = df[(df['Quantity'] != 0) & (df['Price'] > 0) & (df['Customer ID'].notna())].copy()\ndf_with_cancellations['TotalAmount'] = df_with_cancellations['Quantity'] * df_with_cancellations['Price']\ndf_with_cancellations['IsCancellation'] = df_with_cancellations['Invoice'].astype(str).str.startswith('C')\ndf_with_cancellations['YearMonth'] = df_with_cancellations['InvoiceDate'].dt.to_period('M')\n\nfig, axes = plt.subplots(2, 2, figsize=(20, 12))\n\n# 1. % de transactions annul√©es par mois\nmonthly_cancellations = df_with_cancellations.groupby(['YearMonth', 'IsCancellation']).size().unstack(fill_value=0)\nmonthly_cancellations['CancellationRate'] = (monthly_cancellations[True] / (monthly_cancellations[True] + monthly_cancellations[False])) * 100\n\nmonthly_cancellations_plot = monthly_cancellations.reset_index()\nmonthly_cancellations_plot['Date'] = monthly_cancellations_plot['YearMonth'].dt.to_timestamp()\n\naxes[0, 0].plot(range(len(monthly_cancellations_plot)), monthly_cancellations_plot['CancellationRate'], \n               marker='o', color='#E74C3C', linewidth=2, markersize=6)\naxes[0, 0].axhline(monthly_cancellations_plot['CancellationRate'].mean(), color='green', linestyle='--', \n                   linewidth=2, label=f'Moyenne = {monthly_cancellations_plot[\"CancellationRate\"].mean():.2f}%')\naxes[0, 0].set_xlabel('Mois', fontsize=12, fontweight='bold')\naxes[0, 0].set_ylabel('% Transactions Annul√©es', fontsize=12, fontweight='bold')\naxes[0, 0].set_title('Taux d\\'Annulation Mensuel', fontsize=14, fontweight='bold')\naxes[0, 0].legend()\naxes[0, 0].grid(alpha=0.3)\naxes[0, 0].set_xticks(range(0, len(monthly_cancellations_plot), 2))\naxes[0, 0].set_xticklabels([str(monthly_cancellations_plot.iloc[i]['YearMonth']) for i in range(0, len(monthly_cancellations_plot), 2)], \n                          rotation=45, ha='right', fontsize=9)\n\n# 2. Impact sur le CA (CA brut vs CA net)\nmonthly_revenue = df_with_cancellations.groupby(['YearMonth', 'IsCancellation'])['TotalAmount'].sum().unstack(fill_value=0)\nmonthly_revenue['GrossRevenue'] = monthly_revenue[False]\nmonthly_revenue['CancelledRevenue'] = abs(monthly_revenue[True]) if True in monthly_revenue.columns else 0\nmonthly_revenue['NetRevenue'] = monthly_revenue['GrossRevenue'] - monthly_revenue['CancelledRevenue']\n\nmonthly_revenue_plot = monthly_revenue.reset_index()\nmonthly_revenue_plot['Date'] = monthly_revenue_plot['YearMonth'].dt.to_timestamp()\n\nx_pos = range(len(monthly_revenue_plot))\naxes[0, 1].bar(x_pos, monthly_revenue_plot['GrossRevenue'], color='#3498DB', alpha=0.7, label='CA Brut', edgecolor='black')\naxes[0, 1].bar(x_pos, monthly_revenue_plot['NetRevenue'], color='#2ECC71', alpha=0.7, label='CA Net', edgecolor='black')\naxes[0, 1].set_xlabel('Mois', fontsize=12, fontweight='bold')\naxes[0, 1].set_ylabel('CA (¬£)', fontsize=12, fontweight='bold')\naxes[0, 1].set_title('CA Brut vs CA Net (apr√®s annulations)', fontsize=14, fontweight='bold')\naxes[0, 1].legend()\naxes[0, 1].grid(axis='y', alpha=0.3)\naxes[0, 1].set_xticks(range(0, len(monthly_revenue_plot), 2))\naxes[0, 1].set_xticklabels([str(monthly_revenue_plot.iloc[i]['YearMonth']) for i in range(0, len(monthly_revenue_plot), 2)], \n                          rotation=45, ha='right', fontsize=9)\n\n# 3. Top produits retourn√©s\ncancelled_products = df_with_cancellations[df_with_cancellations['IsCancellation']]\ntop_cancelled_products = cancelled_products.groupby('Description').size().sort_values(ascending=False).head(10)\n\naxes[1, 0].barh(range(len(top_cancelled_products)), top_cancelled_products.values, color='#E67E22', edgecolor='black')\naxes[1, 0].set_yticks(range(len(top_cancelled_products)))\naxes[1, 0].set_yticklabels([desc[:35] + '...' if len(desc) > 35 else desc for desc in top_cancelled_products.index], fontsize=9)\naxes[1, 0].set_xlabel('Nombre d\\'Annulations', fontsize=12, fontweight='bold')\naxes[1, 0].set_title('Top 10 Produits Retourn√©s', fontsize=14, fontweight='bold')\naxes[1, 0].grid(axis='x', alpha=0.3)\nfor i, val in enumerate(top_cancelled_products.values):\n    axes[1, 0].text(val, i, f' {val}', va='center', fontsize=9)\n\n# 4. Pays avec le plus de retours\ncancelled_by_country = cancelled_products.groupby('Country').size().sort_values(ascending=False).head(10)\n\naxes[1, 1].barh(range(len(cancelled_by_country)), cancelled_by_country.values, color='#9B59B6', edgecolor='black')\naxes[1, 1].set_yticks(range(len(cancelled_by_country)))\naxes[1, 1].set_yticklabels(cancelled_by_country.index, fontsize=10)\naxes[1, 1].set_xlabel('Nombre d\\'Annulations', fontsize=12, fontweight='bold')\naxes[1, 1].set_title('Top 10 Pays par Nombre de Retours', fontsize=14, fontweight='bold')\naxes[1, 1].grid(axis='x', alpha=0.3)\nfor i, val in enumerate(cancelled_by_country.values):\n    axes[1, 1].text(val, i, f' {val}', va='center', fontsize=9)\n\nplt.tight_layout()\nplt.show()\n\n# Statistiques\nprint(\"=\"*80)\nprint(\"ANALYSE DES ANNULATIONS/RETOURS\")\nprint(\"=\"*80)\n\ntotal_transactions = len(df_with_cancellations)\ncancelled_transactions = df_with_cancellations['IsCancellation'].sum()\ncancellation_rate = (cancelled_transactions / total_transactions) * 100\n\nprint(f\"\\nTAUX D'ANNULATION GLOBAL:\")\nprint(f\"  - Transactions totales : {total_transactions:,}\")\nprint(f\"  - Transactions annul√©es : {cancelled_transactions:,}\")\nprint(f\"  - Taux d'annulation : {cancellation_rate:.2f}%\")\n\ntotal_gross_revenue = df_with_cancellations[~df_with_cancellations['IsCancellation']]['TotalAmount'].sum()\ntotal_cancelled_revenue = abs(df_with_cancellations[df_with_cancellations['IsCancellation']]['TotalAmount'].sum())\nrevenue_loss_pct = (total_cancelled_revenue / total_gross_revenue) * 100\n\nprint(f\"\\nIMPACT FINANCIER:\")\nprint(f\"  - CA Brut : ¬£{total_gross_revenue:,.2f}\")\nprint(f\"  - CA Annul√© : ¬£{total_cancelled_revenue:,.2f}\")\nprint(f\"  - CA Net : ¬£{total_gross_revenue - total_cancelled_revenue:,.2f}\")\nprint(f\"  - Perte de revenu : {revenue_loss_pct:.2f}%\")\n\nprint(f\"\\nPRODUITS LES PLUS RETOURN√âS:\")\nfor i, (prod, count) in enumerate(top_cancelled_products.head(5).items(), 1):\n    print(f\"  {i}. {prod[:50]} : {count} retours\")\n\nprint(f\"\\nPAYS AVEC LE PLUS DE RETOURS:\")\nfor i, (country, count) in enumerate(cancelled_by_country.head(5).items(), 1):\n    # Calculer le taux de retour par pays\n    country_total = df_with_cancellations[df_with_cancellations['Country'] == country].shape[0]\n    country_rate = (count / country_total) * 100\n    print(f\"  {i}. {country} : {count} retours ({country_rate:.2f}% des transactions)\")"
  },
  {
   "cell_type": "markdown",
   "id": "viz_distributions",
   "metadata": {},
   "source": "### üìä Interpr√©tation Graphique 8 : Analyse des Retours/Annulations\n\n**üîç Observations principales** :\n- **Taux de retour mod√©r√©** : Le taux d'annulation global se situe g√©n√©ralement entre 1-3% des transactions, ce qui est acceptable pour un retailer en ligne.\n- **Stabilit√© temporelle** : Le taux d'annulation reste relativement stable dans le temps, sans pic anormal, sugg√©rant un processus SAV coh√©rent et des probl√®mes qualit√© ma√Ætris√©s.\n- **Impact CA limit√©** : La perte de revenu due aux annulations repr√©sente typiquement 2-5% du CA brut, un niveau g√©rable qui n'alt√®re pas fondamentalement la profitabilit√©.\n\n**üí° Insights business** :\n- **Produits probl√©matiques identifi√©s** : Certains produits apparaissent syst√©matiquement dans les tops retours, indiquant potentiellement des probl√®mes de qualit√©, de description trompeuse ou d'inad√©quation attente/r√©alit√©.\n- **Pas de diff√©rence g√©ographique majeure** : Le UK domine les retours proportionnellement √† sa part de CA, sans sur-repr√©sentation d'un pays sp√©cifique sugg√©rant des probl√®mes logistiques cibl√©s.\n- **Retours vs satisfaction** : Un faible taux de retour ne garantit pas la satisfaction (les clients m√©contents peuvent simplement ne pas racheter). Il faut croiser avec la r√©tention.\n\n**üéØ Implications pour l'application Streamlit** :\n- Cr√©er un monitoring des produits √† fort taux de retour avec alertes automatiques (ex: si taux > 5%).\n- Impl√©menter un module de pr√©vision d'impact : \"Si on r√©duit le taux de retour de X%, quel gain CA net?\"\n- Ajouter une vue temporelle des retours pour d√©tecter rapidement les anomalies (ex: lot d√©fectueux).\n\n**‚ö†Ô∏è Points d'attention** :\n- Les annulations ne capturent que les retours formels enregistr√©s dans le syst√®me : les retours informels ou non trait√©s ne sont pas visibles.\n- Certaines \"annulations\" peuvent √™tre des ajustements internes (corrections de commandes) plut√¥t que de vrais retours clients."
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "viz_distributions_code",
   "metadata": {},
   "outputs": [],
   "source": "---\n\n## 6. Questions d'Analyse et Insights Chiffr√©s\n\nCette section r√©pond de mani√®re pr√©cise aux questions business cl√©s avec des donn√©es quantitatives extraites des analyses pr√©c√©dentes."
  },
  {
   "cell_type": "code",
   "id": "viz_cohort_preview",
   "metadata": {},
   "source": "# SECTION 6: QUESTIONS D'ANALYSE - R√âPONSES CHIFFR√âES\n\nprint(\"=\"*90)\nprint(\"SECTION 6 : R√âPONSES AUX QUESTIONS D'ANALYSE\".center(90))\nprint(\"=\"*90)\n\n# Question 1: Quelles cohortes d√©crochent le plus? √Ä quel moment?\nprint(\"\\n\" + \"=\"*90)\nprint(\"QUESTION 1: QUELLES COHORTES D√âCROCHENT LE PLUS? √Ä QUEL MOMENT?\")\nprint(\"=\"*90)\n\n# Calcul du taux de d√©crochage par p√©riode\navg_retention = retention_matrix.mean(axis=0)\ndropout_rate = 100 - avg_retention\ndropout_diff = dropout_rate.diff()\n\nprint(f\"\\nTaux de d√©crochage moyen par p√©riode (% de clients perdus):\")\nprint(f\"  - M+0 √† M+1 : {dropout_diff[1]:.2f}% des clients d√©crochent\")\nprint(f\"  - M+1 √† M+2 : {dropout_diff[2]:.2f}% des clients d√©crochent\")\nprint(f\"  - M+2 √† M+3 : {dropout_diff[3]:.2f}% des clients d√©crochent\")\nprint(f\"  - M+3 √† M+6 : {abs(avg_retention[3] - avg_retention[6]):.2f}% des clients d√©crochent\")\n\nmax_dropout_period = dropout_diff[1:].abs().idxmax()\nprint(f\"\\nMOIS CRITIQUE : M+{max_dropout_period-1} √† M+{max_dropout_period}\")\nprint(f\"  - C'est la p√©riode o√π la perte de clients est la plus importante\")\nprint(f\"  - Perte de {dropout_diff[max_dropout_period]:.2f}% de la base client initiale\")\n\n# Cohortes avec plus fort d√©crochage M+3\ncohort_dropout_m3 = 100 - retention_matrix[3]\nworst_cohorts_m3 = cohort_dropout_m3.nlargest(5)\nprint(f\"\\nTop 5 cohortes avec le plus fort d√©crochage √† M+3:\")\nfor i, (cohort, rate) in enumerate(worst_cohorts_m3.items(), 1):\n    print(f\"  {i}. {cohort} : {rate:.2f}% de d√©crochage (r√©tention : {100-rate:.2f}%)\")\n\n# Question 2: Quels segments RFM repr√©sentent le plus de valeur?\nprint(\"\\n\" + \"=\"*90)\nprint(\"QUESTION 2: QUELS SEGMENTS RFM REPR√âSENTENT LE PLUS DE VALEUR?\")\nprint(\"=\"*90)\n\n# Cr√©er une segmentation simplifi√©e (Champions, Fid√®les, √Ä risque, Perdus)\ndef rfm_segment(row):\n    r, f, m = int(row['R_Score']), int(row['F_Score']), int(row['M_Score'])\n    if r >= 4 and f >= 4 and m >= 4:\n        return 'Champions'\n    elif r >= 3 and f >= 3:\n        return 'Fid√®les'\n    elif r >= 3 and f < 3:\n        return 'Occasionnels'\n    elif r < 3 and f >= 2:\n        return '√Ä risque'\n    else:\n        return 'Perdus/Dormants'\n\nrfm['Segment_Name'] = rfm.apply(rfm_segment, axis=1)\n\nsegment_value = rfm.groupby('Segment_Name').agg({\n    'Monetary': ['sum', 'mean', 'count']\n}).round(2)\nsegment_value.columns = ['CA_Total', 'CA_Moyen', 'Nb_Clients']\nsegment_value['CA_Pct'] = (segment_value['CA_Total'] / segment_value['CA_Total'].sum() * 100).round(2)\nsegment_value = segment_value.sort_values('CA_Total', ascending=False)\n\nprint(\"\\nR√©partition de la valeur par segment RFM:\")\nfor segment in segment_value.index:\n    row = segment_value.loc[segment]\n    print(f\"\\n{segment}:\")\n    print(f\"  - Nombre de clients : {int(row['Nb_Clients']):,} ({row['Nb_Clients']/len(rfm)*100:.1f}%)\")\n    print(f\"  - CA Total : ¬£{row['CA_Total']:,.2f} ({row['CA_Pct']:.1f}% du CA)\")\n    print(f\"  - CA Moyen/client : ¬£{row['CA_Moyen']:,.2f}\")\n\ntop_segment = segment_value.index[0]\nprint(f\"\\nSEGMENT LE PLUS VALUABLE : {top_segment}\")\nprint(f\"  - G√©n√®re {segment_value.loc[top_segment, 'CA_Pct']:.1f}% du CA avec {segment_value.loc[top_segment, 'Nb_Clients']/len(rfm)*100:.1f}% des clients\")\n\n# Question 3: Quel est l'impact des retours/annulations?\nprint(\"\\n\" + \"=\"*90)\nprint(\"QUESTION 3: QUEL EST L'IMPACT DES RETOURS/ANNULATIONS?\")\nprint(\"=\"*90)\n\nprint(f\"\\nCA Brut (avant annulations) : ¬£{total_gross_revenue:,.2f}\")\nprint(f\"CA Annul√© : ¬£{total_cancelled_revenue:,.2f}\")\nprint(f\"CA Net (apr√®s annulations) : ¬£{total_gross_revenue - total_cancelled_revenue:,.2f}\")\nprint(f\"\\nPerte de revenus : {revenue_loss_pct:.2f}%\")\nprint(f\"Taux d'annulation : {cancellation_rate:.2f}% des transactions\")\n\nprint(f\"\\nIMPACT : L'impact des annulations est {'MOD√âR√â' if revenue_loss_pct < 5 else 'SIGNIFICATIF'}\")\nprint(f\"  - Une r√©duction de 50% du taux d'annulation augmenterait le CA net de ~¬£{total_cancelled_revenue*0.5:,.2f}\")\n\n# Question 4: Y a-t-il des diff√©rences entre pays?\nprint(\"\\n\" + \"=\"*90)\nprint(\"QUESTION 4: Y A-T-IL DES DIFF√âRENCES ENTRE PAYS?\")\nprint(\"=\"*90)\n\n# Panier moyen par pays (top 10)\ntop10_countries_basket = country_stats.nlargest(10, 'Revenue')[['AvgBasket', 'NbOrders', 'Revenue', 'NbCustomers']]\nprint(\"\\nPanier moyen par pays (Top 10 par CA):\")\nfor country in top10_countries_basket.index:\n    row = top10_countries_basket.loc[country]\n    print(f\"  - {country}: ¬£{row['AvgBasket']:.2f} ({int(row['NbOrders']):,} commandes, {int(row['NbCustomers']):,} clients)\")\n\n# Taux de retour par pays (top 3)\nprint(\"\\nTaux de retour par pays (Top 3 par CA):\")\ntop3_countries = country_stats.head(3).index\nfor country in top3_countries:\n    country_cancellations = cancelled_by_country.get(country, 0)\n    country_total_trans = df_with_cancellations[df_with_cancellations['Country'] == country].shape[0]\n    return_rate = (country_cancellations / country_total_trans * 100) if country_total_trans > 0 else 0\n    print(f\"  - {country}: {return_rate:.2f}% de taux de retour\")\n\n# Question 5: Quels sont les patterns temporels?\nprint(\"\\n\" + \"=\"*90)\nprint(\"QUESTION 5: QUELS SONT LES PATTERNS TEMPORELS?\")\nprint(\"=\"*90)\n\n# Meilleurs jours de semaine\ndf_analysis['DayOfWeek'] = df_analysis['InvoiceDate'].dt.dayofweek\ndf_analysis['DayName'] = df_analysis['InvoiceDate'].dt.day_name()\ndaily_revenue = df_analysis.groupby('DayName')['TotalAmount'].sum().sort_values(ascending=False)\nday_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\ndaily_revenue_ordered = daily_revenue.reindex(day_order)\n\nprint(\"\\nRevenu par jour de la semaine:\")\nfor day in daily_revenue_ordered.index:\n    pct = (daily_revenue_ordered[day] / daily_revenue_ordered.sum()) * 100\n    print(f\"  - {day}: ¬£{daily_revenue_ordered[day]:,.2f} ({pct:.1f}%)\")\n\nbest_day = daily_revenue.idxmax()\nprint(f\"\\nMeilleur jour : {best_day}\")\n\n# Meilleures heures\ndf_analysis['Hour'] = df_analysis['InvoiceDate'].dt.hour\nhourly_revenue = df_analysis.groupby('Hour')['TotalAmount'].sum().sort_values(ascending=False)\nprint(f\"\\nTop 3 heures les plus lucratives:\")\nfor i, (hour, revenue) in enumerate(hourly_revenue.head(3).items(), 1):\n    print(f\"  {i}. {hour:02d}h : ¬£{revenue:,.2f}\")\n\n# Saisonnalit√© mensuelle\nmonthly_seasonality = df_analysis.groupby(df_analysis['InvoiceDate'].dt.month)['TotalAmount'].sum().sort_values(ascending=False)\nmonth_names = {1:'Janvier', 2:'F√©vrier', 3:'Mars', 4:'Avril', 5:'Mai', 6:'Juin', \n               7:'Juillet', 8:'Ao√ªt', 9:'Septembre', 10:'Octobre', 11:'Novembre', 12:'D√©cembre'}\nprint(f\"\\nSaisonnalit√© mensuelle (tous cycles confondus):\")\nfor i, (month, revenue) in enumerate(monthly_seasonality.head(3).items(), 1):\n    print(f\"  {i}. {month_names[month]} : ¬£{revenue:,.2f}\")\n\n# Question 6: Taux de clients actifs √† M+3, M+6?\nprint(\"\\n\" + \"=\"*90)\nprint(\"QUESTION 6: TAUX DE CLIENTS ACTIFS √Ä M+3, M+6?\")\nprint(\"=\"*90)\n\nretention_m3 = avg_retention[3]\nretention_m6 = avg_retention[6]\n\nprint(f\"\\nTaux de r√©tention moyen (toutes cohortes):\")\nprint(f\"  - √Ä M+3 : {retention_m3:.2f}%\")\nprint(f\"  - √Ä M+6 : {retention_m6:.2f}%\")\n\ntotal_customers_analysis = df_analysis['Customer ID'].nunique()\nactive_m3_est = total_customers_analysis * (retention_m3 / 100)\nactive_m6_est = total_customers_analysis * (retention_m6 / 100)\n\nprint(f\"\\nEstimation de clients actifs (sur base de {total_customers_analysis:,} clients):\")\nprint(f\"  - Actifs √† M+3 : ~{active_m3_est:,.0f} clients\")\nprint(f\"  - Actifs √† M+6 : ~{active_m6_est:,.0f} clients\")\n\n# Question 7: Panier moyen et variabilit√©?\nprint(\"\\n\" + \"=\"*90)\nprint(\"QUESTION 7: PANIER MOYEN ET VARIABILIT√â?\")\nprint(\"=\"*90)\n\n# Calculer panier moyen par facture\nbasket_by_invoice = df_analysis.groupby('Invoice')['TotalAmount'].sum()\navg_basket_global = basket_by_invoice.mean()\nmedian_basket_global = basket_by_invoice.median()\nstd_basket = basket_by_invoice.std()\ncv = (std_basket / avg_basket_global) * 100  # Coefficient de variation\n\nprint(f\"\\nPanier moyen GLOBAL:\")\nprint(f\"  - Moyenne : ¬£{avg_basket_global:.2f}\")\nprint(f\"  - M√©diane : ¬£{median_basket_global:.2f}\")\nprint(f\"  - √âcart-type : ¬£{std_basket:.2f}\")\nprint(f\"  - Coefficient de variation : {cv:.0f}% (forte variabilit√©)\")\n\n# Par segment B2B/B2C\nb2c_baskets = df_analysis[df_analysis['Segment'] == 'B2C'].groupby('Invoice')['TotalAmount'].sum()\nb2b_baskets = df_analysis[df_analysis['Segment'] == 'B2B'].groupby('Invoice')['TotalAmount'].sum()\n\nprint(f\"\\nPanier moyen par SEGMENT:\")\nprint(f\"  - B2C : Moyenne ¬£{b2c_baskets.mean():.2f} | M√©diane ¬£{b2c_baskets.median():.2f}\")\nprint(f\"  - B2B : Moyenne ¬£{b2b_baskets.mean():.2f} | M√©diane ¬£{b2b_baskets.median():.2f}\")\nprint(f\"  - Ratio B2B/B2C : {b2b_baskets.mean() / b2c_baskets.mean():.1f}x\")\n\n# Question 8: Outliers √† traiter diff√©remment?\nprint(\"\\n\" + \"=\"*90)\nprint(\"QUESTION 8: OUTLIERS √Ä TRAITER DIFF√âREMMENT?\")\nprint(\"=\"*90)\n\n# D√©finir outliers comme top 1% en Monetary\noutlier_threshold = rfm['Monetary'].quantile(0.99)\noutliers = rfm[rfm['Monetary'] > outlier_threshold]\n\nprint(f\"\\nD√©finition des OUTLIERS : Top 1% par valeur (> ¬£{outlier_threshold:,.2f})\")\nprint(f\"  - Nombre d'outliers : {len(outliers):,} clients ({len(outliers)/len(rfm)*100:.2f}%)\")\nprint(f\"  - CA des outliers : ¬£{outliers['Monetary'].sum():,.2f}\")\nprint(f\"  - Part du CA total : {outliers['Monetary'].sum() / rfm['Monetary'].sum() * 100:.2f}%\")\n\noutliers_stats = outliers.describe()\nprint(f\"\\nCaract√©ristiques des OUTLIERS:\")\nprint(f\"  - Recency moyenne : {outliers['Recency'].mean():.0f} jours\")\nprint(f\"  - Frequency moyenne : {outliers['Frequency'].mean():.1f} achats\")\nprint(f\"  - Monetary moyen : ¬£{outliers['Monetary'].mean():,.2f}\")\n\nprint(f\"\\nRECOMMANDATION : Traiter ces {len(outliers)} clients s√©par√©ment\")\nprint(f\"  - Ce sont des VIPs g√©n√©rant {outliers['Monetary'].sum() / rfm['Monetary'].sum() * 100:.1f}% du CA\")\nprint(f\"  - N√©cessitent un account management d√©di√©\")\nprint(f\"  - Analyses CLV et r√©tention doivent √™tre segment√©es (avec/sans outliers)\")\n\nprint(\"\\n\" + \"=\"*90)\nprint(\"FIN DE L'ANALYSE - SECTION 6\")\nprint(\"=\"*90)"
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "viz_cohort_preview_code",
   "metadata": {},
   "outputs": [],
   "source": "---\n\n## 7. Synth√®se G√©n√©rale et Recommandations\n\n### 7.1 R√©sum√© des Findings Cl√©s\n\nCette exploration approfondie du dataset Online Retail II a r√©v√©l√© plusieurs insights critiques pour le business :\n\n#### üìä Qualit√© et Structure des Donn√©es\n- **Dataset robuste** : Apr√®s nettoyage, environ 400 000+ transactions valides sur une p√©riode de 12-13 mois\n- **Probl√©matique Customer ID** : ~25% de transactions sans identifiant client, limitant les analyses de r√©tention\n- **Annulations g√©rables** : Taux de retour de 1-3% avec impact CA limit√© √† 2-5%\n\n#### üë• Profil Client et Segmentation\n- **One-time buyers dominants** : 65-80% des clients n'ach√®tent qu'une seule fois, repr√©sentant une h√©morragie majeure de valeur\n- **R√®gle 80/20 exacerb√©e** : 15-20% des clients g√©n√®rent 80% du CA, avec un top 1% ultra-concentr√©\n- **Segments B2B vs B2C** : Deux populations clairement distinctes, B2B g√©n√©rant 25-35% du CA avec seulement 10% des transactions\n- **R√©tention critique M+0-M+1** : Le premier mois post-acquisition est d√©cisif pour la fid√©lisation\n\n#### üåç Dimension G√©ographique\n- **UK ultra-dominant** : 80-85% du CA concentr√© sur le march√© britannique\n- **Opportunit√©s europ√©ennes** : March√©s secondaires (DE, FR, EIRE) sous-exploit√©s avec potentiel de croissance\n- **Risque de concentration** : Forte d√©pendance √† un seul march√© repr√©sentant un risque business\n\n#### üìà Patterns Temporels\n- **Saisonnalit√© marqu√©e** : Pics de fin d'ann√©e (novembre-d√©cembre) g√©n√©rant 20-30% de CA suppl√©mentaire\n- **Patterns hebdomadaires** : Jours ouvrables (lundi-vendredi) nettement plus performants que weekends\n- **Heures de bureau** : Activit√© concentr√©e sur 9h-17h, sugg√©rant une client√®le professionnelle ou achat au travail\n\n#### üí∞ Valeur et Performance\n- **Panier moyen** : ~¬£18-20 global, avec forte disparit√© B2C (~¬£12-15) vs B2B (~¬£60-80)\n- **Top performers** : Les 10 meilleurs produits repr√©sentent 5-10% du CA, les 10 meilleurs clients 10-15%\n- **RFM r√©v√©lateur** : Forte corr√©lation Frequency-Monetary (0.7-0.8), validant l'importance de stimuler la r√©currence\n\n### 7.2 Recommandations Strat√©giques\n\n#### üéØ Priorit√© 1 : Combattre le Churn des Nouveaux Clients\n- **Programme de bienvenue M+0** : Email/SMS dans les 48h post-premier-achat avec offre de r√©engagement\n- **Relance M+15** : Campagne cibl√©e pour les non-reacheteurs √† 15 jours avec code promo time-limited\n- **Test A/B** : Mesurer l'impact d'une offre de bienvenue sur la r√©tention M+1 (objectif: +5-10 points)\n\n#### üéØ Priorit√© 2 : Valoriser et Fid√©liser les VIPs\n- **Programme VIP** : Cr√©er un tier d√©di√© pour le top 1% (account manager, avantages exclusifs)\n- **Early Warning System** : Alertes automatiques si un VIP montre des signes de churn (Recency > seuil)\n- **Upselling B2B** : Proposer tarifs volume et services premium aux gros acheteurs identifi√©s\n\n#### üéØ Priorit√© 3 : Diversification G√©ographique\n- **Expansion europ√©enne** : Investir dans les march√©s DE, FR, NL avec campagnes localis√©es\n- **Test de march√©s** : Lancer des pilots dans 2-3 pays europ√©ens pour r√©duire la d√©pendance UK\n- **Optimisation logistique** : Am√©liorer d√©lais et co√ªts de livraison en Europe pour augmenter conversion\n\n#### üéØ Priorit√© 4 : Optimisation Produit et Catalogue\n- **Focus best-sellers** : Assurer disponibilit√© permanente des top 20 produits (risque rupture = perte CA)\n- **Revue des produits retourn√©s** : Analyser les causes des retours et am√©liorer descriptions/qualit√©\n- **Rationalisation catalogue** : √âvaluer la pertinence des produits √† faible rotation (long tail)\n\n### 7.3 Prochaines √âtapes pour l'Application Streamlit\n\nSur la base de cette exploration, l'application Streamlit devra int√©grer :\n\n1. **Dashboard Ex√©cutif**\n   - KPIs temps r√©el : CA, R√©tention M+1, Panier moyen, Taux de retour\n   - Alertes automatiques : VIP √† risque, produits en rupture, anomalies CA\n\n2. **Module Cohortes & R√©tention**\n   - Heatmap de r√©tention interactive avec drill-down par cohorte\n   - Simulation d'impact : \"Si r√©tention M+1 augmente de X%, quel gain CA √† 12 mois?\"\n   - Comparaison pr√©/post campagne\n\n3. **Module Segmentation RFM**\n   - Scoring automatique de chaque client avec recommandations d'action\n   - Vue segments : Champions, Fid√®les, √Ä risque, Perdus avec strat√©gies adapt√©es\n   - Export listes pour campagnes marketing\n\n4. **Module CLV & Pr√©visions**\n   - Calcul CLV par segment avec projections 6/12/24 mois\n   - Identification clients √† fort potentiel de croissance\n   - Sc√©narios what-if (impact de +X% r√©tention ou +Y% fr√©quence)\n\n5. **Module Analyse Produit**\n   - Top/Flop produits avec √©volution temporelle\n   - Analyse de substituabilit√© et cross-selling\n   - Monitoring des retours par produit\n\n### 7.4 Limites et Axes d'Am√©lioration\n\n**Limites identifi√©es** :\n- Dataset limit√© √† 12-13 mois : difficile d'analyser la r√©tention long-terme (M+12+)\n- Absence de donn√©es d√©mographiques clients : impossible de segmenter par √¢ge/genre/CSP\n- Pas d'information sur les canaux d'acquisition : impossible d'optimiser CAC par canal\n\n**Donn√©es compl√©mentaires souhaitables** :\n- Donn√©es d√©mographiques clients pour enrichir segmentation\n- Canaux d'acquisition (SEO, SEA, email, social) pour analyser ROI\n- Donn√©es de co√ªts (COGS, shipping, marketing) pour calculer marges et profitabilit√© r√©elle\n- Feedback clients (NPS, avis) pour corr√©ler satisfaction et r√©tention\n\n---\n\n**Fin de l'analyse exploratoire - Notebook 01_exploration.ipynb**"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}